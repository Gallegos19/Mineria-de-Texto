{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistema de Minería de Texto para Mejora de Contenido Ambiental\n",
    "\n",
    "Este notebook implementa un sistema completo de procesamiento y mejora de texto utilizando técnicas de NLP y el modelo BERT. El sistema está diseñado para mejorar textos relacionados con concientización ambiental, optimizando títulos, descripciones y contenido para un panel de administración.\n",
    "\n",
    "## Estructura del Sistema\n",
    "\n",
    "El sistema sigue las siguientes fases de procesamiento:\n",
    "\n",
    "1. **Ingesta del texto**: Entrada de texto desde diferentes fuentes\n",
    "2. **Limpieza básica**: Eliminación de caracteres especiales, espacios, etc.\n",
    "3. **Tokenización**: División del texto en tokens (palabras, frases)\n",
    "4. **Normalización**: Conversión a minúsculas, eliminación de acentos, etc.\n",
    "5. **Eliminación de ruido**: Eliminación de stopwords y contenido irrelevante\n",
    "6. **Lematización/Stemming**: Reducción de palabras a su forma base\n",
    "7. **Procesamiento con BERT**: Mejora del texto utilizando modelos de lenguaje avanzados\n",
    "8. **Generación de salida**: Texto mejorado con enfoque personalizable\n",
    "\n",
    "Cada módulo se implementa de forma independiente para facilitar su comprensión y posible reutilización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de Dependencias\n",
    "\n",
    "Primero, instalemos las bibliotecas necesarias para el procesamiento de texto y el uso de BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NumPy 1.24.4 y Pandas 2.0.3 cargados\n",
      "✓ Matplotlib y Seaborn cargados\n",
      "✓ NLTK 3.8.1 cargado\n",
      "✓ spaCy 3.7.2 cargado\n",
      "✓ PyTorch 2.7.1+cpu cargado\n",
      "✓ Dispositivo configurado: cpu\n",
      "✓ Transformers 4.53.2 cargado (solo PyTorch)\n",
      "\n",
      "Descargando recursos de NLTK...\n",
      "✓ Recursos de NLTK descargados\n",
      "Cargando modelo de spaCy para español...\n",
      "✓ Modelo de spaCy cargado exitosamente\n",
      "✓ Stopwords configuradas\n",
      "✓ Stemmer y Lemmatizer configurados\n",
      "\n",
      "==================================================\n",
      "ESTADO DEL SISTEMA:\n",
      "✓ Bibliotecas básicas: Cargadas\n",
      "✓ NumPy/Pandas: Cargadas\n",
      "✓ NLTK: Cargado\n",
      "✓ spaCy: Cargado\n",
      "✓ PyTorch: Disponible\n",
      "✓ Transformers: Disponible\n",
      "✓ BERT: Disponible\n",
      "✓ Dispositivo: cpu\n",
      "✓ Modelo BERT configurado: dccuchile/bert-base-spanish-wwm-uncased\n",
      "==================================================\n",
      "✓ CONFIGURACIÓN COMPLETADA\n",
      "✓ Variables del sistema configuradas\n"
     ]
    }
   ],
   "source": [
    "# Importación de bibliotecas con manejo de errores\n",
    "import re\n",
    "import unicodedata\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "# Configuraciones generales\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importar bibliotecas científicas básicas\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    print(f\"✓ NumPy {np.__version__} y Pandas {pd.__version__} cargados\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error con NumPy/Pandas: {e}\")\n",
    "    raise\n",
    "\n",
    "# Importar bibliotecas de visualización\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    plt.style.use('default')  # Usar estilo por defecto más compatible\n",
    "    sns.set_palette(\"husl\")\n",
    "    print(\"✓ Matplotlib y Seaborn cargados\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error con visualización: {e}\")\n",
    "\n",
    "# Importar NLTK\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "    print(f\"✓ NLTK {nltk.__version__} cargado\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error con NLTK: {e}\")\n",
    "    raise\n",
    "\n",
    "# Importar spaCy\n",
    "try:\n",
    "    import spacy\n",
    "    print(f\"✓ spaCy {spacy.__version__} cargado\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Error con spaCy: {e}\")\n",
    "    raise\n",
    "\n",
    "# Configurar PyTorch y Transformers con manejo de errores\n",
    "TORCH_AVAILABLE = False\n",
    "TRANSFORMERS_AVAILABLE = False\n",
    "\n",
    "# Configurar variables de entorno para evitar conflictos con TensorFlow\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['TRANSFORMERS_VERBOSITY'] = 'error'\n",
    "os.environ['USE_TF'] = 'NO'\n",
    "os.environ['USE_TORCH'] = 'YES'\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    TORCH_AVAILABLE = True\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"✓ PyTorch {torch.__version__} cargado\")\n",
    "    print(f\"✓ Dispositivo configurado: {device}\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ PyTorch no disponible: {e}\")\n",
    "    device = 'cpu'\n",
    "\n",
    "try:\n",
    "    from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "    import transformers\n",
    "    transformers.utils.logging.set_verbosity_error()\n",
    "    TRANSFORMERS_AVAILABLE = True\n",
    "    print(f\"✓ Transformers {transformers.__version__} cargado (solo PyTorch)\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Transformers no disponible: {e}\")\n",
    "    print(\"El sistema funcionará sin capacidades de BERT\")\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "print(\"\\nDescargando recursos de NLTK...\")\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    nltk.download('omw-1.4', quiet=True)\n",
    "    try:\n",
    "        nltk.download('punkt_tab', quiet=True)\n",
    "    except:\n",
    "        pass  # Este recurso puede no estar disponible en todas las versiones\n",
    "    print(\"✓ Recursos de NLTK descargados\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Advertencia con recursos NLTK: {e}\")\n",
    "\n",
    "# Cargar modelo de spaCy para español\n",
    "print(\"Cargando modelo de spaCy para español...\")\n",
    "try:\n",
    "    nlp = spacy.load('es_core_news_sm')\n",
    "    print(\"✓ Modelo de spaCy cargado exitosamente\")\n",
    "except OSError:\n",
    "    print(\"Descargando modelo de spaCy para español...\")\n",
    "    try:\n",
    "        import subprocess\n",
    "        import sys\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_sm\"])\n",
    "        nlp = spacy.load('es_core_news_sm')\n",
    "        print(\"✓ Modelo de spaCy descargado y cargado exitosamente\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error descargando modelo de spaCy: {e}\")\n",
    "        print(\"Usando modelo en blanco como fallback...\")\n",
    "        nlp = spacy.blank('es')\n",
    "\n",
    "# Configurar stopwords en español\n",
    "try:\n",
    "    spanish_stopwords = set(stopwords.words('spanish'))\n",
    "    \n",
    "    # Añadir stopwords personalizadas para contexto ambiental\n",
    "    environmental_stopwords = {\n",
    "        'además', 'asimismo', 'también', 'igualmente', 'por tanto', \n",
    "        'sin embargo', 'no obstante', 'por consiguiente'\n",
    "    }\n",
    "    spanish_stopwords.update(environmental_stopwords)\n",
    "    print(\"✓ Stopwords configuradas\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error configurando stopwords: {e}\")\n",
    "    spanish_stopwords = set()\n",
    "\n",
    "# Configurar stemmer y lemmatizer\n",
    "try:\n",
    "    stemmer = SnowballStemmer('spanish')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    print(\"✓ Stemmer y Lemmatizer configurados\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error configurando stemmer/lemmatizer: {e}\")\n",
    "\n",
    "# Configurar modelos BERT (se cargarán cuando sea necesario)\n",
    "BERT_MODEL_NAME = 'dccuchile/bert-base-spanish-wwm-uncased' if TRANSFORMERS_AVAILABLE else None\n",
    "\n",
    "# Resumen del estado del sistema\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ESTADO DEL SISTEMA:\")\n",
    "print(f\"✓ Bibliotecas básicas: Cargadas\")\n",
    "print(f\"✓ NumPy/Pandas: Cargadas\")\n",
    "print(f\"✓ NLTK: Cargado\")\n",
    "print(f\"✓ spaCy: Cargado\")\n",
    "print(f\"{'✓' if TORCH_AVAILABLE else '⚠️'} PyTorch: {'Disponible' if TORCH_AVAILABLE else 'No disponible'}\")\n",
    "print(f\"{'✓' if TRANSFORMERS_AVAILABLE else '⚠️'} Transformers: {'Disponible' if TRANSFORMERS_AVAILABLE else 'No disponible'}\")\n",
    "print(f\"{'✓' if TRANSFORMERS_AVAILABLE else '⚠️'} BERT: {'Disponible' if TRANSFORMERS_AVAILABLE else 'No disponible'}\")\n",
    "print(f\"✓ Dispositivo: {device}\")\n",
    "\n",
    "if BERT_MODEL_NAME:\n",
    "    print(f\"✓ Modelo BERT configurado: {BERT_MODEL_NAME}\")\n",
    "else:\n",
    "    print(\"⚠️ BERT no disponible - usando métodos alternativos\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"✓ CONFIGURACIÓN COMPLETADA\")\n",
    "\n",
    "# Variables globales para uso en el resto del código\n",
    "SYSTEM_CONFIG = {\n",
    "    'torch_available': TORCH_AVAILABLE,\n",
    "    'transformers_available': TRANSFORMERS_AVAILABLE,\n",
    "    'bert_model_name': BERT_MODEL_NAME,\n",
    "    'device': device,\n",
    "    'nlp_model': nlp,\n",
    "    'spanish_stopwords': spanish_stopwords,\n",
    "    'stemmer': stemmer if 'stemmer' in locals() else None,\n",
    "    'lemmatizer': lemmatizer if 'lemmatizer' in locals() else None\n",
    "}\n",
    "\n",
    "print(\"✓ Variables del sistema configuradas\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 1: Ingesta del Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÓDULO 1: INGESTA DEL TEXTO ===\n",
      "Texto ingresado exitosamente:\n",
      "- Fuente: manual\n",
      "- Longitud: 393 caracteres\n",
      "- Palabras: 53 palabras\n",
      "\n",
      "Validaciones:\n",
      "- is_empty: False\n",
      "- min_length: True\n",
      "- has_letters: True\n",
      "- encoding_issues: False\n"
     ]
    }
   ],
   "source": [
    "# ===== MÓDULO 1: INGESTA DEL TEXTO =====\n",
    "\n",
    "class TextIngestion:\n",
    "    \"\"\"\n",
    "    Módulo para la ingesta de texto desde diferentes fuentes.\n",
    "    Permite entrada manual, desde archivos o URLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.supported_formats = ['txt', 'csv', 'json']\n",
    "    \n",
    "    def ingest_manual_text(self, text):\n",
    "        \"\"\"\n",
    "        Ingesta de texto manual directo\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"El texto debe ser una cadena de caracteres\")\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'source': 'manual',\n",
    "            'length': len(text),\n",
    "            'word_count': len(text.split())\n",
    "        }\n",
    "    \n",
    "    def ingest_from_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Ingesta de texto desde archivo\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            return {\n",
    "                'original_text': text,\n",
    "                'source': f'file: {file_path}',\n",
    "                'length': len(text),\n",
    "                'word_count': len(text.split())\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error al leer archivo: {str(e)}\")\n",
    "    \n",
    "    def validate_text(self, text_data):\n",
    "        \"\"\"\n",
    "        Validación básica del texto ingresado\n",
    "        \"\"\"\n",
    "        text = text_data['original_text']\n",
    "        \n",
    "        validations = {\n",
    "            'is_empty': len(text.strip()) == 0,\n",
    "            'min_length': len(text) >= 10,\n",
    "            'has_letters': bool(re.search(r'[a-zA-ZáéíóúÁÉÍÓÚñÑ]', text)),\n",
    "            'encoding_issues': '�' in text\n",
    "        }\n",
    "        \n",
    "        return validations\n",
    "\n",
    "# Ejemplo de uso del módulo de ingesta\n",
    "print(\"=== MÓDULO 1: INGESTA DEL TEXTO ===\")\n",
    "\n",
    "# Crear instancia del módulo\n",
    "ingestion = TextIngestion()\n",
    "\n",
    "# Texto de ejemplo relacionado con medio ambiente\n",
    "sample_text = \"\"\"\n",
    "El cambio climático representa uno de los mayores desafíos de nuestro tiempo. \n",
    "Cambio Climatico mayor desafio tiempo\n",
    "La deforestación, la contaminación del aire y el uso excesivo de combustibles fósiles \n",
    "están afectando gravemente nuestro planeta. Es fundamental implementar energías renovables \n",
    "y promover prácticas sostenibles para proteger el medio ambiente para las futuras generaciones.\n",
    "\"\"\"\n",
    "\n",
    "# Ingestar el texto\n",
    "text_data = ingestion.ingest_manual_text(sample_text)\n",
    "print(\"Texto ingresado exitosamente:\")\n",
    "print(f\"- Fuente: {text_data['source']}\")\n",
    "print(f\"- Longitud: {text_data['length']} caracteres\")\n",
    "print(f\"- Palabras: {text_data['word_count']} palabras\")\n",
    "\n",
    "# Validar el texto\n",
    "validations = ingestion.validate_text(text_data)\n",
    "print(\"\\nValidaciones:\")\n",
    "for key, value in validations.items():\n",
    "    print(f\"- {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 2: Limpieza Basica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÓDULO 2: LIMPIEZA BÁSICA ===\n",
      "Texto original:\n",
      "'\\n¡¡¡El cambio climático!!! es un problema muy serio...    \\n\\nVisita https://ejemplo.com para más información.\\nContacto: info@medioambiente.org o llama al +1-555-123-4567\\n\\n<p>La <strong>deforestación</strong> afecta nuestro planeta.</p>\\n\\nÃ¡reas protegidas son fundamentales!!!   Â¿Qué podemos hacer???\\n\\n\\n'\n",
      "\n",
      "==================================================\n",
      "Texto limpio:\n",
      "'El cambio climático! es un problema muy serio. Visita para más información. Contacto: o llama al La deforestación afecta nuestro planeta. áreas protegidas son fundamentales! Qué podemos hacer?'\n",
      "\n",
      "Estadísticas de limpieza:\n",
      "- Longitud original: 302 caracteres\n",
      "- Longitud limpia: 192 caracteres\n",
      "- Reducción: 36.42%\n",
      "\n",
      "==============================\n",
      "Limpieza con opciones personalizadas:\n",
      "Texto con limpieza personalizada:\n",
      "'El cambio climático es un problema muy serio Visita para más información Contacto info medioambiente org o llama al La deforestación afecta nuestro planeta áreas protegidas son fundamentales Qué podemos hacer'\n"
     ]
    }
   ],
   "source": [
    "# ===== MÓDULO 2: LIMPIEZA BÁSICA =====\n",
    "\n",
    "class TextCleaner:\n",
    "    \"\"\"\n",
    "    Módulo para la limpieza básica del texto.\n",
    "    Elimina caracteres especiales, espacios extra, URLs, emails, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Patrones regex para diferentes tipos de limpieza\n",
    "        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        self.email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "        self.phone_pattern = re.compile(r'(\\+?[0-9]{1,3}[-.\\s]?)?(\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4})')\n",
    "        self.html_pattern = re.compile(r'<[^>]+>')\n",
    "        self.special_chars_pattern = re.compile(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)áéíóúÁÉÍÓÚñÑüÜ]')\n",
    "        self.multiple_spaces_pattern = re.compile(r'\\s+')\n",
    "        self.multiple_punctuation_pattern = re.compile(r'([.!?]){2,}')\n",
    "    \n",
    "    def remove_urls(self, text: str) -> str:\n",
    "        \"\"\"Elimina URLs del texto\"\"\"\n",
    "        return self.url_pattern.sub('', text)\n",
    "    \n",
    "    def remove_emails(self, text: str) -> str:\n",
    "        \"\"\"Elimina direcciones de email del texto\"\"\"\n",
    "        return self.email_pattern.sub('', text)\n",
    "    \n",
    "    def remove_phone_numbers(self, text: str) -> str:\n",
    "        \"\"\"Elimina números de teléfono del texto\"\"\"\n",
    "        return self.phone_pattern.sub('', text)\n",
    "    \n",
    "    def remove_html_tags(self, text: str) -> str:\n",
    "        \"\"\"Elimina etiquetas HTML del texto\"\"\"\n",
    "        return self.html_pattern.sub('', text)\n",
    "    \n",
    "    def remove_special_characters(self, text: str, keep_punctuation: bool = True) -> str:\n",
    "        \"\"\"\n",
    "        Elimina caracteres especiales manteniendo letras, números y opcionalmente puntuación\n",
    "        \"\"\"\n",
    "        if keep_punctuation:\n",
    "            # Mantener puntuación básica y caracteres en español\n",
    "            cleaned = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)áéíóúÁÉÍÓÚñÑüÜ]', ' ', text)\n",
    "        else:\n",
    "            # Solo mantener letras, números y espacios\n",
    "            cleaned = re.sub(r'[^\\w\\s áéíóúÁÉÍÓÚñÑüÜ]', ' ', text)\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def normalize_whitespace(self, text: str) -> str:\n",
    "        \"\"\"Normaliza espacios en blanco múltiples\"\"\"\n",
    "        # Reemplazar múltiples espacios con uno solo\n",
    "        text = self.multiple_spaces_pattern.sub(' ', text)\n",
    "        # Eliminar espacios al inicio y final\n",
    "        return text.strip()\n",
    "    \n",
    "    def normalize_punctuation(self, text: str) -> str:\n",
    "        \"\"\"Normaliza puntuación repetida\"\"\"\n",
    "        return self.multiple_punctuation_pattern.sub(r'\\1', text)\n",
    "    \n",
    "    def remove_extra_newlines(self, text: str) -> str:\n",
    "        \"\"\"Elimina saltos de línea excesivos\"\"\"\n",
    "        # Reemplazar múltiples saltos de línea con uno solo\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        # Reemplazar saltos de línea con espacios para texto continuo\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def clean_encoding_issues(self, text: str) -> str:\n",
    "        \"\"\"Corrige problemas de codificación comunes\"\"\"\n",
    "        # Diccionario de reemplazos comunes\n",
    "        encoding_fixes = {\n",
    "            'Ã¡': 'á', 'Ã©': 'é', 'Ã­': 'í', 'Ã³': 'ó', 'Ãº': 'ú',\n",
    "            'Ã±': 'ñ', 'Ã¼': 'ü', 'Â': '', 'â€™': \"'\", 'â€œ': '\"',\n",
    "            'â€': '\"', 'â€¦': '...', 'â€\"': '-', '�': ''\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in encoding_fixes.items():\n",
    "            text = text.replace(wrong, correct)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def basic_clean(self, text: str, options: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Realiza limpieza básica completa del texto\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a limpiar\n",
    "            options: Diccionario con opciones de limpieza\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con texto limpio y estadísticas\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            options = {\n",
    "                'remove_urls': True,\n",
    "                'remove_emails': True,\n",
    "                'remove_phones': True,\n",
    "                'remove_html': True,\n",
    "                'remove_special_chars': True,\n",
    "                'keep_punctuation': True,\n",
    "                'normalize_whitespace': True,\n",
    "                'normalize_punctuation': True,\n",
    "                'remove_newlines': True,\n",
    "                'fix_encoding': True\n",
    "            }\n",
    "        \n",
    "        original_text = text\n",
    "        original_length = len(text)\n",
    "        \n",
    "        # Aplicar limpiezas según opciones\n",
    "        if options.get('fix_encoding', True):\n",
    "            text = self.clean_encoding_issues(text)\n",
    "        \n",
    "        if options.get('remove_html', True):\n",
    "            text = self.remove_html_tags(text)\n",
    "        \n",
    "        if options.get('remove_urls', True):\n",
    "            text = self.remove_urls(text)\n",
    "        \n",
    "        if options.get('remove_emails', True):\n",
    "            text = self.remove_emails(text)\n",
    "        \n",
    "        if options.get('remove_phones', True):\n",
    "            text = self.remove_phone_numbers(text)\n",
    "        \n",
    "        if options.get('remove_newlines', True):\n",
    "            text = self.remove_extra_newlines(text)\n",
    "        \n",
    "        if options.get('remove_special_chars', True):\n",
    "            text = self.remove_special_characters(text, options.get('keep_punctuation', True))\n",
    "        \n",
    "        if options.get('normalize_punctuation', True):\n",
    "            text = self.normalize_punctuation(text)\n",
    "        \n",
    "        if options.get('normalize_whitespace', True):\n",
    "            text = self.normalize_whitespace(text)\n",
    "        \n",
    "        # Calcular estadísticas\n",
    "        cleaned_length = len(text)\n",
    "        reduction_percentage = ((original_length - cleaned_length) / original_length) * 100 if original_length > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'original_text': original_text,\n",
    "            'cleaned_text': text,\n",
    "            'original_length': original_length,\n",
    "            'cleaned_length': cleaned_length,\n",
    "            'reduction_percentage': round(reduction_percentage, 2),\n",
    "            'cleaning_options': options\n",
    "        }\n",
    "\n",
    "# Ejemplo de uso del módulo de limpieza\n",
    "print(\"=== MÓDULO 2: LIMPIEZA BÁSICA ===\")\n",
    "\n",
    "# Crear instancia del limpiador\n",
    "cleaner = TextCleaner()\n",
    "\n",
    "# Texto de ejemplo con problemas comunes\n",
    "dirty_text = \"\"\"\n",
    "¡¡¡El cambio climático!!! es un problema muy serio...    \n",
    "\n",
    "Visita https://ejemplo.com para más información.\n",
    "Contacto: info@medioambiente.org o llama al +1-555-123-4567\n",
    "\n",
    "<p>La <strong>deforestación</strong> afecta nuestro planeta.</p>\n",
    "\n",
    "Ã¡reas protegidas son fundamentales!!!   Â¿Qué podemos hacer???\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texto original:\")\n",
    "print(repr(dirty_text))\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Limpiar el texto\n",
    "result = cleaner.basic_clean(dirty_text)\n",
    "\n",
    "print(\"Texto limpio:\")\n",
    "print(f\"'{result['cleaned_text']}'\")\n",
    "print(f\"\\nEstadísticas de limpieza:\")\n",
    "print(f\"- Longitud original: {result['original_length']} caracteres\")\n",
    "print(f\"- Longitud limpia: {result['cleaned_length']} caracteres\")\n",
    "print(f\"- Reducción: {result['reduction_percentage']}%\")\n",
    "\n",
    "# Ejemplo con opciones personalizadas\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"Limpieza con opciones personalizadas:\")\n",
    "\n",
    "custom_options = {\n",
    "    'remove_urls': True,\n",
    "    'remove_emails': False,  # Mantener emails\n",
    "    'remove_phones': True,\n",
    "    'remove_html': True,\n",
    "    'remove_special_chars': True,\n",
    "    'keep_punctuation': False,  # Eliminar toda la puntuación\n",
    "    'normalize_whitespace': True,\n",
    "    'normalize_punctuation': True,\n",
    "    'remove_newlines': True,\n",
    "    'fix_encoding': True\n",
    "}\n",
    "\n",
    "custom_result = cleaner.basic_clean(dirty_text, custom_options)\n",
    "print(f\"Texto con limpieza personalizada:\")\n",
    "print(f\"'{custom_result['cleaned_text']}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 3: Tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÓDULO 3: TOKENIZACIÓN ===\n",
      "Texto original:\n",
      "\n",
      "El cambio climático es uno de los mayores desafíos ambientales de nuestro tiempo. \n",
      "La deforestación y la contaminación del aire están afectando gravemente los ecosistemas.\n",
      "\n",
      "Las energías renovables como la solar y eólica ofrecen soluciones sostenibles. \n",
      "Es fundamental proteger la biodiversidad y conservar los recursos naturales para las futuras generaciones.\n",
      "México tiene importantes reservas naturales que debemos preservar.\n",
      "\n",
      "\n",
      "==================================================\n",
      "1. TOKENIZACIÓN EN ORACIONES:\n",
      "   1. El cambio climático es uno de los mayores desafíos ambientales de nuestro tiempo.\n",
      "   2. La deforestación y la contaminación del aire están afectando gravemente los ecosistemas.\n",
      "   3. Las energías renovables como la solar y eólica ofrecen soluciones sostenibles.\n",
      "   4. Es fundamental proteger la biodiversidad y conservar los recursos naturales para las futuras generaciones.\n",
      "   5. México tiene importantes reservas naturales que debemos preservar.\n",
      "\n",
      "2. TOKENIZACIÓN EN PALABRAS (primeras 20):\n",
      "   ['El', 'cambio', 'climático', 'es', 'uno', 'de', 'los', 'mayores', 'desafíos', 'ambientales', 'de', 'nuestro', 'tiempo', '.', 'La', 'deforestación', 'y', 'la', 'contaminación', 'del']...\n",
      "   Total de palabras: 63\n",
      "\n",
      "3. TOKENIZACIÓN EN PÁRRAFOS:\n",
      "   Párrafo 1: El cambio climático es uno de los mayores desafíos...\n",
      "   Párrafo 2: Las energías renovables como la solar y eólica ofr...\n",
      "\n",
      "4. TOKENIZACIÓN AVANZADA:\n",
      "   Total de tokens: 63\n",
      "   Entidades encontradas: [('La deforestación y la contaminación', 'MISC'), ('México', 'LOC')]\n",
      "\n",
      "5. AGRUPACIÓN POR CATEGORÍAS GRAMATICALES:\n",
      "   Sustantivos: ['cambio', 'desafío', 'tiempo', 'deforestación', 'contaminación']...\n",
      "   Verbos: ['afectar', 'ofrecer', 'proteger', 'conservar', 'preservar']\n",
      "   Adjetivos: ['climático', 'mayor', 'ambiental', 'renovable', 'solar']...\n",
      "   Adverbios: ['gravemente']\n",
      "   Otros: ['méxico', 'deber']\n",
      "\n",
      "6. TÉRMINOS AMBIENTALES IDENTIFICADOS:\n",
      "   Problemas Ambientales: ['cambio climático', 'deforestación', 'contaminación']\n",
      "   Soluciones: ['sostenible']\n",
      "   Recursos Naturales: ['biodiversidad', 'ecosistema']\n",
      "   Energia: ['solar', 'eólica']\n",
      "   Conservacion: ['reserva']\n",
      "\n",
      "7. ESTADÍSTICAS DE TOKENIZACIÓN:\n",
      "   Total Characters: 428\n",
      "   Total Words: 63\n",
      "   Total Sentences: 5\n",
      "   Total Paragraphs: 2\n",
      "   Unique Words: 47\n",
      "   Avg Word Length: 6.22\n",
      "   Avg Sentence Length: 11.60\n",
      "   Pos Distribution: {'sustantivos': 13, 'verbos': 5, 'adjetivos': 11, 'adverbios': 1, 'preposiciones': 0, 'conjunciones': 0, 'determinantes': 0, 'pronombres': 0, 'otros': 2}\n",
      "   Environmental Terms Count: 9\n",
      "   Entities Found: 2\n",
      "   Lexical Diversity: 0.75\n"
     ]
    }
   ],
   "source": [
    "# ===== MÓDULO 3: TOKENIZACIÓN =====\n",
    "\n",
    "class TextTokenizer:\n",
    "    \"\"\"\n",
    "    Módulo para la tokenización del texto.\n",
    "    Divide el texto en tokens (palabras, oraciones, párrafos) usando diferentes estrategias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Configurar tokenizadores\n",
    "        self.nlp = nlp  # spaCy model cargado anteriormente\n",
    "        \n",
    "        # Patrones para tokenización personalizada\n",
    "        self.sentence_endings = re.compile(r'[.!?]+')\n",
    "        self.word_pattern = re.compile(r'\\b\\w+\\b')\n",
    "        self.punctuation_pattern = re.compile(r'[^\\w\\s]')\n",
    "        \n",
    "    def tokenize_sentences(self, text: str, method: str = 'nltk') -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokeniza el texto en oraciones\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a tokenizar\n",
    "            method: Método a usar ('nltk', 'spacy', 'regex')\n",
    "        \n",
    "        Returns:\n",
    "            Lista de oraciones\n",
    "        \"\"\"\n",
    "        if method == 'nltk':\n",
    "            sentences = sent_tokenize(text, language='spanish')\n",
    "        elif method == 'spacy':\n",
    "            doc = self.nlp(text)\n",
    "            sentences = [sent.text.strip() for sent in doc.sents]\n",
    "        elif method == 'regex':\n",
    "            # Método simple con regex\n",
    "            sentences = self.sentence_endings.split(text)\n",
    "            sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        else:\n",
    "            raise ValueError(\"Método debe ser 'nltk', 'spacy' o 'regex'\")\n",
    "        \n",
    "        return [s for s in sentences if len(s.strip()) > 0]\n",
    "    \n",
    "    def tokenize_words(self, text: str, method: str = 'nltk') -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokeniza el texto en palabras\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a tokenizar\n",
    "            method: Método a usar ('nltk', 'spacy', 'regex')\n",
    "        \n",
    "        Returns:\n",
    "            Lista de palabras\n",
    "        \"\"\"\n",
    "        if method == 'nltk':\n",
    "            words = word_tokenize(text, language='spanish')\n",
    "        elif method == 'spacy':\n",
    "            doc = self.nlp(text)\n",
    "            words = [token.text for token in doc if not token.is_space]\n",
    "        elif method == 'regex':\n",
    "            words = self.word_pattern.findall(text)\n",
    "        else:\n",
    "            raise ValueError(\"Método debe ser 'nltk', 'spacy' o 'regex'\")\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def tokenize_paragraphs(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokeniza el texto en párrafos\n",
    "        \"\"\"\n",
    "        # Dividir por dobles saltos de línea o líneas vacías\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        return [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    def advanced_tokenization(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Tokenización avanzada usando spaCy con información lingüística\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con tokens y sus propiedades\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        tokens_info = []\n",
    "        for token in doc:\n",
    "            if not token.is_space:\n",
    "                token_info = {\n",
    "                    'text': token.text,\n",
    "                    'lemma': token.lemma_,\n",
    "                    'pos': token.pos_,  # Part of speech\n",
    "                    'tag': token.tag_,  # Detailed POS tag\n",
    "                    'is_alpha': token.is_alpha,\n",
    "                    'is_stop': token.is_stop,\n",
    "                    'is_punct': token.is_punct,\n",
    "                    'is_digit': token.is_digit,\n",
    "                    'shape': token.shape_,  # Forma de la palabra (Xxxx, dddd, etc.)\n",
    "                    'is_title': token.is_title,\n",
    "                    'is_lower': token.is_lower,\n",
    "                    'is_upper': token.is_upper\n",
    "                }\n",
    "                tokens_info.append(token_info)\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens_info,\n",
    "            'total_tokens': len(tokens_info),\n",
    "            'sentences': [sent.text for sent in doc.sents],\n",
    "            'entities': [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        }\n",
    "    \n",
    "    def tokenize_by_pos(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Tokeniza y agrupa por categorías gramaticales\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con tokens agrupados por POS\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        pos_groups = {\n",
    "            'sustantivos': [],\n",
    "            'verbos': [],\n",
    "            'adjetivos': [],\n",
    "            'adverbios': [],\n",
    "            'preposiciones': [],\n",
    "            'conjunciones': [],\n",
    "            'determinantes': [],\n",
    "            'pronombres': [],\n",
    "            'otros': []\n",
    "        }\n",
    "        \n",
    "        pos_mapping = {\n",
    "            'NOUN': 'sustantivos',\n",
    "            'VERB': 'verbos',\n",
    "            'ADJ': 'adjetivos',\n",
    "            'ADV': 'adverbios',\n",
    "            'ADP': 'preposiciones',\n",
    "            'CONJ': 'conjunciones',\n",
    "            'CCONJ': 'conjunciones',\n",
    "            'DET': 'determinantes',\n",
    "            'PRON': 'pronombres'\n",
    "        }\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.is_alpha and not token.is_stop:\n",
    "                category = pos_mapping.get(token.pos_, 'otros')\n",
    "                pos_groups[category].append(token.lemma_.lower())\n",
    "        \n",
    "        # Eliminar duplicados manteniendo orden\n",
    "        for category in pos_groups:\n",
    "            pos_groups[category] = list(dict.fromkeys(pos_groups[category]))\n",
    "        \n",
    "        return pos_groups\n",
    "    \n",
    "    def extract_environmental_terms(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Extrae términos específicos relacionados con medio ambiente\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        # Términos ambientales por categoría\n",
    "        environmental_categories = {\n",
    "            'problemas_ambientales': [],\n",
    "            'soluciones': [],\n",
    "            'recursos_naturales': [],\n",
    "            'energia': [],\n",
    "            'contaminacion': [],\n",
    "            'conservacion': []\n",
    "        }\n",
    "        \n",
    "        # Diccionarios de términos ambientales\n",
    "        environmental_terms = {\n",
    "            'problemas_ambientales': [\n",
    "                'cambio climático', 'calentamiento global', 'deforestación', \n",
    "                'contaminación', 'extinción', 'desertificación', 'erosión'\n",
    "            ],\n",
    "            'soluciones': [\n",
    "                'reciclaje', 'energía renovable', 'sostenible', 'sustentable',\n",
    "                'conservación', 'reforestación', 'eficiencia energética'\n",
    "            ],\n",
    "            'recursos_naturales': [\n",
    "                'agua', 'bosque', 'océano', 'biodiversidad', 'ecosistema',\n",
    "                'fauna', 'flora', 'selva', 'río', 'lago'\n",
    "            ],\n",
    "            'energia': [\n",
    "                'solar', 'eólica', 'hidroeléctrica', 'geotérmica', \n",
    "                'biomasa', 'combustible fósil', 'petróleo', 'carbón'\n",
    "            ],\n",
    "            'contaminacion': [\n",
    "                'emisiones', 'gases', 'residuos', 'basura', 'tóxicos',\n",
    "                'plástico', 'químicos', 'desechos'\n",
    "            ],\n",
    "            'conservacion': [\n",
    "                'protección', 'preservación', 'área protegida', 'parque nacional',\n",
    "                'reserva', 'santuario', 'hábitat'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for category, terms in environmental_terms.items():\n",
    "            for term in terms:\n",
    "                if term in text_lower:\n",
    "                    environmental_categories[category].append(term)\n",
    "        \n",
    "        return environmental_categories\n",
    "    \n",
    "    def tokenization_statistics(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Genera estadísticas completas de tokenización\n",
    "        \"\"\"\n",
    "        sentences = self.tokenize_sentences(text, 'spacy')\n",
    "        words = self.tokenize_words(text, 'spacy')\n",
    "        paragraphs = self.tokenize_paragraphs(text)\n",
    "        advanced_tokens = self.advanced_tokenization(text)\n",
    "        pos_groups = self.tokenize_by_pos(text)\n",
    "        env_terms = self.extract_environmental_terms(text)\n",
    "        \n",
    "        # Calcular estadísticas\n",
    "        word_lengths = [len(word) for word in words if word.isalpha()]\n",
    "        sentence_lengths = [len(sent.split()) for sent in sentences]\n",
    "        \n",
    "        stats = {\n",
    "            'total_characters': len(text),\n",
    "            'total_words': len(words),\n",
    "            'total_sentences': len(sentences),\n",
    "            'total_paragraphs': len(paragraphs),\n",
    "            'unique_words': len(set(word.lower() for word in words if word.isalpha())),\n",
    "            'avg_word_length': np.mean(word_lengths) if word_lengths else 0,\n",
    "            'avg_sentence_length': np.mean(sentence_lengths) if sentence_lengths else 0,\n",
    "            'pos_distribution': {k: len(v) for k, v in pos_groups.items()},\n",
    "            'environmental_terms_count': sum(len(terms) for terms in env_terms.values()),\n",
    "            'entities_found': len(advanced_tokens['entities']),\n",
    "            'lexical_diversity': len(set(word.lower() for word in words if word.isalpha())) / len(words) if words else 0\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Ejemplo de uso del módulo de tokenización\n",
    "print(\"=== MÓDULO 3: TOKENIZACIÓN ===\")\n",
    "\n",
    "# Crear instancia del tokenizador\n",
    "tokenizer = TextTokenizer()\n",
    "\n",
    "# Texto de ejemplo sobre medio ambiente\n",
    "sample_text = \"\"\"\n",
    "El cambio climático es uno de los mayores desafíos ambientales de nuestro tiempo. \n",
    "La deforestación y la contaminación del aire están afectando gravemente los ecosistemas.\n",
    "\n",
    "Las energías renovables como la solar y eólica ofrecen soluciones sostenibles. \n",
    "Es fundamental proteger la biodiversidad y conservar los recursos naturales para las futuras generaciones.\n",
    "México tiene importantes reservas naturales que debemos preservar.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texto original:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Tokenización básica\n",
    "print(\"1. TOKENIZACIÓN EN ORACIONES:\")\n",
    "sentences = tokenizer.tokenize_sentences(sample_text, 'spacy')\n",
    "for i, sentence in enumerate(sentences, 1):\n",
    "    print(f\"   {i}. {sentence}\")\n",
    "\n",
    "print(f\"\\n2. TOKENIZACIÓN EN PALABRAS (primeras 20):\")\n",
    "words = tokenizer.tokenize_words(sample_text, 'spacy')\n",
    "print(f\"   {words[:20]}...\")\n",
    "print(f\"   Total de palabras: {len(words)}\")\n",
    "\n",
    "print(f\"\\n3. TOKENIZACIÓN EN PÁRRAFOS:\")\n",
    "paragraphs = tokenizer.tokenize_paragraphs(sample_text)\n",
    "for i, paragraph in enumerate(paragraphs, 1):\n",
    "    print(f\"   Párrafo {i}: {paragraph[:50]}...\")\n",
    "\n",
    "# Tokenización avanzada\n",
    "print(f\"\\n4. TOKENIZACIÓN AVANZADA:\")\n",
    "advanced = tokenizer.advanced_tokenization(sample_text)\n",
    "print(f\"   Total de tokens: {advanced['total_tokens']}\")\n",
    "print(f\"   Entidades encontradas: {advanced['entities']}\")\n",
    "\n",
    "# Agrupación por categorías gramaticales\n",
    "print(f\"\\n5. AGRUPACIÓN POR CATEGORÍAS GRAMATICALES:\")\n",
    "pos_groups = tokenizer.tokenize_by_pos(sample_text)\n",
    "for category, terms in pos_groups.items():\n",
    "    if terms:\n",
    "        print(f\"   {category.capitalize()}: {terms[:5]}{'...' if len(terms) > 5 else ''}\")\n",
    "\n",
    "# Términos ambientales\n",
    "print(f\"\\n6. TÉRMINOS AMBIENTALES IDENTIFICADOS:\")\n",
    "env_terms = tokenizer.extract_environmental_terms(sample_text)\n",
    "for category, terms in env_terms.items():\n",
    "    if terms:\n",
    "        print(f\"   {category.replace('_', ' ').title()}: {terms}\")\n",
    "\n",
    "# Estadísticas completas\n",
    "print(f\"\\n7. ESTADÍSTICAS DE TOKENIZACIÓN:\")\n",
    "stats = tokenizer.tokenization_statistics(sample_text)\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"   {key.replace('_', ' ').title()}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"   {key.replace('_', ' ').title()}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 4: Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÓDULO 4: NORMALIZACIÓN (CON TOKENS) ===\n",
      "Texto original:\n",
      "\n",
      "El Dr. García explicó q el CO2 y otros GEI están causando el CALENTAMIENTO GLOBAL.\n",
      "Las ONGs trabajan pa proteger la biodiversidad. Aprox. el 30% de los bosques\n",
      "han sido deforestados. ¿Cuándo tomaremos acción? ¡Es urgente!\n",
      "\n",
      "Necesitamos energías renovables como la solar y eólica... También debemos\n",
      "reducir nuestro \"carbon footprint\" y promover el \"sustainable development\".\n",
      "Tres millones de hectáreas se pierden cada año.\n",
      "\n",
      "\n",
      "============================================================\n",
      "1. TOKENIZACIÓN:\n",
      "Tokens originales: ['El', 'Dr.', 'García', 'explicó', 'q', 'el', 'CO2', 'y', 'otros', 'GEI', 'están', 'causando', 'el', 'CALENTAMIENTO', 'GLOBAL', '.', 'Las', 'ONGs', 'trabajan', 'pa']...\n",
      "Total de tokens: 78\n",
      "\n",
      "2. NORMALIZACIÓN DE TOKENS:\n",
      "Tokens normalizados: ['el', 'doctor', 'garcia', 'explico', 'que', 'el', 'dioxido', 'de', 'carbono', 'y', 'otros', 'gases', 'de', 'efecto', 'invernadero', 'estan', 'causando', 'el', 'calentamiento', 'global']...\n",
      "Total de tokens normalizados: 84\n",
      "Cambio en cantidad de tokens: 6\n",
      "Pasos aplicados: case_patterns, lowercase, contractions, abbreviations, environmental_terms, accents\n",
      "\n",
      "3. NORMALIZACIÓN DE TOKENIZACIÓN AVANZADA:\n",
      "Tokens avanzados normalizados: 84\n",
      "Ejemplos de tokens con información lingüística preservada:\n",
      "  Original: 'El' → Normalizado: 'el' (POS: DET)\n",
      "  Original: 'Dr.' → Normalizado: 'doctor' (POS: PROPN)\n",
      "  Original: 'García' → Normalizado: 'garcia' (POS: PROPN)\n",
      "  Original: 'explicó' → Normalizado: 'explico' (POS: VERB)\n",
      "  Original: 'q' → Normalizado: 'que' (POS: ADP)\n",
      "\n",
      "========================================\n",
      "4. NORMALIZACIÓN CON OPCIONES PERSONALIZADAS:\n",
      "Tokens con normalización personalizada:\n",
      "'['el', 'doctor', 'garcía', 'explicó', 'que', 'el', 'dióxido', 'de', 'carbono', 'y', 'otros', 'gases', 'de', 'efecto', 'invernadero', 'están', 'causando', 'el', 'calentamiento', 'global']...'\n",
      "Pasos aplicados: case_patterns, lowercase, contractions, abbreviations, environmental_terms, numbers\n",
      "\n",
      "============================================================\n",
      "5. INTEGRACIÓN COMPLETA:\n",
      "Oraciones tokenizadas: 7\n",
      "Oraciones normalizadas:\n",
      "  1. el doctor garcia explico que el dioxido de carbono y otros gases de efecto invernadero estan causando el calentamiento global .\n",
      "  2. las ongs trabajan para proteger la biodiversidad .\n",
      "  3. aprox .\n",
      "\n",
      "Texto final normalizado:\n",
      "'el doctor garcia explico que el dioxido de carbono y otros gases de efecto invernadero estan causando el calentamiento global . las ongs trabajan para proteger la biodiversidad . aprox . el 30% de los bosques han sido deforestados . ¿ cuando tomaremos accion ? ¡ es urgente ! necesitamos energias renovables como la solar y eolica ... tambien debemos reducir nuestro \" huella de carbono \" y promover el \" desarrollo sostenible \" . tres millones de hectareas se pierden cada ano .'\n"
     ]
    }
   ],
   "source": [
    "# ===== MÓDULO 4: NORMALIZACIÓN =====\n",
    "\n",
    "class TextNormalizer:\n",
    "    \"\"\"\n",
    "    Módulo para la normalización del texto.\n",
    "    Convierte texto a formas estándar: minúsculas, eliminación de acentos, \n",
    "    expansión de contracciones, normalización de números, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Diccionario de contracciones en español\n",
    "        self.contractions = {\n",
    "            'del': 'de el',\n",
    "            'al': 'a el',\n",
    "            'pa': 'para',\n",
    "            'pal': 'para el',\n",
    "            'pá': 'para',\n",
    "            'q': 'que',\n",
    "            'xq': 'porque',\n",
    "            'porq': 'porque',\n",
    "            'x': 'por',\n",
    "            'tb': 'también',\n",
    "            'tmb': 'también',\n",
    "            'tbn': 'también',\n",
    "            'pq': 'porque',\n",
    "            'xk': 'porque',\n",
    "            'k': 'que',\n",
    "            'dnd': 'donde',\n",
    "            'dónde': 'donde',\n",
    "            'cuándo': 'cuando',\n",
    "            'cómo': 'como',\n",
    "            'qué': 'que',\n",
    "            'cuál': 'cual',\n",
    "            'cuáles': 'cuales',\n",
    "            'quién': 'quien',\n",
    "            'quiénes': 'quienes'\n",
    "        }\n",
    "        \n",
    "        # Abreviaciones comunes\n",
    "        self.abbreviations = {\n",
    "            'dr.': 'doctor',\n",
    "            'dra.': 'doctora',\n",
    "            'sr.': 'señor',\n",
    "            'sra.': 'señora',\n",
    "            'srta.': 'señorita',\n",
    "            'prof.': 'profesor',\n",
    "            'profa.': 'profesora',\n",
    "            'ing.': 'ingeniero',\n",
    "            'lic.': 'licenciado',\n",
    "            'etc.': 'etcétera',\n",
    "            'vs.': 'versus',\n",
    "            'ej.': 'ejemplo',\n",
    "            'p.ej.': 'por ejemplo',\n",
    "            'i.e.': 'es decir',\n",
    "            'e.g.': 'por ejemplo',\n",
    "            'aprox.': 'aproximadamente',\n",
    "            'máx.': 'máximo',\n",
    "            'mín.': 'mínimo',\n",
    "            'kg.': 'kilogramos',\n",
    "            'km.': 'kilómetros',\n",
    "            'm.': 'metros',\n",
    "            'cm.': 'centímetros',\n",
    "            'mm.': 'milímetros',\n",
    "            'co2': 'dióxido de carbono',\n",
    "            'ong': 'organización no gubernamental',\n",
    "            'onu': 'organización de las naciones unidas'\n",
    "        }\n",
    "        \n",
    "        # Números escritos\n",
    "        self.number_words = {\n",
    "            'cero': '0', 'uno': '1', 'dos': '2', 'tres': '3', 'cuatro': '4',\n",
    "            'cinco': '5', 'seis': '6', 'siete': '7', 'ocho': '8', 'nueve': '9',\n",
    "            'diez': '10', 'once': '11', 'doce': '12', 'trece': '13', 'catorce': '14',\n",
    "            'quince': '15', 'dieciséis': '16', 'diecisiete': '17', 'dieciocho': '18',\n",
    "            'diecinueve': '19', 'veinte': '20', 'treinta': '30', 'cuarenta': '40',\n",
    "            'cincuenta': '50', 'sesenta': '60', 'setenta': '70', 'ochenta': '80',\n",
    "            'noventa': '90', 'cien': '100', 'mil': '1000', 'millón': '1000000'\n",
    "        }\n",
    "        \n",
    "        # Términos ambientales para normalización\n",
    "        self.env_normalizations = {\n",
    "            'co2': 'dióxido de carbono',\n",
    "            'co₂': 'dióxido de carbono',\n",
    "            'ch4': 'metano',\n",
    "            'ch₄': 'metano',\n",
    "            'n2o': 'óxido nitroso',\n",
    "            'n₂o': 'óxido nitroso',\n",
    "            'ghg': 'gases de efecto invernadero',\n",
    "            'gei': 'gases de efecto invernadero',\n",
    "            'renewable energy': 'energía renovable',\n",
    "            'green house': 'efecto invernadero',\n",
    "            'global warming': 'calentamiento global',\n",
    "            'climate change': 'cambio climático',\n",
    "            'sustainable development': 'desarrollo sostenible',\n",
    "            'carbon footprint': 'huella de carbono',\n",
    "            'biodiversity': 'biodiversidad',\n",
    "            'ecosystem': 'ecosistema',\n",
    "            'deforestation': 'deforestación',\n",
    "            'reforestation': 'reforestación'\n",
    "        }\n",
    "    \n",
    "    def to_lowercase(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Convierte tokens a minúsculas\"\"\"\n",
    "        return [token.lower() for token in tokens]\n",
    "    \n",
    "    def remove_accents(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Elimina acentos y diacríticos de los tokens\n",
    "        \"\"\"\n",
    "        normalized_tokens = []\n",
    "        for token in tokens:\n",
    "            # Normalizar usando NFD (Canonical Decomposition)\n",
    "            text_nfd = unicodedata.normalize('NFD', token)\n",
    "            # Filtrar caracteres diacríticos\n",
    "            text_without_accents = ''.join(\n",
    "                char for char in text_nfd \n",
    "                if unicodedata.category(char) != 'Mn'\n",
    "            )\n",
    "            normalized_tokens.append(text_without_accents)\n",
    "        return normalized_tokens\n",
    "    \n",
    "    def expand_contractions(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Expande contracciones comunes en español\n",
    "        \"\"\"\n",
    "        expanded_tokens = []\n",
    "        for token in tokens:\n",
    "            token_lower = token.lower()\n",
    "            # Buscar contracción exacta\n",
    "            if token_lower in self.contractions:\n",
    "                # Dividir la expansión en tokens si contiene espacios\n",
    "                expansion = self.contractions[token_lower]\n",
    "                if ' ' in expansion:\n",
    "                    expanded_tokens.extend(expansion.split())\n",
    "                else:\n",
    "                    expanded_tokens.append(expansion)\n",
    "            else:\n",
    "                expanded_tokens.append(token)\n",
    "        return expanded_tokens\n",
    "    \n",
    "    def expand_abbreviations(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Expande abreviaciones comunes\n",
    "        \"\"\"\n",
    "        expanded_tokens = []\n",
    "        for token in tokens:\n",
    "            token_lower = token.lower()\n",
    "            # Verificar si el token es una abreviación\n",
    "            if token_lower in self.abbreviations:\n",
    "                expansion = self.abbreviations[token_lower]\n",
    "                if ' ' in expansion:\n",
    "                    expanded_tokens.extend(expansion.split())\n",
    "                else:\n",
    "                    expanded_tokens.append(expansion)\n",
    "            else:\n",
    "                expanded_tokens.append(token)\n",
    "        return expanded_tokens\n",
    "    \n",
    "    def normalize_numbers(self, tokens: List[str], strategy: str = 'keep') -> List[str]:\n",
    "        \"\"\"\n",
    "        Normaliza números en los tokens\n",
    "        \n",
    "        Args:\n",
    "            tokens: Lista de tokens\n",
    "            strategy: 'keep', 'remove', 'words_to_digits', 'digits_to_words'\n",
    "        \"\"\"\n",
    "        if strategy == 'remove':\n",
    "            # Eliminar todos los tokens que son números\n",
    "            return [token for token in tokens if not token.isdigit()]\n",
    "        \n",
    "        elif strategy == 'words_to_digits':\n",
    "            # Convertir números escritos a dígitos\n",
    "            normalized_tokens = []\n",
    "            for token in tokens:\n",
    "                token_lower = token.lower()\n",
    "                if token_lower in self.number_words:\n",
    "                    normalized_tokens.append(self.number_words[token_lower])\n",
    "                else:\n",
    "                    normalized_tokens.append(token)\n",
    "            return normalized_tokens\n",
    "        \n",
    "        elif strategy == 'digits_to_words':\n",
    "            # Convertir dígitos simples a palabras (0-20)\n",
    "            digit_to_word = {v: k for k, v in self.number_words.items() if int(v) <= 20}\n",
    "            normalized_tokens = []\n",
    "            for token in tokens:\n",
    "                if token.isdigit() and token in digit_to_word:\n",
    "                    normalized_tokens.append(digit_to_word[token])\n",
    "                else:\n",
    "                    normalized_tokens.append(token)\n",
    "            return normalized_tokens\n",
    "        \n",
    "        else:  # 'keep'\n",
    "            return tokens\n",
    "    \n",
    "    def normalize_case_patterns(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Normaliza patrones de mayúsculas y minúsculas\n",
    "        \"\"\"\n",
    "        normalized_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.isupper() and len(token) > 1:\n",
    "                # Si toda la palabra está en mayúsculas, convertir a minúsculas\n",
    "                normalized_tokens.append(token.lower())\n",
    "            elif token.islower():\n",
    "                # Si está en minúsculas, mantener\n",
    "                normalized_tokens.append(token)\n",
    "            else:\n",
    "                # Casos mixtos, mantener como está\n",
    "                normalized_tokens.append(token)\n",
    "        return normalized_tokens\n",
    "    \n",
    "    def normalize_environmental_terms(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Normaliza términos específicos del dominio ambiental\n",
    "        \"\"\"\n",
    "        # Primero, reconstruir el texto para buscar términos compuestos\n",
    "        text = ' '.join(tokens).lower()\n",
    "        \n",
    "        # Aplicar normalizaciones\n",
    "        for term, normalized in self.env_normalizations.items():\n",
    "            text = re.sub(r'\\b' + re.escape(term) + r'\\b', normalized, text)\n",
    "        \n",
    "        # Volver a tokenizar\n",
    "        return text.split()\n",
    "    \n",
    "    def normalize_tokens(self, tokens: List[str], options: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Realiza normalización completa de los tokens\n",
    "        \n",
    "        Args:\n",
    "            tokens: Lista de tokens a normalizar\n",
    "            options: Diccionario con opciones de normalización\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con tokens normalizados y estadísticas\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            options = {\n",
    "                'to_lowercase': True,\n",
    "                'remove_accents': True,\n",
    "                'expand_contractions': True,\n",
    "                'expand_abbreviations': True,\n",
    "                'normalize_numbers': 'keep',\n",
    "                'normalize_case_patterns': True,\n",
    "                'normalize_environmental_terms': True\n",
    "            }\n",
    "        \n",
    "        original_tokens = tokens.copy()\n",
    "        steps_applied = []\n",
    "        \n",
    "        # Aplicar normalizaciones según opciones\n",
    "        if options.get('normalize_case_patterns', True):\n",
    "            tokens = self.normalize_case_patterns(tokens)\n",
    "            steps_applied.append('case_patterns')\n",
    "        \n",
    "        if options.get('to_lowercase', True):\n",
    "            tokens = self.to_lowercase(tokens)\n",
    "            steps_applied.append('lowercase')\n",
    "        \n",
    "        if options.get('expand_contractions', True):\n",
    "            tokens = self.expand_contractions(tokens)\n",
    "            steps_applied.append('contractions')\n",
    "        \n",
    "        if options.get('expand_abbreviations', True):\n",
    "            tokens = self.expand_abbreviations(tokens)\n",
    "            steps_applied.append('abbreviations')\n",
    "        \n",
    "        if options.get('normalize_environmental_terms', True):\n",
    "            tokens = self.normalize_environmental_terms(tokens)\n",
    "            steps_applied.append('environmental_terms')\n",
    "        \n",
    "        if options.get('normalize_numbers', 'keep') != 'keep':\n",
    "            tokens = self.normalize_numbers(tokens, options['normalize_numbers'])\n",
    "            steps_applied.append('numbers')\n",
    "        \n",
    "        if options.get('remove_accents', True):\n",
    "            tokens = self.remove_accents(tokens)\n",
    "            steps_applied.append('accents')\n",
    "        \n",
    "        return {\n",
    "            'original_tokens': original_tokens,\n",
    "            'normalized_tokens': tokens,\n",
    "            'original_count': len(original_tokens),\n",
    "            'normalized_count': len(tokens),\n",
    "            'token_count_change': len(tokens) - len(original_tokens),\n",
    "            'steps_applied': steps_applied,\n",
    "            'normalization_options': options\n",
    "        }\n",
    "    \n",
    "    def comprehensive_normalization(self, text: str, options: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Realiza normalización completa del texto (para compatibilidad con versiones anteriores)\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a normalizar\n",
    "            options: Diccionario con opciones de normalización\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con texto normalizado y estadísticas\n",
    "        \"\"\"\n",
    "        # Tokenizar el texto\n",
    "        tokens = text.split()\n",
    "        \n",
    "        # Normalizar los tokens\n",
    "        result = self.normalize_tokens(tokens, options)\n",
    "        \n",
    "        # Reconstruir el texto normalizado\n",
    "        normalized_text = ' '.join(result['normalized_tokens'])\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'normalized_text': normalized_text,\n",
    "            'original_word_count': result['original_count'],\n",
    "            'normalized_word_count': result['normalized_count'],\n",
    "            'word_count_change': result['token_count_change'],\n",
    "            'steps_applied': result['steps_applied'],\n",
    "            'normalization_options': result['normalization_options']\n",
    "        }\n",
    "    \n",
    "    def normalize_tokenized_text(self, tokenization_result: Dict, options: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Normaliza el resultado de la tokenización\n",
    "        \n",
    "        Args:\n",
    "            tokenization_result: Resultado del módulo de tokenización\n",
    "            options: Opciones de normalización\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con resultados normalizados\n",
    "        \"\"\"\n",
    "        # Extraer tokens del resultado de tokenización\n",
    "        if 'tokens' in tokenization_result:\n",
    "            # Si es resultado de advanced_tokenization\n",
    "            tokens = [token['text'] for token in tokenization_result['tokens']]\n",
    "        elif isinstance(tokenization_result, list):\n",
    "            # Si es una lista simple de tokens\n",
    "            tokens = tokenization_result\n",
    "        else:\n",
    "            # Intentar extraer palabras\n",
    "            tokens = tokenization_result.get('words', [])\n",
    "            if not tokens:\n",
    "                raise ValueError(\"No se pudieron extraer tokens del resultado de tokenización\")\n",
    "        \n",
    "        # Normalizar los tokens\n",
    "        normalization_result = self.normalize_tokens(tokens, options)\n",
    "        \n",
    "        # Si el resultado original incluye información POS, mantenerla\n",
    "        if 'tokens' in tokenization_result and isinstance(tokenization_result['tokens'], list):\n",
    "            # Crear un mapeo de tokens originales a normalizados\n",
    "            # (esto es una aproximación, ya que la normalización puede cambiar el número de tokens)\n",
    "            normalized_tokens_info = []\n",
    "            \n",
    "            # Intentar preservar información lingüística\n",
    "            for i, token_info in enumerate(tokenization_result['tokens']):\n",
    "                if i < len(normalization_result['normalized_tokens']):\n",
    "                    normalized_token = normalization_result['normalized_tokens'][i]\n",
    "                    normalized_tokens_info.append({\n",
    "                        **token_info,\n",
    "                        'original_text': token_info['text'],\n",
    "                        'text': normalized_token,\n",
    "                        'normalized': True\n",
    "                    })\n",
    "            \n",
    "            normalization_result['normalized_tokens_info'] = normalized_tokens_info\n",
    "        \n",
    "        return normalization_result\n",
    "\n",
    "# Ejemplo de uso del módulo de normalización con tokens\n",
    "print(\"=== MÓDULO 4: NORMALIZACIÓN (CON TOKENS) ===\")\n",
    "\n",
    "# Crear instancias de tokenizador y normalizador\n",
    "tokenizer = TextTokenizer()\n",
    "normalizer = TextNormalizer()\n",
    "\n",
    "# Texto de ejemplo con varios problemas de normalización\n",
    "sample_text = \"\"\"\n",
    "El Dr. García explicó q el CO2 y otros GEI están causando el CALENTAMIENTO GLOBAL.\n",
    "Las ONGs trabajan pa proteger la biodiversidad. Aprox. el 30% de los bosques\n",
    "han sido deforestados. ¿Cuándo tomaremos acción? ¡Es urgente!\n",
    "\n",
    "Necesitamos energías renovables como la solar y eólica... También debemos\n",
    "reducir nuestro \"carbon footprint\" y promover el \"sustainable development\".\n",
    "Tres millones de hectáreas se pierden cada año.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texto original:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Tokenizar primero\n",
    "print(\"1. TOKENIZACIÓN:\")\n",
    "tokens = tokenizer.tokenize_words(sample_text, 'spacy')\n",
    "print(f\"Tokens originales: {tokens[:20]}...\")\n",
    "print(f\"Total de tokens: {len(tokens)}\")\n",
    "\n",
    "# Normalizar los tokens\n",
    "print(\"\\n2. NORMALIZACIÓN DE TOKENS:\")\n",
    "normalization_result = normalizer.normalize_tokens(tokens)\n",
    "print(f\"Tokens normalizados: {normalization_result['normalized_tokens'][:20]}...\")\n",
    "print(f\"Total de tokens normalizados: {normalization_result['normalized_count']}\")\n",
    "print(f\"Cambio en cantidad de tokens: {normalization_result['token_count_change']}\")\n",
    "print(f\"Pasos aplicados: {', '.join(normalization_result['steps_applied'])}\")\n",
    "\n",
    "# Normalizar resultado de tokenización avanzada\n",
    "print(\"\\n3. NORMALIZACIÓN DE TOKENIZACIÓN AVANZADA:\")\n",
    "advanced_tokens = tokenizer.advanced_tokenization(sample_text)\n",
    "advanced_normalization = normalizer.normalize_tokenized_text(advanced_tokens)\n",
    "\n",
    "print(f\"Tokens avanzados normalizados: {len(advanced_normalization['normalized_tokens'])}\")\n",
    "if 'normalized_tokens_info' in advanced_normalization:\n",
    "    print(\"Ejemplos de tokens con información lingüística preservada:\")\n",
    "    for token_info in advanced_normalization['normalized_tokens_info'][:5]:\n",
    "        print(f\"  Original: '{token_info['original_text']}' → Normalizado: '{token_info['text']}' (POS: {token_info['pos']})\")\n",
    "\n",
    "# Ejemplo con opciones personalizadas\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"4. NORMALIZACIÓN CON OPCIONES PERSONALIZADAS:\")\n",
    "\n",
    "custom_options = {\n",
    "    'to_lowercase': True,\n",
    "    'remove_accents': False,  # Mantener acentos\n",
    "    'expand_contractions': True,\n",
    "    'expand_abbreviations': True,\n",
    "    'normalize_numbers': 'words_to_digits',  # Convertir números escritos a dígitos\n",
    "    'normalize_case_patterns': True,\n",
    "    'normalize_environmental_terms': True\n",
    "}\n",
    "\n",
    "custom_result = normalizer.normalize_tokens(tokens, custom_options)\n",
    "print(\"Tokens con normalización personalizada:\")\n",
    "print(f\"'{custom_result['normalized_tokens'][:20]}...'\")\n",
    "print(f\"Pasos aplicados: {', '.join(custom_result['steps_applied'])}\")\n",
    "\n",
    "# Integración completa de tokenización y normalización\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. INTEGRACIÓN COMPLETA:\")\n",
    "\n",
    "# Tokenizar por oraciones\n",
    "sentences = tokenizer.tokenize_sentences(sample_text, 'spacy')\n",
    "print(f\"Oraciones tokenizadas: {len(sentences)}\")\n",
    "\n",
    "# Normalizar cada oración\n",
    "normalized_sentences = []\n",
    "for sentence in sentences:\n",
    "    # Tokenizar la oración en palabras\n",
    "    sentence_tokens = tokenizer.tokenize_words(sentence, 'spacy')\n",
    "    # Normalizar los tokens\n",
    "    normalized_result = normalizer.normalize_tokens(sentence_tokens)\n",
    "    # Reconstruir la oración normalizada\n",
    "    normalized_sentence = ' '.join(normalized_result['normalized_tokens'])\n",
    "    normalized_sentences.append(normalized_sentence)\n",
    "\n",
    "print(\"Oraciones normalizadas:\")\n",
    "for i, sentence in enumerate(normalized_sentences[:3], 1):\n",
    "    print(f\"  {i}. {sentence}\")\n",
    "\n",
    "# Reconstruir texto completo normalizado\n",
    "final_text = ' '.join(normalized_sentences)\n",
    "print(\"\\nTexto final normalizado:\")\n",
    "print(f\"'{final_text}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 5: Eliminacion de Ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÓDULO 5: ELIMINACIÓN DE RUIDO ===\n",
      "Texto original con ruido:\n",
      "\n",
      "Bueno, pues, el tema del cambio climático es, eh, muy importante y fundamental.\n",
      "La cosa es que, mm, tenemos muchos problemas ambientales que son, así, bastante serios.\n",
      "Ok, entonces, la deforestación y la contaminación del aire son, pues, cuestiones\n",
      "que debemos, eh, abordar de manera urgente. ¿No? Sí, es algo muy necesario.\n",
      "\n",
      "Los aspectos más importantes incluyen: a) energías renovables, b) conservación,\n",
      "c) sostenibilidad. Etc, etc. Jaja, pero en serio, es un asunto muy general\n",
      "que requiere, mm, acciones específicas y particulares para el futuro.\n",
      "\n",
      "\n",
      "============================================================\n",
      "1. ELIMINACIÓN DE STOPWORDS:\n",
      "Bueno, pues, cambio climático es, eh, importante fundamental. que, mm, ambientales son, así, serios. Ok, entonces, deforestación contaminación aire so...\n",
      "\n",
      "2. ELIMINACIÓN DE PALABRAS DE RUIDO:\n",
      "Bueno, pues, cambio climático es, eh, importante fundamental. que, mm, ambientales son, así, serios. Ok, entonces, deforestación contaminación aire so...\n",
      "\n",
      "3. ELIMINACIÓN DE PALABRAS CORTAS:\n",
      "Bueno, pues, cambio climático es, eh, importante fundamental. que, mm, ambientales son, así, serios. Ok, entonces, deforestación contaminación aire so...\n",
      "\n",
      "4. ELIMINACIÓN DE RUIDO AMBIENTAL:\n",
      "Bueno, pues, cambio climático es, eh, fundamental. que, mm, ambientales son, así, serios. Ok, entonces, deforestación contaminación aire son, pues, de...\n",
      "\n",
      "5. ANÁLISIS POR FRECUENCIA:\n",
      "Palabras raras eliminadas: 35\n",
      "Palabras muy frecuentes eliminadas: 0\n",
      "Ejemplos de palabras raras: ['necesario.', 'sostenibilidad.', 'entonces,', 'renovables,', 'contaminación', 'así,', 'conservación,', 'abordar', 'energías', 'etc,']\n",
      "\n",
      "============================================================\n",
      "ELIMINACIÓN COMPLETA DE RUIDO:\n",
      "Texto completamente limpio:\n",
      "''\n",
      "\n",
      "Estadísticas de limpieza:\n",
      "- Palabras originales: 85\n",
      "- Palabras finales: 0\n",
      "- Palabras eliminadas: 85\n",
      "- Reducción: 100.0%\n",
      "- Pasos aplicados: stopwords, noise_words, short_words, environmental_noise, non_alphabetic, pos_filtering, frequency_filtering, advanced_removal\n",
      "\n",
      "========================================\n",
      "ANÁLISIS AVANZADO DE ELIMINACIÓN:\n",
      "Tokens originales: 127\n",
      "Tokens mantenidos: 32\n",
      "Tokens eliminados: 95\n",
      "\n",
      "Detalles de eliminación (primeros 10):\n",
      "  '\n",
      "' -> whitespace\n",
      "  'Bueno' -> stopword\n",
      "  ',' -> punctuation\n",
      "  'pues' -> stopword\n",
      "  ',' -> punctuation\n",
      "  'el' -> stopword\n",
      "  'del' -> stopword\n",
      "  'es' -> stopword\n",
      "  ',' -> punctuation\n",
      "  'eh' -> too_short\n"
     ]
    }
   ],
   "source": [
    "# ===== MÓDULO 5: ELIMINACIÓN DE RUIDO =====\n",
    "\n",
    "class NoiseRemover:\n",
    "    \"\"\"\n",
    "    Módulo para la eliminación de ruido del texto.\n",
    "    Elimina stopwords, palabras muy frecuentes/raras, contenido irrelevante\n",
    "    y ruido específico del dominio.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stopwords básicas en español\n",
    "        self.spanish_stopwords = set(stopwords.words('spanish'))\n",
    "        \n",
    "        # Stopwords adicionales personalizadas\n",
    "        self.custom_stopwords = {\n",
    "            # Conectores y muletillas\n",
    "            'pues', 'bueno', 'entonces', 'así', 'ahora', 'luego', 'después',\n",
    "            'antes', 'mientras', 'durante', 'mediante', 'según', 'incluso',\n",
    "            'además', 'también', 'tampoco', 'sino', 'aunque', 'sin embargo',\n",
    "            'no obstante', 'por tanto', 'por consiguiente', 'en consecuencia',\n",
    "            \n",
    "            # Palabras de relleno\n",
    "            'cosa', 'cosas', 'algo', 'nada', 'todo', 'todos', 'todas',\n",
    "            'mucho', 'muchos', 'muchas', 'poco', 'pocos', 'pocas',\n",
    "            'bastante', 'demasiado', 'suficiente',\n",
    "            \n",
    "            # Palabras muy generales\n",
    "            'forma', 'manera', 'modo', 'tipo', 'tipos', 'clase', 'clases',\n",
    "            'parte', 'partes', 'lado', 'lados', 'vez', 'veces', 'momento',\n",
    "            'momentos', 'tiempo', 'tiempos', 'lugar', 'lugares', 'caso', 'casos'\n",
    "        }\n",
    "        \n",
    "        # Stopwords específicas para contexto ambiental (palabras muy comunes que no aportan)\n",
    "        self.environmental_stopwords = {\n",
    "            'tema', 'temas', 'problema', 'problemas', 'situación', 'situaciones',\n",
    "            'aspecto', 'aspectos', 'factor', 'factores', 'elemento', 'elementos',\n",
    "            'punto', 'puntos', 'cuestión', 'cuestiones', 'asunto', 'asuntos'\n",
    "        }\n",
    "        \n",
    "        # Combinar todos los stopwords\n",
    "        self.all_stopwords = (self.spanish_stopwords | \n",
    "                             self.custom_stopwords | \n",
    "                             self.environmental_stopwords)\n",
    "        \n",
    "        # Patrones de ruido común\n",
    "        self.noise_patterns = [\n",
    "            r'\\b\\w{1,2}\\b',  # Palabras muy cortas (1-2 caracteres)\n",
    "            r'\\b\\d+\\b',      # Números sueltos (opcional)\n",
    "            r'[^\\w\\s]',      # Signos de puntuación sueltos\n",
    "            r'\\b[a-zA-Z]\\b', # Letras sueltas\n",
    "        ]\n",
    "        \n",
    "        # Palabras de ruido específicas\n",
    "        self.noise_words = {\n",
    "            'mm', 'hmm', 'eh', 'ah', 'oh', 'uh', 'um', 'er',\n",
    "            'ok', 'okay', 'si', 'no', 'ya', 'je', 'ja', 'jaja', 'jeje',\n",
    "            'etc', 'etcetera', 'bla', 'blah'\n",
    "        }\n",
    "    \n",
    "    def remove_stopwords(self, text: str, custom_stopwords: set = None) -> str:\n",
    "        \"\"\"\n",
    "        Elimina stopwords del texto\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a procesar\n",
    "            custom_stopwords: Stopwords adicionales personalizadas\n",
    "        \n",
    "        Returns:\n",
    "            Texto sin stopwords\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        stopwords_to_use = self.all_stopwords.copy()\n",
    "        \n",
    "        if custom_stopwords:\n",
    "            stopwords_to_use.update(custom_stopwords)\n",
    "        \n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word.lower() not in stopwords_to_use\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def remove_by_frequency(self, text: str, min_freq: int = 2, max_freq_ratio: float = 0.1) -> Dict:\n",
    "        \"\"\"\n",
    "        Elimina palabras muy raras o muy frecuentes\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a procesar\n",
    "            min_freq: Frecuencia mínima para mantener una palabra\n",
    "            max_freq_ratio: Ratio máximo de frecuencia (ej: 0.1 = 10% del total)\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con texto filtrado y estadísticas\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        word_freq = {}\n",
    "        \n",
    "        # Contar frecuencias\n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            word_freq[word_lower] = word_freq.get(word_lower, 0) + 1\n",
    "        \n",
    "        total_words = len(words)\n",
    "        max_freq = int(total_words * max_freq_ratio)\n",
    "        \n",
    "        # Identificar palabras a eliminar\n",
    "        rare_words = {word for word, freq in word_freq.items() if freq < min_freq}\n",
    "        frequent_words = {word for word, freq in word_freq.items() if freq > max_freq}\n",
    "        words_to_remove = rare_words | frequent_words\n",
    "        \n",
    "        # Filtrar palabras\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word.lower() not in words_to_remove\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'filtered_text': ' '.join(filtered_words),\n",
    "            'original_word_count': len(words),\n",
    "            'filtered_word_count': len(filtered_words),\n",
    "            'rare_words_removed': len(rare_words),\n",
    "            'frequent_words_removed': len(frequent_words),\n",
    "            'rare_words': list(rare_words)[:10],  # Mostrar solo las primeras 10\n",
    "            'frequent_words': list(frequent_words)\n",
    "        }\n",
    "    \n",
    "    def remove_noise_patterns(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Elimina patrones de ruido usando regex\n",
    "        \"\"\"\n",
    "        for pattern in self.noise_patterns:\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "        \n",
    "        # Normalizar espacios\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def remove_noise_words(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Elimina palabras de ruido específicas\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word.lower() not in self.noise_words\n",
    "        ]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def remove_short_words(self, text: str, min_length: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        Elimina palabras muy cortas\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if len(word) >= min_length\n",
    "        ]\n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def remove_non_alphabetic(self, text: str, keep_numbers: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Elimina tokens que no son alfabéticos\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        if keep_numbers:\n",
    "            # Mantener palabras alfabéticas y números\n",
    "            filtered_words = [\n",
    "                word for word in words \n",
    "                if word.isalpha() or word.isdigit()\n",
    "            ]\n",
    "        else:\n",
    "            # Solo mantener palabras alfabéticas\n",
    "            filtered_words = [\n",
    "                word for word in words \n",
    "                if word.isalpha()\n",
    "            ]\n",
    "        \n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def remove_by_pos(self, text: str, pos_to_remove: List[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Elimina palabras según su categoría gramatical\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a procesar\n",
    "            pos_to_remove: Lista de POS tags a eliminar\n",
    "        \"\"\"\n",
    "        if pos_to_remove is None:\n",
    "            # Por defecto, eliminar determinantes, preposiciones, conjunciones\n",
    "            pos_to_remove = ['DET', 'ADP', 'CONJ', 'CCONJ', 'SCONJ']\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        filtered_words = [\n",
    "            token.text for token in doc \n",
    "            if token.pos_ not in pos_to_remove and not token.is_space\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def remove_environmental_noise(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Elimina ruido específico del dominio ambiental\n",
    "        \"\"\"\n",
    "        # Palabras muy generales en contexto ambiental que no aportan información específica\n",
    "        env_noise = {\n",
    "            'importante', 'necesario', 'fundamental', 'esencial', 'básico',\n",
    "            'general', 'específico', 'particular', 'especial', 'normal',\n",
    "            'actual', 'presente', 'futuro', 'pasado', 'nuevo', 'viejo',\n",
    "            'grande', 'pequeño', 'mayor', 'menor', 'mejor', 'peor',\n",
    "            'bueno', 'malo', 'positivo', 'negativo', 'principal', 'secundario'\n",
    "        }\n",
    "        \n",
    "        words = text.split()\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word.lower() not in env_noise\n",
    "        ]\n",
    "        \n",
    "        return ' '.join(filtered_words)\n",
    "    \n",
    "    def advanced_noise_removal(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Eliminación avanzada de ruido usando spaCy\n",
    "        \"\"\"\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Criterios para mantener tokens\n",
    "        kept_tokens = []\n",
    "        removed_tokens = []\n",
    "        \n",
    "        for token in doc:\n",
    "            # Criterios para eliminar\n",
    "            should_remove = (\n",
    "                token.is_stop or           # Es stopword\n",
    "                token.is_punct or          # Es puntuación\n",
    "                token.is_space or          # Es espacio\n",
    "                len(token.text) < 3 or     # Muy corto\n",
    "                not token.is_alpha or      # No es alfabético\n",
    "                token.pos_ in ['DET', 'ADP', 'CONJ', 'CCONJ'] or  # POS irrelevantes\n",
    "                token.text.lower() in self.noise_words  # Palabras de ruido\n",
    "            )\n",
    "            \n",
    "            if should_remove:\n",
    "                removed_tokens.append({\n",
    "                    'text': token.text,\n",
    "                    'reason': self._get_removal_reason(token)\n",
    "                })\n",
    "            else:\n",
    "                kept_tokens.append(token.lemma_.lower())\n",
    "        \n",
    "        return {\n",
    "            'cleaned_text': ' '.join(kept_tokens),\n",
    "            'original_tokens': len(doc),\n",
    "            'kept_tokens': len(kept_tokens),\n",
    "            'removed_tokens': len(removed_tokens),\n",
    "            'removal_details': removed_tokens[:20]  # Mostrar solo los primeros 20\n",
    "        }\n",
    "    \n",
    "    def _get_removal_reason(self, token) -> str:\n",
    "        \"\"\"Determina la razón por la cual se elimina un token\"\"\"\n",
    "        if token.is_stop:\n",
    "            return 'stopword'\n",
    "        elif token.is_punct:\n",
    "            return 'punctuation'\n",
    "        elif token.is_space:\n",
    "            return 'whitespace'\n",
    "        elif len(token.text) < 3:\n",
    "            return 'too_short'\n",
    "        elif not token.is_alpha:\n",
    "            return 'non_alphabetic'\n",
    "        elif token.pos_ in ['DET', 'ADP', 'CONJ', 'CCONJ']:\n",
    "            return 'irrelevant_pos'\n",
    "        elif token.text.lower() in self.noise_words:\n",
    "            return 'noise_word'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    def comprehensive_noise_removal(self, text: str, options: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Eliminación completa de ruido con múltiples estrategias\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a limpiar\n",
    "            options: Opciones de limpieza\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con texto limpio y estadísticas\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            options = {\n",
    "                'remove_stopwords': True,\n",
    "                'remove_short_words': True,\n",
    "                'min_word_length': 3,\n",
    "                'remove_noise_words': True,\n",
    "                'remove_by_frequency': True,\n",
    "                'min_frequency': 2,\n",
    "                'max_frequency_ratio': 0.15,\n",
    "                'remove_non_alphabetic': True,\n",
    "                'keep_numbers': False,\n",
    "                'remove_by_pos': True,\n",
    "                'remove_environmental_noise': True,\n",
    "                'use_advanced_removal': True\n",
    "            }\n",
    "        \n",
    "        original_text = text\n",
    "        processing_steps = []\n",
    "        \n",
    "        # Aplicar eliminaciones según opciones\n",
    "        if options.get('remove_stopwords', True):\n",
    "            text = self.remove_stopwords(text)\n",
    "            processing_steps.append('stopwords')\n",
    "        \n",
    "        if options.get('remove_noise_words', True):\n",
    "            text = self.remove_noise_words(text)\n",
    "            processing_steps.append('noise_words')\n",
    "        \n",
    "        if options.get('remove_short_words', True):\n",
    "            min_length = options.get('min_word_length', 3)\n",
    "            text = self.remove_short_words(text, min_length)\n",
    "            processing_steps.append('short_words')\n",
    "        \n",
    "        if options.get('remove_environmental_noise', True):\n",
    "            text = self.remove_environmental_noise(text)\n",
    "            processing_steps.append('environmental_noise')\n",
    "        \n",
    "        if options.get('remove_non_alphabetic', True):\n",
    "            keep_nums = options.get('keep_numbers', False)\n",
    "            text = self.remove_non_alphabetic(text, keep_nums)\n",
    "            processing_steps.append('non_alphabetic')\n",
    "        \n",
    "        if options.get('remove_by_pos', True):\n",
    "            text = self.remove_by_pos(text)\n",
    "            processing_steps.append('pos_filtering')\n",
    "        \n",
    "        # Eliminación por frecuencia (al final para tener estadísticas correctas)\n",
    "        freq_result = None\n",
    "        if options.get('remove_by_frequency', True):\n",
    "            min_freq = options.get('min_frequency', 2)\n",
    "            max_ratio = options.get('max_frequency_ratio', 0.15)\n",
    "            freq_result = self.remove_by_frequency(text, min_freq, max_ratio)\n",
    "            text = freq_result['filtered_text']\n",
    "            processing_steps.append('frequency_filtering')\n",
    "        \n",
    "        # Eliminación avanzada (opcional)\n",
    "        advanced_result = None\n",
    "        if options.get('use_advanced_removal', True):\n",
    "            advanced_result = self.advanced_noise_removal(text)\n",
    "            text = advanced_result['cleaned_text']\n",
    "            processing_steps.append('advanced_removal')\n",
    "        \n",
    "        # Calcular estadísticas finales\n",
    "        original_words = len(original_text.split())\n",
    "        final_words = len(text.split())\n",
    "        reduction_percentage = ((original_words - final_words) / original_words) * 100 if original_words > 0 else 0\n",
    "        \n",
    "        result = {\n",
    "            'original_text': original_text,\n",
    "            'cleaned_text': text,\n",
    "            'original_word_count': original_words,\n",
    "            'final_word_count': final_words,\n",
    "            'words_removed': original_words - final_words,\n",
    "            'reduction_percentage': round(reduction_percentage, 2),\n",
    "            'processing_steps': processing_steps,\n",
    "            'options_used': options\n",
    "        }\n",
    "        \n",
    "        # Agregar resultados específicos si están disponibles\n",
    "        if freq_result:\n",
    "            result['frequency_analysis'] = freq_result\n",
    "        if advanced_result:\n",
    "            result['advanced_analysis'] = advanced_result\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Ejemplo de uso del módulo de eliminación de ruido\n",
    "print(\"=== MÓDULO 5: ELIMINACIÓN DE RUIDO ===\")\n",
    "\n",
    "# Crear instancia del eliminador de ruido\n",
    "noise_remover = NoiseRemover()\n",
    "\n",
    "# Texto de ejemplo con mucho ruido\n",
    "noisy_text = \"\"\"\n",
    "Bueno, pues, el tema del cambio climático es, eh, muy importante y fundamental.\n",
    "La cosa es que, mm, tenemos muchos problemas ambientales que son, así, bastante serios.\n",
    "Ok, entonces, la deforestación y la contaminación del aire son, pues, cuestiones\n",
    "que debemos, eh, abordar de manera urgente. ¿No? Sí, es algo muy necesario.\n",
    "\n",
    "Los aspectos más importantes incluyen: a) energías renovables, b) conservación,\n",
    "c) sostenibilidad. Etc, etc. Jaja, pero en serio, es un asunto muy general\n",
    "que requiere, mm, acciones específicas y particulares para el futuro.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texto original con ruido:\")\n",
    "print(noisy_text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Eliminación paso a paso\n",
    "print(\"1. ELIMINACIÓN DE STOPWORDS:\")\n",
    "no_stopwords = noise_remover.remove_stopwords(noisy_text)\n",
    "print(no_stopwords[:150] + \"...\")\n",
    "\n",
    "print(\"\\n2. ELIMINACIÓN DE PALABRAS DE RUIDO:\")\n",
    "no_noise_words = noise_remover.remove_noise_words(no_stopwords)\n",
    "print(no_noise_words[:150] + \"...\")\n",
    "\n",
    "print(\"\\n3. ELIMINACIÓN DE PALABRAS CORTAS:\")\n",
    "no_short = noise_remover.remove_short_words(no_noise_words, min_length=3)\n",
    "print(no_short[:150] + \"...\")\n",
    "\n",
    "print(\"\\n4. ELIMINACIÓN DE RUIDO AMBIENTAL:\")\n",
    "no_env_noise = noise_remover.remove_environmental_noise(no_short)\n",
    "print(no_env_noise[:150] + \"...\")\n",
    "\n",
    "# Análisis por frecuencia\n",
    "print(\"\\n5. ANÁLISIS POR FRECUENCIA:\")\n",
    "freq_analysis = noise_remover.remove_by_frequency(no_env_noise, min_freq=2, max_freq_ratio=0.1)\n",
    "print(f\"Palabras raras eliminadas: {freq_analysis['rare_words_removed']}\")\n",
    "print(f\"Palabras muy frecuentes eliminadas: {freq_analysis['frequent_words_removed']}\")\n",
    "print(f\"Ejemplos de palabras raras: {freq_analysis['rare_words']}\")\n",
    "\n",
    "# Eliminación completa\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ELIMINACIÓN COMPLETA DE RUIDO:\")\n",
    "\n",
    "result = noise_remover.comprehensive_noise_removal(noisy_text)\n",
    "\n",
    "print(\"Texto completamente limpio:\")\n",
    "print(f\"'{result['cleaned_text']}'\")\n",
    "\n",
    "print(f\"\\nEstadísticas de limpieza:\")\n",
    "print(f\"- Palabras originales: {result['original_word_count']}\")\n",
    "print(f\"- Palabras finales: {result['final_word_count']}\")\n",
    "print(f\"- Palabras eliminadas: {result['words_removed']}\")\n",
    "print(f\"- Reducción: {result['reduction_percentage']}%\")\n",
    "print(f\"- Pasos aplicados: {', '.join(result['processing_steps'])}\")\n",
    "\n",
    "# Eliminación avanzada con spaCy\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"ANÁLISIS AVANZADO DE ELIMINACIÓN:\")\n",
    "\n",
    "advanced_result = noise_remover.advanced_noise_removal(noisy_text)\n",
    "print(f\"Tokens originales: {advanced_result['original_tokens']}\")\n",
    "print(f\"Tokens mantenidos: {advanced_result['kept_tokens']}\")\n",
    "print(f\"Tokens eliminados: {advanced_result['removed_tokens']}\")\n",
    "\n",
    "print(\"\\nDetalles de eliminación (primeros 10):\")\n",
    "for detail in advanced_result['removal_details'][:10]:\n",
    "    print(f\"  '{detail['text']}' -> {detail['reason']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 6: Lematizacion y Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÓDULO 6: LEMATIZACIÓN Y STEMMING ===\n",
      "Texto original:\n",
      "\n",
      "Las contaminaciones ambientales están afectando los ecosistemas naturales.\n",
      "Las energías renovables ofrecen soluciones sostenibles para reducir las emisiones.\n",
      "Los científicos estudian la biodiversidad en diferentes regiones protegidas.\n",
      "Las deforestaciones masivas causan cambios climáticos irreversibles.\n",
      "Necesitamos implementar tecnologías innovadoras para la conservación.\n",
      "\n",
      "\n",
      "============================================================\n",
      "1. LEMATIZACIÓN CON SPACY:\n",
      "Texto lematizado: el contaminación ambiental estar afectar el ecosistema natural el energía renovable ofrecer solución sostenible para reducir el emisión el científico estudiar el biodiversidad en diferente región protegido el deforestación masivo causar cambio climático irreversible necesitar implementar tecnología innovadora para el conservación\n",
      "Cambios realizados: 34\n",
      "Ejemplos de cambios:\n",
      "  'Las' → 'el' (DET)\n",
      "  'contaminaciones' → 'contaminación' (NOUN)\n",
      "  'ambientales' → 'ambiental' (ADJ)\n",
      "  'están' → 'estar' (AUX)\n",
      "  'afectando' → 'afectar' (VERB)\n",
      "\n",
      "========================================\n",
      "2. STEMMING CON SNOWBALL:\n",
      "Texto stemmed: las contamin ambiental estan afect los ecosistem naturales. las energ renov ofrec solucion sosten par reduc las emisiones. los cientif estudi la biodivers en diferent region protegidas. las deforest masiv caus cambi climat irreversibles. necesit implement tecnolog innov par la conservación.\n",
      "Cambios realizados: 27\n",
      "Ejemplos de cambios:\n",
      "  'contaminaciones' → 'contamin'\n",
      "  'ambientales' → 'ambiental'\n",
      "  'están' → 'estan'\n",
      "  'afectando' → 'afect'\n",
      "  'ecosistemas' → 'ecosistem'\n",
      "\n",
      "========================================\n",
      "3. ENFOQUE HÍBRIDO:\n",
      "Texto procesado: el contaminación ambiental estan afect el ecosistema natural el energía renovable ofrec solución sostenible para reduc el emisión el científico estudi el biodiversidad en diferente región protegido el deforestación masivo caus cambio climático irreversible necesit implement tecnología innovadora para el conservación\n",
      "Cambios realizados: 36\n",
      "Ejemplos de cambios:\n",
      "  'Las' → 'el' (lemmatization)\n",
      "  'contaminaciones' → 'contaminación' (custom_lemma)\n",
      "  'ambientales' → 'ambiental' (lemmatization)\n",
      "  'están' → 'estan' (stemming)\n",
      "  'afectando' → 'afect' (stemming)\n",
      "\n",
      "============================================================\n",
      "4. COMPARACIÓN DE MÉTODOS:\n",
      "Tamaños de vocabulario:\n",
      "  Original: 35 palabras únicas\n",
      "  Spacy: 33 palabras únicas\n",
      "  Snowball: 35 palabras únicas\n",
      "  Hybrid: 33 palabras únicas\n",
      "\n",
      "Porcentajes de reducción:\n",
      "  Spacy: 5.71%\n",
      "  Snowball: 0.0%\n",
      "  Hybrid: 5.71%\n",
      "\n",
      "Método más agresivo: spacy\n",
      "Método más conservador: original\n",
      "Método recomendado: spacy\n",
      "\n",
      "========================================\n",
      "5. EVALUACIÓN DE TÉRMINOS AMBIENTALES:\n",
      "Términos de prueba: ['contaminaciones', 'deforestación', 'sostenibilidad'] ...\n",
      "\n",
      "SpaCy output: contaminación deforestación sostenibilidad biodiversidad energía renovable combustible fósil emisión...\n",
      "Snowball output: contamin deforest sostenibil biodivers energ renov combust fosil emision toxic conserv ambiental cal...\n",
      "Hybrid output: contaminación deforestación sostenibilidad biodiversidad energía renovable combustible fósil emisión...\n",
      "\n",
      "Preservación de significado:\n",
      "  spacy_preserves_meaning: 100.0%\n",
      "  snowball_preserves_meaning: 100.0%\n",
      "  hybrid_preserves_meaning: 100.0%\n",
      "\n",
      "============================================================\n",
      "6. PROCESAMIENTO RECOMENDADO (spaCy):\n",
      "Texto final procesado:\n",
      "'el contaminación ambiental estar afectar ecosistema natural energía renovable ofrecer solución sostenible para reducir emisión científico estudiar biodiversidad en diferente región protegido deforestación masivo causar cambio climático irreversible necesitar implementar tecnología innovadora conservación'\n",
      "\n",
      "Estadísticas finales:\n",
      "- Tokens originales: 41\n",
      "- Tokens procesados: 41\n",
      "- Cambios realizados: 34\n"
     ]
    }
   ],
   "source": [
    "# ===== MÓDULO 6: LEMATIZACIÓN Y STEMMING =====\n",
    "\n",
    "class TextLemmatizer:\n",
    "    \"\"\"\n",
    "    Módulo para lematización y stemming del texto.\n",
    "    Reduce las palabras a su forma base usando diferentes estrategias.\n",
    "    Incluye evaluación comparativa entre métodos.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Configurar herramientas\n",
    "        self.nlp = nlp  # spaCy model\n",
    "        self.stemmer = SnowballStemmer('spanish')\n",
    "        self.nltk_lemmatizer = WordNetLemmatizer()  # Para comparación\n",
    "        \n",
    "        # Diccionario de lemas personalizados para términos ambientales\n",
    "        self.environmental_lemmas = {\n",
    "            'contaminaciones': 'contaminación',\n",
    "            'deforestaciones': 'deforestación',\n",
    "            'reforestaciones': 'reforestación',\n",
    "            'sostenibilidades': 'sostenibilidad',\n",
    "            'biodiversidades': 'biodiversidad',\n",
    "            'ecosistemas': 'ecosistema',\n",
    "            'energías': 'energía',\n",
    "            'renovables': 'renovable',\n",
    "            'combustibles': 'combustible',\n",
    "            'emisiones': 'emisión',\n",
    "            'residuos': 'residuo',\n",
    "            'desechos': 'desecho',\n",
    "            'conservaciones': 'conservación',\n",
    "            'preservaciones': 'preservación',\n",
    "            'protecciones': 'protección',\n",
    "            'calentamientos': 'calentamiento',\n",
    "            'cambios': 'cambio',\n",
    "            'efectos': 'efecto',\n",
    "            'impactos': 'impacto',\n",
    "            'consecuencias': 'consecuencia',\n",
    "            'soluciones': 'solución',\n",
    "            'alternativas': 'alternativa',\n",
    "            'tecnologías': 'tecnología',\n",
    "            'innovaciones': 'innovación'\n",
    "        }\n",
    "        \n",
    "        # Excepciones de stemming (palabras que no deben ser stemmed)\n",
    "        self.stemming_exceptions = {\n",
    "            'gases', 'atlas', 'crisis', 'análisis', 'síntesis',\n",
    "            'tesis', 'oasis', 'énfasis', 'paréntesis'\n",
    "        }\n",
    "    \n",
    "    def lemmatize_with_spacy(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Lematización usando spaCy (método recomendado)\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con texto lematizado y análisis detallado\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        lemmatized_tokens = []\n",
    "        lemmatization_details = []\n",
    "        \n",
    "        for token in doc:\n",
    "            if not token.is_space and not token.is_punct:\n",
    "                original = token.text\n",
    "                lemma = token.lemma_.lower()\n",
    "                \n",
    "                # Aplicar lemas personalizados si existen\n",
    "                if original.lower() in self.environmental_lemmas:\n",
    "                    lemma = self.environmental_lemmas[original.lower()]\n",
    "                \n",
    "                lemmatized_tokens.append(lemma)\n",
    "                \n",
    "                # Guardar detalles si hay cambio\n",
    "                if original.lower() != lemma:\n",
    "                    lemmatization_details.append({\n",
    "                        'original': original,\n",
    "                        'lemma': lemma,\n",
    "                        'pos': token.pos_,\n",
    "                        'change_type': self._get_change_type(original, lemma)\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'lemmatized_text': ' '.join(lemmatized_tokens),\n",
    "            'original_tokens': len([t for t in doc if not t.is_space and not t.is_punct]),\n",
    "            'lemmatized_tokens': len(lemmatized_tokens),\n",
    "            'changes_made': len(lemmatization_details),\n",
    "            'lemmatization_details': lemmatization_details,\n",
    "            'method': 'spacy'\n",
    "        }\n",
    "    \n",
    "    def stem_with_snowball(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Stemming usando SnowballStemmer\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con texto stemmed y análisis detallado\n",
    "        \"\"\"\n",
    "        words = text.split()\n",
    "        \n",
    "        stemmed_tokens = []\n",
    "        stemming_details = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word.isalpha():\n",
    "                original = word.lower()\n",
    "                \n",
    "                # Verificar excepciones\n",
    "                if original in self.stemming_exceptions:\n",
    "                    stemmed = original\n",
    "                else:\n",
    "                    stemmed = self.stemmer.stem(original)\n",
    "                \n",
    "                stemmed_tokens.append(stemmed)\n",
    "                \n",
    "                # Guardar detalles si hay cambio\n",
    "                if original != stemmed:\n",
    "                    stemming_details.append({\n",
    "                        'original': word,\n",
    "                        'stem': stemmed,\n",
    "                        'change_type': self._get_change_type(original, stemmed)\n",
    "                    })\n",
    "            else:\n",
    "                stemmed_tokens.append(word)\n",
    "        \n",
    "        return {\n",
    "            'stemmed_text': ' '.join(stemmed_tokens),\n",
    "            'original_tokens': len(words),\n",
    "            'stemmed_tokens': len(stemmed_tokens),\n",
    "            'changes_made': len(stemming_details),\n",
    "            'stemming_details': stemming_details,\n",
    "            'method': 'snowball'\n",
    "        }\n",
    "    \n",
    "    def hybrid_approach(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Enfoque híbrido: lematización para sustantivos/adjetivos, stemming para verbos\n",
    "        \"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        processed_tokens = []\n",
    "        processing_details = []\n",
    "        \n",
    "        for token in doc:\n",
    "            if not token.is_space and not token.is_punct and token.is_alpha:\n",
    "                original = token.text\n",
    "                \n",
    "                # Decidir método según POS\n",
    "                if token.pos_ in ['NOUN', 'ADJ', 'PROPN']:\n",
    "                    # Usar lematización para sustantivos y adjetivos\n",
    "                    processed = token.lemma_.lower()\n",
    "                    method_used = 'lemmatization'\n",
    "                    \n",
    "                    # Aplicar lemas personalizados\n",
    "                    if original.lower() in self.environmental_lemmas:\n",
    "                        processed = self.environmental_lemmas[original.lower()]\n",
    "                        method_used = 'custom_lemma'\n",
    "                        \n",
    "                elif token.pos_ in ['VERB', 'AUX']:\n",
    "                    # Usar stemming para verbos\n",
    "                    if original.lower() not in self.stemming_exceptions:\n",
    "                        processed = self.stemmer.stem(original.lower())\n",
    "                        method_used = 'stemming'\n",
    "                    else:\n",
    "                        processed = original.lower()\n",
    "                        method_used = 'exception'\n",
    "                else:\n",
    "                    # Para otras categorías, usar lematización\n",
    "                    processed = token.lemma_.lower()\n",
    "                    method_used = 'lemmatization'\n",
    "                \n",
    "                processed_tokens.append(processed)\n",
    "                \n",
    "                # Guardar detalles si hay cambio\n",
    "                if original.lower() != processed:\n",
    "                    processing_details.append({\n",
    "                        'original': original,\n",
    "                        'processed': processed,\n",
    "                        'pos': token.pos_,\n",
    "                        'method': method_used,\n",
    "                        'change_type': self._get_change_type(original, processed)\n",
    "                    })\n",
    "        \n",
    "        return {\n",
    "            'processed_text': ' '.join(processed_tokens),\n",
    "            'original_tokens': len([t for t in doc if not t.is_space and not t.is_punct and t.is_alpha]),\n",
    "            'processed_tokens': len(processed_tokens),\n",
    "            'changes_made': len(processing_details),\n",
    "            'processing_details': processing_details,\n",
    "            'method': 'hybrid'\n",
    "        }\n",
    "    \n",
    "    def _get_change_type(self, original: str, processed: str) -> str:\n",
    "        \"\"\"Determina el tipo de cambio realizado\"\"\"\n",
    "        if len(processed) < len(original):\n",
    "            return 'reduction'\n",
    "        elif len(processed) > len(original):\n",
    "            return 'expansion'\n",
    "        elif processed != original.lower():\n",
    "            return 'transformation'\n",
    "        else:\n",
    "            return 'no_change'\n",
    "    \n",
    "    def compare_methods(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Compara los tres métodos de procesamiento\n",
    "        \"\"\"\n",
    "        # Aplicar cada método\n",
    "        spacy_result = self.lemmatize_with_spacy(text)\n",
    "        snowball_result = self.stem_with_snowball(text)\n",
    "        hybrid_result = self.hybrid_approach(text)\n",
    "        \n",
    "        # Calcular métricas de comparación\n",
    "        original_words = set(text.lower().split())\n",
    "        spacy_words = set(spacy_result['lemmatized_text'].split())\n",
    "        snowball_words = set(snowball_result['stemmed_text'].split())\n",
    "        hybrid_words = set(hybrid_result['processed_text'].split())\n",
    "        \n",
    "        # Calcular reducción de vocabulario\n",
    "        vocab_reduction = {\n",
    "            'original': len(original_words),\n",
    "            'spacy': len(spacy_words),\n",
    "            'snowball': len(snowball_words),\n",
    "            'hybrid': len(hybrid_words)\n",
    "        }\n",
    "        \n",
    "        # Calcular porcentajes de reducción\n",
    "        reduction_percentages = {}\n",
    "        for method, vocab_size in vocab_reduction.items():\n",
    "            if method != 'original':\n",
    "                reduction_percentages[method] = round(\n",
    "                    ((vocab_reduction['original'] - vocab_size) / vocab_reduction['original']) * 100, 2\n",
    "                )\n",
    "        \n",
    "        return {\n",
    "            'results': {\n",
    "                'spacy': spacy_result,\n",
    "                'snowball': snowball_result,\n",
    "                'hybrid': hybrid_result\n",
    "            },\n",
    "            'vocabulary_sizes': vocab_reduction,\n",
    "            'reduction_percentages': reduction_percentages,\n",
    "            'comparison_summary': {\n",
    "                'most_aggressive': min(vocab_reduction, key=vocab_reduction.get),\n",
    "                'most_conservative': max(vocab_reduction, key=vocab_reduction.get),\n",
    "                'recommended': 'spacy'  # Nuestra recomendación inicial\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def evaluate_environmental_terms(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Evalúa cómo cada método maneja términos ambientales específicos\n",
    "        \"\"\"\n",
    "        # Términos ambientales de prueba\n",
    "        test_terms = [\n",
    "            'contaminaciones', 'deforestación', 'sostenibilidad', 'biodiversidad',\n",
    "            'energías renovables', 'combustibles fósiles', 'emisiones tóxicas',\n",
    "            'conservación ambiental', 'calentamiento global', 'cambio climático'\n",
    "        ]\n",
    "        \n",
    "        test_text = ' '.join(test_terms)\n",
    "        \n",
    "        # Aplicar métodos\n",
    "        spacy_result = self.lemmatize_with_spacy(test_text)\n",
    "        snowball_result = self.stem_with_snowball(test_text)\n",
    "        hybrid_result = self.hybrid_approach(test_text)\n",
    "        \n",
    "        return {\n",
    "            'test_terms': test_terms,\n",
    "            'spacy_output': spacy_result['lemmatized_text'],\n",
    "            'snowball_output': snowball_result['stemmed_text'],\n",
    "            'hybrid_output': hybrid_result['processed_text'],\n",
    "            'evaluation': {\n",
    "                'spacy_preserves_meaning': self._check_meaning_preservation(test_terms, spacy_result['lemmatized_text']),\n",
    "                'snowball_preserves_meaning': self._check_meaning_preservation(test_terms, snowball_result['stemmed_text']),\n",
    "                'hybrid_preserves_meaning': self._check_meaning_preservation(test_terms, hybrid_result['processed_text'])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def _check_meaning_preservation(self, original_terms: List[str], processed_text: str) -> float:\n",
    "        \"\"\"\n",
    "        Verifica qué porcentaje de términos mantienen su significado reconocible\n",
    "        \"\"\"\n",
    "        processed_words = processed_text.split()\n",
    "        recognizable_count = 0\n",
    "        \n",
    "        for word in processed_words:\n",
    "            # Verificar si la palabra procesada es reconocible\n",
    "            # (esto es una aproximación simple)\n",
    "            if len(word) >= 4 and word.isalpha():\n",
    "                recognizable_count += 1\n",
    "        \n",
    "        return round((recognizable_count / len(processed_words)) * 100, 2) if processed_words else 0\n",
    "    \n",
    "    def comprehensive_processing(self, text: str, method: str = 'spacy', options: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Procesamiento completo con el método seleccionado\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a procesar\n",
    "            method: 'spacy', 'snowball', 'hybrid', o 'compare'\n",
    "            options: Opciones adicionales\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            options = {\n",
    "                'preserve_environmental_terms': True,\n",
    "                'min_word_length': 2,\n",
    "                'remove_duplicates': True\n",
    "            }\n",
    "        \n",
    "        if method == 'compare':\n",
    "            return self.compare_methods(text)\n",
    "        elif method == 'spacy':\n",
    "            result = self.lemmatize_with_spacy(text)\n",
    "        elif method == 'snowball':\n",
    "            result = self.stem_with_snowball(text)\n",
    "        elif method == 'hybrid':\n",
    "            result = self.hybrid_approach(text)\n",
    "        else:\n",
    "            raise ValueError(\"Método debe ser 'spacy', 'snowball', 'hybrid', o 'compare'\")\n",
    "        \n",
    "        # Aplicar opciones adicionales\n",
    "        processed_text = result.get('lemmatized_text') or result.get('stemmed_text') or result.get('processed_text')\n",
    "        \n",
    "        if options.get('min_word_length', 2) > 1:\n",
    "            words = processed_text.split()\n",
    "            words = [w for w in words if len(w) >= options['min_word_length']]\n",
    "            processed_text = ' '.join(words)\n",
    "        \n",
    "        if options.get('remove_duplicates', True):\n",
    "            words = processed_text.split()\n",
    "            # Mantener orden pero eliminar duplicados\n",
    "            seen = set()\n",
    "            unique_words = []\n",
    "            for word in words:\n",
    "                if word not in seen:\n",
    "                    seen.add(word)\n",
    "                    unique_words.append(word)\n",
    "            processed_text = ' '.join(unique_words)\n",
    "        \n",
    "        # Actualizar resultado\n",
    "        result['final_processed_text'] = processed_text\n",
    "        result['options_applied'] = options\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Ejemplo de uso del módulo de lematización y stemming\n",
    "print(\"=== MÓDULO 6: LEMATIZACIÓN Y STEMMING ===\")\n",
    "\n",
    "# Crear instancia del lematizador\n",
    "lemmatizer = TextLemmatizer()\n",
    "\n",
    "# Texto de ejemplo con términos ambientales variados\n",
    "sample_text = \"\"\"\n",
    "Las contaminaciones ambientales están afectando los ecosistemas naturales.\n",
    "Las energías renovables ofrecen soluciones sostenibles para reducir las emisiones.\n",
    "Los científicos estudian la biodiversidad en diferentes regiones protegidas.\n",
    "Las deforestaciones masivas causan cambios climáticos irreversibles.\n",
    "Necesitamos implementar tecnologías innovadoras para la conservación.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Texto original:\")\n",
    "print(sample_text)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Lematización con spaCy\n",
    "print(\"1. LEMATIZACIÓN CON SPACY:\")\n",
    "spacy_result = lemmatizer.lemmatize_with_spacy(sample_text)\n",
    "print(f\"Texto lematizado: {spacy_result['lemmatized_text']}\")\n",
    "print(f\"Cambios realizados: {spacy_result['changes_made']}\")\n",
    "print(\"Ejemplos de cambios:\")\n",
    "for detail in spacy_result['lemmatization_details'][:5]:\n",
    "    print(f\"  '{detail['original']}' → '{detail['lemma']}' ({detail['pos']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"2. STEMMING CON SNOWBALL:\")\n",
    "snowball_result = lemmatizer.stem_with_snowball(sample_text)\n",
    "print(f\"Texto stemmed: {snowball_result['stemmed_text']}\")\n",
    "print(f\"Cambios realizados: {snowball_result['changes_made']}\")\n",
    "print(\"Ejemplos de cambios:\")\n",
    "for detail in snowball_result['stemming_details'][:5]:\n",
    "    print(f\"  '{detail['original']}' → '{detail['stem']}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"3. ENFOQUE HÍBRIDO:\")\n",
    "hybrid_result = lemmatizer.hybrid_approach(sample_text)\n",
    "print(f\"Texto procesado: {hybrid_result['processed_text']}\")\n",
    "print(f\"Cambios realizados: {hybrid_result['changes_made']}\")\n",
    "print(\"Ejemplos de cambios:\")\n",
    "for detail in hybrid_result['processing_details'][:5]:\n",
    "    print(f\"  '{detail['original']}' → '{detail['processed']}' ({detail['method']})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. COMPARACIÓN DE MÉTODOS:\")\n",
    "comparison = lemmatizer.compare_methods(sample_text)\n",
    "print(\"Tamaños de vocabulario:\")\n",
    "for method, size in comparison['vocabulary_sizes'].items():\n",
    "    print(f\"  {method.capitalize()}: {size} palabras únicas\")\n",
    "\n",
    "print(\"\\nPorcentajes de reducción:\")\n",
    "for method, percentage in comparison['reduction_percentages'].items():\n",
    "    print(f\"  {method.capitalize()}: {percentage}%\")\n",
    "\n",
    "print(f\"\\nMétodo más agresivo: {comparison['comparison_summary']['most_aggressive']}\")\n",
    "print(f\"Método más conservador: {comparison['comparison_summary']['most_conservative']}\")\n",
    "print(f\"Método recomendado: {comparison['comparison_summary']['recommended']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"5. EVALUACIÓN DE TÉRMINOS AMBIENTALES:\")\n",
    "env_evaluation = lemmatizer.evaluate_environmental_terms(sample_text)\n",
    "print(\"Términos de prueba:\", env_evaluation['test_terms'][:3], \"...\")\n",
    "print(f\"\\nSpaCy output: {env_evaluation['spacy_output'][:100]}...\")\n",
    "print(f\"Snowball output: {env_evaluation['snowball_output'][:100]}...\")\n",
    "print(f\"Hybrid output: {env_evaluation['hybrid_output'][:100]}...\")\n",
    "\n",
    "print(\"\\nPreservación de significado:\")\n",
    "for method, score in env_evaluation['evaluation'].items():\n",
    "    print(f\"  {method}: {score}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. PROCESAMIENTO RECOMENDADO (spaCy):\")\n",
    "final_result = lemmatizer.comprehensive_processing(sample_text, method='spacy')\n",
    "print(f\"Texto final procesado:\")\n",
    "print(f\"'{final_result['final_processed_text']}'\")\n",
    "print(f\"\\nEstadísticas finales:\")\n",
    "print(f\"- Tokens originales: {final_result['original_tokens']}\")\n",
    "print(f\"- Tokens procesados: {final_result['lemmatized_tokens']}\")\n",
    "print(f\"- Cambios realizados: {final_result['changes_made']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 7: Procesamiento con BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÓDULO 7: PROCESAMIENTO CON BERT ===\n",
      "Texto original:\n",
      "'El reciclaje es importante para el planeta. Debemos cuidar el medio ambiente.'\n",
      "\n",
      "============================================================\n",
      "1. GENERACIÓN DE EMBEDDINGS:\n",
      "Cargando modelo BERT: dccuchile/bert-base-spanish-wwm-uncased\n",
      "✓ Modelo BERT cargado exitosamente\n",
      "Tokens procesados: 16\n",
      "Dimensión de embeddings: 768\n",
      "Modelo usado: dccuchile/bert-base-spanish-wwm-uncased\n",
      "\n",
      "2. MEJORA DE TEXTO CON CONTEXTO:\n",
      "Mejor mejora encontrada:\n",
      "Versión: length_optimized\n",
      "Texto mejorado: 'El reciclaje es importante para el planeta. Debemos cuidar el medio ambiente. Esta iniciativa representa un avance significativo en la protección ambiental y el desarrollo sostenible.'\n",
      "Score de similitud: 0.836\n",
      "Términos ambientales: 3\n",
      "\n",
      "3. GENERACIÓN DE VARIACIONES:\n",
      "Variaciones generadas:\n",
      "\n",
      "  TECHNICAL:\n",
      "  Texto: 'Desde una perspectiva técnica, el reciclaje es crítico para el planeta. debemos cuidar el medio ambi...'\n",
      "  Score ambiental: 10.00\n",
      "  Similitud semántica: 0.774\n",
      "\n",
      "  EMOTIONAL:\n",
      "  Texto: 'Imagina un futuro donde el reciclaje es importante para el planeta. debemos cuidar el medio ambiente...'\n",
      "  Score ambiental: 9.09\n",
      "  Similitud semántica: 0.758\n",
      "\n",
      "  EDUCATIONAL:\n",
      "  Texto: 'Es importante entender que el reciclaje es importante para el planeta. debemos cuidar el medio ambie...'\n",
      "  Score ambiental: 6.45\n",
      "  Similitud semántica: 0.880\n",
      "\n",
      "============================================================\n",
      "4. MEJORA COMPLETA DEL TEXTO:\n",
      "RESULTADO FINAL:\n",
      "Texto original: 'El reciclaje es importante para el planeta. Debemos cuidar el medio ambiente.'\n",
      "Texto mejorado: 'Juntos podemos lograr que el reciclaje es importante para el planeta. debemos cuidar el medio ambiente. esta iniciativa representa un avance significativo en la protección ambiental y el desarrollo sostenible. ¡El momento de actuar es ahora!'\n",
      "\n",
      "Resumen de mejoras:\n",
      "- Longitud original: 12 palabras\n",
      "- Longitud final: 36 palabras\n",
      "- Términos ambientales añadidos: -1.7\n",
      "- Preservación semántica: 0.836\n",
      "\n",
      "========================================\n",
      "5. EJEMPLOS CON DIFERENTES TIPOS:\n",
      "\n",
      "TITULO:\n",
      "Original: 'Energía solar'\n",
      "Mejorado: 'Desde una perspectiva técnica, energía solar la renovable es fundamental. Este enfoque garantiza resultados medibles y sostenibles.'\n",
      "\n",
      "DESCRIPCION:\n",
      "Original: 'Las plantas purifican el aire'\n",
      "Mejorado: 'Desde una perspectiva técnica, las plantas purifican el aire Este enfoque garantiza resultados medibles y sostenibles.'\n",
      "\n",
      "DESCRIPCION:\n",
      "Original: 'plantas purificar aire'\n",
      "Mejorado: 'Desde una perspectiva técnica, plantas purificar aire Este enfoque garantiza resultados medibles y sostenibles.'\n",
      "\n",
      "LLAMADA_ACCION:\n",
      "Original: 'Recicla para salvar el planeta'\n",
      "Mejorado: 'Desde una perspectiva técnica, recicla para salvar el planeta Este enfoque garantiza resultados medibles y sostenibles.'\n"
     ]
    }
   ],
   "source": [
    "# ===== MÓDULO 7: PROCESAMIENTO CON BERT =====\n",
    "\n",
    "class BERTProcessor:\n",
    "    \"\"\"\n",
    "    Módulo para procesamiento y mejora de texto usando BERT.\n",
    "    Incluye generación de embeddings, mejora de texto, y análisis semántico.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = 'dccuchile/bert-base-spanish-wwm-uncased'):\n",
    "        self.model_name = model_name\n",
    "        self.device = device  # Configurado anteriormente\n",
    "        \n",
    "        # Inicializar modelos (se cargarán cuando sea necesario)\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.text_generator = None\n",
    "        self.summarizer = None\n",
    "        self.classifier = None\n",
    "        \n",
    "        # Configuraciones para diferentes tareas\n",
    "        self.max_length = 512\n",
    "        self.environmental_keywords = [\n",
    "            'medio ambiente', 'sostenible', 'ecológico', 'verde', 'limpio',\n",
    "            'renovable', 'conservación', 'biodiversidad', 'clima', 'carbono',\n",
    "            'emisiones', 'contaminación', 'reciclaje', 'energía', 'natural'\n",
    "        ]\n",
    "        \n",
    "        # Templates para mejora de texto\n",
    "        self.improvement_templates = {\n",
    "            'titulo': \"Mejora este título para que sea más atractivo y claro: {text}\",\n",
    "            'descripcion': \"Reescribe esta descripción para que sea más informativa y engaging: {text}\",\n",
    "            'contenido': \"Mejora este contenido haciéndolo más profesional y completo: {text}\",\n",
    "            'llamada_accion': \"Convierte este texto en una llamada a la acción convincente: {text}\",\n",
    "            'resumen': \"Crea un resumen conciso y impactante de: {text}\"\n",
    "        }\n",
    "    \n",
    "    def _load_bert_model(self):\n",
    "        \"\"\"Carga el modelo BERT si no está cargado\"\"\"\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            print(f\"Cargando modelo BERT: {self.model_name}\")\n",
    "            try:\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "                self.model = BertModel.from_pretrained(self.model_name)\n",
    "                self.model.to(self.device)\n",
    "                self.model.eval()\n",
    "                print(\"✓ Modelo BERT cargado exitosamente\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error cargando BERT: {e}\")\n",
    "                # Fallback a modelo en inglés\n",
    "                print(\"Usando modelo BERT en inglés como fallback...\")\n",
    "                self.model_name = 'bert-base-uncased'\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "                self.model = BertModel.from_pretrained(self.model_name)\n",
    "                self.model.to(self.device)\n",
    "                self.model.eval()\n",
    "    \n",
    "    def _load_text_generator(self):\n",
    "        \"\"\"Carga el generador de texto si no está cargado\"\"\"\n",
    "        if self.text_generator is None:\n",
    "            print(\"Cargando generador de texto...\")\n",
    "            try:\n",
    "                # Intentar con modelo en español\n",
    "                self.text_generator = pipeline(\n",
    "                    'text-generation',\n",
    "                    model='DeepESP/gpt2-spanish',\n",
    "                    tokenizer='DeepESP/gpt2-spanish',\n",
    "                    device=0 if self.device.type == 'cuda' else -1\n",
    "                )\n",
    "                print(\"✓ Generador de texto en español cargado\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error con modelo español: {e}\")\n",
    "                try:\n",
    "                    # Fallback a modelo multilingüe\n",
    "                    self.text_generator = pipeline(\n",
    "                        'text-generation',\n",
    "                        model='gpt2',\n",
    "                        device=0 if self.device.type == 'cuda' else -1\n",
    "                    )\n",
    "                    print(\"✓ Generador de texto (fallback) cargado\")\n",
    "                except Exception as e2:\n",
    "                    print(f\"Error cargando generador: {e2}\")\n",
    "                    self.text_generator = None\n",
    "    \n",
    "    def _load_summarizer(self):\n",
    "        \"\"\"Carga el resumidor si no está cargado\"\"\"\n",
    "        if self.summarizer is None:\n",
    "            print(\"Cargando resumidor...\")\n",
    "            try:\n",
    "                self.summarizer = pipeline(\n",
    "                    'summarization',\n",
    "                    model='facebook/bart-large-cnn',\n",
    "                    device=0 if self.device.type == 'cuda' else -1\n",
    "                )\n",
    "                print(\"✓ Resumidor cargado\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error cargando resumidor: {e}\")\n",
    "                self.summarizer = None\n",
    "    \n",
    "    def generate_embeddings(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Genera embeddings BERT para el texto\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con embeddings y estadísticas\n",
    "        \"\"\"\n",
    "        self._load_bert_model()\n",
    "        \n",
    "        # Tokenizar texto\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors='pt',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # Mover a dispositivo\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Generar embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "            # Obtener diferentes tipos de embeddings\n",
    "            last_hidden_states = outputs.last_hidden_state\n",
    "            pooler_output = outputs.pooler_output\n",
    "            \n",
    "            # Embedding promedio de todos los tokens\n",
    "            mean_embedding = torch.mean(last_hidden_states, dim=1)\n",
    "            \n",
    "            # Embedding del token [CLS]\n",
    "            cls_embedding = last_hidden_states[:, 0, :]\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'token_count': inputs['input_ids'].shape[1],\n",
    "            'embeddings': {\n",
    "                'cls_embedding': cls_embedding.cpu().numpy(),\n",
    "                'mean_embedding': mean_embedding.cpu().numpy(),\n",
    "                'pooler_output': pooler_output.cpu().numpy()\n",
    "            },\n",
    "            'embedding_dimension': last_hidden_states.shape[-1],\n",
    "            'model_used': self.model_name\n",
    "        }\n",
    "    \n",
    "    def calculate_semantic_similarity(self, text1: str, text2: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Calcula similitud semántica entre dos textos usando BERT\n",
    "        \"\"\"\n",
    "        # Generar embeddings para ambos textos\n",
    "        emb1 = self.generate_embeddings(text1)\n",
    "        emb2 = self.generate_embeddings(text2)\n",
    "        \n",
    "        # Calcular similitud coseno para diferentes tipos de embeddings\n",
    "        similarities = {}\n",
    "        \n",
    "        for emb_type in ['cls_embedding', 'mean_embedding', 'pooler_output']:\n",
    "            vec1 = emb1['embeddings'][emb_type].flatten()\n",
    "            vec2 = emb2['embeddings'][emb_type].flatten()\n",
    "            \n",
    "            # Similitud coseno\n",
    "            dot_product = np.dot(vec1, vec2)\n",
    "            norm1 = np.linalg.norm(vec1)\n",
    "            norm2 = np.linalg.norm(vec2)\n",
    "            \n",
    "            similarity = dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0\n",
    "            similarities[emb_type] = float(similarity)\n",
    "        \n",
    "        return {\n",
    "            'text1': text1,\n",
    "            'text2': text2,\n",
    "            'similarities': similarities,\n",
    "            'average_similarity': np.mean(list(similarities.values())),\n",
    "            'most_similar_embedding': max(similarities, key=similarities.get)\n",
    "        }\n",
    "    \n",
    "    def improve_text_with_context(self, text: str, improvement_type: str = 'contenido', \n",
    "                                 environmental_focus: bool = True) -> Dict:\n",
    "        \"\"\"\n",
    "        Mejora el texto usando contexto ambiental y BERT\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a mejorar\n",
    "            improvement_type: Tipo de mejora ('titulo', 'descripcion', 'contenido', etc.)\n",
    "            environmental_focus: Si enfocar en términos ambientales\n",
    "        \"\"\"\n",
    "        # Analizar el texto original\n",
    "        original_embeddings = self.generate_embeddings(text)\n",
    "        \n",
    "        # Identificar términos ambientales presentes\n",
    "        environmental_terms_found = [\n",
    "            term for term in self.environmental_keywords \n",
    "            if term.lower() in text.lower()\n",
    "        ]\n",
    "        \n",
    "        # Generar versiones mejoradas\n",
    "        improvements = []\n",
    "        \n",
    "        # Mejora 1: Expansión con términos ambientales\n",
    "        if environmental_focus:\n",
    "            expanded_text = self._expand_with_environmental_terms(text, environmental_terms_found)\n",
    "            improvements.append({\n",
    "                'version': 'environmental_expansion',\n",
    "                'text': expanded_text,\n",
    "                'description': 'Expandido con términos ambientales relevantes'\n",
    "            })\n",
    "        \n",
    "        # Mejora 2: Reestructuración para claridad\n",
    "        restructured_text = self._restructure_for_clarity(text, improvement_type)\n",
    "        improvements.append({\n",
    "            'version': 'restructured',\n",
    "            'text': restructured_text,\n",
    "            'description': 'Reestructurado para mayor claridad'\n",
    "        })\n",
    "        \n",
    "        # Mejora 3: Optimización de longitud\n",
    "        optimized_text = self._optimize_length(text, improvement_type)\n",
    "        improvements.append({\n",
    "            'version': 'length_optimized',\n",
    "            'text': optimized_text,\n",
    "            'description': 'Optimizado para longitud apropiada'\n",
    "        })\n",
    "        \n",
    "        # Evaluar cada mejora usando BERT\n",
    "        evaluated_improvements = []\n",
    "        for improvement in improvements:\n",
    "            improved_embeddings = self.generate_embeddings(improvement['text'])\n",
    "            similarity = self.calculate_semantic_similarity(text, improvement['text'])\n",
    "            \n",
    "            evaluated_improvements.append({\n",
    "                **improvement,\n",
    "                'semantic_similarity': similarity['average_similarity'],\n",
    "                'token_count': improved_embeddings['token_count'],\n",
    "                'environmental_terms': len([\n",
    "                    term for term in self.environmental_keywords \n",
    "                    if term.lower() in improvement['text'].lower()\n",
    "                ])\n",
    "            })\n",
    "        \n",
    "        # Seleccionar la mejor mejora\n",
    "        best_improvement = max(\n",
    "            evaluated_improvements, \n",
    "            key=lambda x: x['semantic_similarity'] + (x['environmental_terms'] * 0.1)\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'improvement_type': improvement_type,\n",
    "            'environmental_focus': environmental_focus,\n",
    "            'original_environmental_terms': environmental_terms_found,\n",
    "            'all_improvements': evaluated_improvements,\n",
    "            'best_improvement': best_improvement,\n",
    "            'improvement_score': best_improvement['semantic_similarity']\n",
    "        }\n",
    "    \n",
    "    def _expand_with_environmental_terms(self, text: str, existing_terms: List[str]) -> str:\n",
    "        \"\"\"Expande el texto con términos ambientales relevantes\"\"\"\n",
    "        # Mapeo de términos relacionados\n",
    "        term_expansions = {\n",
    "            'medio ambiente': ['ecosistema', 'naturaleza', 'biodiversidad'],\n",
    "            'sostenible': ['ecológico', 'verde', 'responsable'],\n",
    "            'contaminación': ['emisiones', 'residuos', 'tóxicos'],\n",
    "            'energía': ['renovable', 'limpia', 'eficiente'],\n",
    "            'clima': ['calentamiento global', 'cambio climático'],\n",
    "            'conservación': ['protección', 'preservación']\n",
    "        }\n",
    "        \n",
    "        expanded_text = text\n",
    "        \n",
    "        # Agregar términos relacionados si no están presentes\n",
    "        for existing_term in existing_terms:\n",
    "            if existing_term in term_expansions:\n",
    "                for related_term in term_expansions[existing_term]:\n",
    "                    if related_term.lower() not in text.lower():\n",
    "                        # Insertar término relacionado de manera natural\n",
    "                        expanded_text += f\" La {related_term} es fundamental.\"\n",
    "                        break  # Solo agregar uno por término existente\n",
    "        \n",
    "        return expanded_text\n",
    "    \n",
    "    def _restructure_for_clarity(self, text: str, improvement_type: str) -> str:\n",
    "        \"\"\"Reestructura el texto para mayor claridad\"\"\"\n",
    "        sentences = text.split('.')\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        \n",
    "        if improvement_type == 'titulo':\n",
    "            # Para títulos, hacer más conciso y atractivo\n",
    "            main_concept = sentences[0] if sentences else text\n",
    "            return f\"{main_concept.strip()}: Solución Ambiental Innovadora\"\n",
    "        \n",
    "        elif improvement_type == 'descripcion':\n",
    "            # Para descripciones, estructura problema-solución\n",
    "            if len(sentences) >= 2:\n",
    "                return f\"{sentences[0]}. Esta situación requiere acción inmediata. {' '.join(sentences[1:])}.\"\n",
    "            else:\n",
    "                return f\"{text} Esta iniciativa contribuye significativamente a la sostenibilidad ambiental.\"\n",
    "        \n",
    "        elif improvement_type == 'contenido':\n",
    "            # Para contenido, agregar estructura y transiciones\n",
    "            if len(sentences) >= 2:\n",
    "                restructured = f\"En primer lugar, {sentences[0].lower()}. \"\n",
    "                if len(sentences) > 2:\n",
    "                    restructured += f\"Además, {sentences[1].lower()}. \"\n",
    "                    restructured += f\"Por último, {' '.join(sentences[2:]).lower()}.\"\n",
    "                else:\n",
    "                    restructured += f\"En consecuencia, {sentences[1].lower()}.\"\n",
    "                return restructured\n",
    "            else:\n",
    "                return f\"Es importante destacar que {text.lower()} Esto representa un paso crucial hacia la sostenibilidad.\"\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def _optimize_length(self, text: str, improvement_type: str) -> str:\n",
    "        \"\"\"Optimiza la longitud del texto según el tipo\"\"\"\n",
    "        words = text.split()\n",
    "        \n",
    "        target_lengths = {\n",
    "            'titulo': (5, 12),\n",
    "            'descripcion': (20, 50),\n",
    "            'contenido': (50, 200),\n",
    "            'llamada_accion': (10, 25),\n",
    "            'resumen': (15, 40)\n",
    "        }\n",
    "        \n",
    "        min_len, max_len = target_lengths.get(improvement_type, (20, 100))\n",
    "        \n",
    "        if len(words) < min_len:\n",
    "            # Expandir texto corto\n",
    "            if improvement_type == 'titulo':\n",
    "                return f\"{text}: Innovación para el Futuro Sostenible\"\n",
    "            else:\n",
    "                return f\"{text} Esta iniciativa representa un avance significativo en la protección ambiental y el desarrollo sostenible.\"\n",
    "        \n",
    "        elif len(words) > max_len:\n",
    "            # Acortar texto largo\n",
    "            if improvement_type == 'titulo':\n",
    "                # Tomar las primeras palabras clave\n",
    "                key_words = words[:8]\n",
    "                return ' '.join(key_words)\n",
    "            else:\n",
    "                # Resumir manteniendo ideas principales\n",
    "                sentences = text.split('.')\n",
    "                main_sentences = sentences[:2] if len(sentences) > 2 else sentences\n",
    "                return '. '.join(main_sentences).strip() + '.'\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def generate_variations(self, text: str, num_variations: int = 3) -> Dict:\n",
    "        \"\"\"\n",
    "        Genera múltiples variaciones del texto\n",
    "        \"\"\"\n",
    "        variations = []\n",
    "        \n",
    "        # Variación 1: Enfoque técnico\n",
    "        technical_variation = self._create_technical_variation(text)\n",
    "        variations.append({\n",
    "            'type': 'technical',\n",
    "            'text': technical_variation,\n",
    "            'description': 'Enfoque técnico y profesional'\n",
    "        })\n",
    "        \n",
    "        # Variación 2: Enfoque emocional\n",
    "        emotional_variation = self._create_emotional_variation(text)\n",
    "        variations.append({\n",
    "            'type': 'emotional',\n",
    "            'text': emotional_variation,\n",
    "            'description': 'Enfoque emocional y persuasivo'\n",
    "        })\n",
    "        \n",
    "        # Variación 3: Enfoque educativo\n",
    "        educational_variation = self._create_educational_variation(text)\n",
    "        variations.append({\n",
    "            'type': 'educational',\n",
    "            'text': educational_variation,\n",
    "            'description': 'Enfoque educativo e informativo'\n",
    "        })\n",
    "        \n",
    "        # Evaluar cada variación\n",
    "        evaluated_variations = []\n",
    "        for variation in variations:\n",
    "            similarity = self.calculate_semantic_similarity(text, variation['text'])\n",
    "            embeddings = self.generate_embeddings(variation['text'])\n",
    "            \n",
    "            evaluated_variations.append({\n",
    "                **variation,\n",
    "                'semantic_similarity': similarity['average_similarity'],\n",
    "                'token_count': embeddings['token_count'],\n",
    "                'environmental_score': self._calculate_environmental_score(variation['text'])\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'variations': evaluated_variations,\n",
    "            'best_variation': max(evaluated_variations, key=lambda x: x['environmental_score'])\n",
    "        }\n",
    "    \n",
    "    def _create_technical_variation(self, text: str) -> str:\n",
    "        \"\"\"Crea una variación con enfoque técnico\"\"\"\n",
    "        technical_terms = {\n",
    "            'problema': 'desafío técnico',\n",
    "            'solución': 'implementación estratégica',\n",
    "            'importante': 'crítico',\n",
    "            'bueno': 'eficiente',\n",
    "            'malo': 'ineficiente',\n",
    "            'ayuda': 'optimiza',\n",
    "            'hace': 'ejecuta'\n",
    "        }\n",
    "        \n",
    "        technical_text = text\n",
    "        for original, technical in technical_terms.items():\n",
    "            technical_text = technical_text.replace(original, technical)\n",
    "        \n",
    "        return f\"Desde una perspectiva técnica, {technical_text.lower()} Este enfoque garantiza resultados medibles y sostenibles.\"\n",
    "    \n",
    "    def _create_emotional_variation(self, text: str) -> str:\n",
    "        \"\"\"Crea una variación con enfoque emocional\"\"\"\n",
    "        emotional_starters = [\n",
    "            \"Imagina un futuro donde\",\n",
    "            \"Es hora de actuar:\",\n",
    "            \"Nuestro planeta necesita que\",\n",
    "            \"Juntos podemos lograr que\"\n",
    "        ]\n",
    "        \n",
    "        starter = np.random.choice(emotional_starters)\n",
    "        return f\"{starter} {text.lower()} ¡El momento de actuar es ahora!\"\n",
    "    \n",
    "    def _create_educational_variation(self, text: str) -> str:\n",
    "        \"\"\"Crea una variación con enfoque educativo\"\"\"\n",
    "        return f\"Es importante entender que {text.lower()} Los estudios demuestran que estas acciones tienen un impacto positivo significativo en el medio ambiente.\"\n",
    "    \n",
    "    def _calculate_environmental_score(self, text: str) -> float:\n",
    "        \"\"\"Calcula un score de relevancia ambiental\"\"\"\n",
    "        score = 0\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for keyword in self.environmental_keywords:\n",
    "            if keyword in text_lower:\n",
    "                score += 1\n",
    "        \n",
    "        # Normalizar por longitud del texto\n",
    "        words = len(text.split())\n",
    "        normalized_score = (score / words) * 100 if words > 0 else 0\n",
    "        \n",
    "        return min(normalized_score, 10.0)  # Máximo 10\n",
    "    \n",
    "    def comprehensive_text_improvement(self, text: str, target_type: str = 'contenido', \n",
    "                                     options: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Mejora completa del texto usando todas las capacidades de BERT\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            options = {\n",
    "                'environmental_focus': True,\n",
    "                'generate_variations': True,\n",
    "                'optimize_length': True,\n",
    "                'include_embeddings': False,\n",
    "                'similarity_threshold': 0.7\n",
    "            }\n",
    "        \n",
    "        # Análisis inicial\n",
    "        original_embeddings = self.generate_embeddings(text) if options.get('include_embeddings') else None\n",
    "        \n",
    "        # Mejora principal\n",
    "        main_improvement = self.improve_text_with_context(\n",
    "            text, \n",
    "            target_type, \n",
    "            options.get('environmental_focus', True)\n",
    "        )\n",
    "        \n",
    "        # Generar variaciones si se solicita\n",
    "        variations = None\n",
    "        if options.get('generate_variations', True):\n",
    "            variations = self.generate_variations(main_improvement['best_improvement']['text'])\n",
    "        \n",
    "        # Resultado final\n",
    "        final_text = variations['best_variation']['text'] if variations else main_improvement['best_improvement']['text']\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'target_type': target_type,\n",
    "            'options': options,\n",
    "            'main_improvement': main_improvement,\n",
    "            'variations': variations,\n",
    "            'final_improved_text': final_text,\n",
    "            'improvement_summary': {\n",
    "                'original_length': len(text.split()),\n",
    "                'final_length': len(final_text.split()),\n",
    "                'environmental_terms_added': self._calculate_environmental_score(final_text) - self._calculate_environmental_score(text),\n",
    "                'semantic_preservation': main_improvement['improvement_score']\n",
    "            },\n",
    "            'original_embeddings': original_embeddings\n",
    "        }\n",
    "\n",
    "# Ejemplo de uso del módulo BERT\n",
    "print(\"=== MÓDULO 7: PROCESAMIENTO CON BERT ===\")\n",
    "\n",
    "# Crear instancia del procesador BERT\n",
    "bert_processor = BERTProcessor()\n",
    "\n",
    "# Texto de ejemplo para mejorar\n",
    "sample_text = \"El reciclaje es importante para el planeta. Debemos cuidar el medio ambiente.\"\n",
    "\n",
    "print(\"Texto original:\")\n",
    "print(f\"'{sample_text}'\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Generar embeddings\n",
    "print(\"1. GENERACIÓN DE EMBEDDINGS:\")\n",
    "embeddings_result = bert_processor.generate_embeddings(sample_text)\n",
    "print(f\"Tokens procesados: {embeddings_result['token_count']}\")\n",
    "print(f\"Dimensión de embeddings: {embeddings_result['embedding_dimension']}\")\n",
    "print(f\"Modelo usado: {embeddings_result['model_used']}\")\n",
    "\n",
    "# Mejora de texto con contexto\n",
    "print(\"\\n2. MEJORA DE TEXTO CON CONTEXTO:\")\n",
    "improvement_result = bert_processor.improve_text_with_context(\n",
    "    sample_text, \n",
    "    improvement_type='contenido',\n",
    "    environmental_focus=True\n",
    ")\n",
    "\n",
    "print(f\"Mejor mejora encontrada:\")\n",
    "print(f\"Versión: {improvement_result['best_improvement']['version']}\")\n",
    "print(f\"Texto mejorado: '{improvement_result['best_improvement']['text']}'\")\n",
    "print(f\"Score de similitud: {improvement_result['improvement_score']:.3f}\")\n",
    "print(f\"Términos ambientales: {improvement_result['best_improvement']['environmental_terms']}\")\n",
    "\n",
    "# Generar variaciones\n",
    "print(\"\\n3. GENERACIÓN DE VARIACIONES:\")\n",
    "variations_result = bert_processor.generate_variations(sample_text)\n",
    "print(\"Variaciones generadas:\")\n",
    "for var in variations_result['variations']:\n",
    "    print(f\"\\n  {var['type'].upper()}:\")\n",
    "    print(f\"  Texto: '{var['text'][:100]}{'...' if len(var['text']) > 100 else ''}'\")\n",
    "    print(f\"  Score ambiental: {var['environmental_score']:.2f}\")\n",
    "    print(f\"  Similitud semántica: {var['semantic_similarity']:.3f}\")\n",
    "\n",
    "# Mejora completa\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. MEJORA COMPLETA DEL TEXTO:\")\n",
    "\n",
    "final_result = bert_processor.comprehensive_text_improvement(\n",
    "    sample_text,\n",
    "    target_type='descripcion',\n",
    "    options={\n",
    "        'environmental_focus': True,\n",
    "        'generate_variations': True,\n",
    "        'optimize_length': True,\n",
    "        'include_embeddings': False\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"RESULTADO FINAL:\")\n",
    "print(f\"Texto original: '{final_result['original_text']}'\")\n",
    "print(f\"Texto mejorado: '{final_result['final_improved_text']}'\")\n",
    "\n",
    "print(f\"\\nResumen de mejoras:\")\n",
    "summary = final_result['improvement_summary']\n",
    "print(f\"- Longitud original: {summary['original_length']} palabras\")\n",
    "print(f\"- Longitud final: {summary['final_length']} palabras\")\n",
    "print(f\"- Términos ambientales añadidos: {summary['environmental_terms_added']:.1f}\")\n",
    "print(f\"- Preservación semántica: {summary['semantic_preservation']:.3f}\")\n",
    "\n",
    "# Ejemplo con diferentes tipos de texto\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"5. EJEMPLOS CON DIFERENTES TIPOS:\")\n",
    "\n",
    "test_cases = [\n",
    "    (\"Energía solar\", \"titulo\"),\n",
    "    (\"Las plantas purifican el aire\", \"descripcion\"),\n",
    "    (\"plantas purificar aire\",\"descripcion\"),\n",
    "    (\"Recicla para salvar el planeta\", \"llamada_accion\")\n",
    "]\n",
    "\n",
    "for text, text_type in test_cases:\n",
    "    result = bert_processor.comprehensive_text_improvement(text, text_type)\n",
    "    print(f\"\\n{text_type.upper()}:\")\n",
    "    print(f\"Original: '{text}'\")\n",
    "    print(f\"Mejorado: '{result['final_improved_text']}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 8: Generacion de Salida y Sistema Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== SISTEMA COMPLETO MEJORADO =====\n",
    "\n",
    "\n",
    "class TextMiningSystem:\n",
    "    def __init__(self):\n",
    "        self.ingestion = TextIngestion()\n",
    "        self.cleaner = TextCleaner()\n",
    "        self.tokenizer = TextTokenizer()\n",
    "        self.normalizer = TextNormalizer()\n",
    "        self.noise_remover = NoiseRemover()  # Versión corregida\n",
    "        self.lemmatizer = TextLemmatizer()\n",
    "        self.bert_processor = BERTProcessor()\n",
    "    \n",
    "    def process_text_complete_enhanced(self, text: str, content_type: str = 'contenido', track_steps: bool = False) -> Dict:\n",
    "        \"\"\"Procesamiento mejorado con mejor control de calidad\"\"\"\n",
    "        \n",
    "        # Configuraciones específicas por tipo de contenido\n",
    "        processing_configs = {\n",
    "            'titulo': {\n",
    "                'aggressive_cleaning': False,\n",
    "                'preserve_length': True,\n",
    "                'min_similarity': 0.7\n",
    "            },\n",
    "            'descripcion': {\n",
    "                'aggressive_cleaning': False,\n",
    "                'preserve_length': False,\n",
    "                'min_similarity': 0.6\n",
    "            },\n",
    "            'contenido': {\n",
    "                'aggressive_cleaning': True,\n",
    "                'preserve_length': False,\n",
    "                'min_similarity': 0.5\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        config = processing_configs.get(content_type, processing_configs['contenido'])\n",
    "        \n",
    "        # Validación inicial\n",
    "        original_input = text\n",
    "        text = self._validate_text(text, \"entrada inicial\")\n",
    "        if text is None:\n",
    "            text = original_input\n",
    "        \n",
    "        intermediate_results = {}\n",
    "        processing_steps = []\n",
    "        \n",
    "        try:\n",
    "            # Paso 1: Ingesta (sin cambios)\n",
    "            print(\"Ejecutando Paso 1: Ingesta...\")\n",
    "            ingestion_result = self.ingestion.ingest_manual_text(text)\n",
    "            current_text = ingestion_result['original_text']\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['ingestion'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'ingesta',\n",
    "                    'metrics': {'length': ingestion_result['length']}\n",
    "                })\n",
    "            \n",
    "            # Paso 2: Limpieza (menos agresiva)\n",
    "            print(\"Ejecutando Paso 2: Limpieza...\")\n",
    "            cleaning_options = {\n",
    "                'remove_urls': True,\n",
    "                'remove_emails': True,\n",
    "                'remove_phones': True,\n",
    "                'remove_html': True,\n",
    "                'remove_special_chars': not config['preserve_length'],\n",
    "                'keep_punctuation': True,\n",
    "                'normalize_whitespace': True,\n",
    "                'normalize_punctuation': True,\n",
    "                'remove_newlines': True,\n",
    "                'fix_encoding': True\n",
    "            }\n",
    "            cleaning_result = self.cleaner.basic_clean(current_text, cleaning_options)\n",
    "            current_text = cleaning_result['cleaned_text']\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['cleaning'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'limpieza',\n",
    "                    'metrics': {'reduction_percentage': cleaning_result['reduction_percentage']}\n",
    "                })\n",
    "            \n",
    "            # Paso 3: Tokenización (sin cambios)\n",
    "            print(\"Ejecutando Paso 3: Tokenización...\")\n",
    "            tokens = self.tokenizer.tokenize_words(current_text, 'spacy')\n",
    "            if not tokens:\n",
    "                tokens = current_text.split()\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['tokenization'] = ' '.join(tokens[:20]) + ('...' if len(tokens) > 20 else '')\n",
    "                processing_steps.append({\n",
    "                    'step': 'tokenización',\n",
    "                    'metrics': {'token_count': len(tokens)}\n",
    "                })\n",
    "            \n",
    "            # Paso 4: Normalización (mejorada)\n",
    "            print(\"Ejecutando Paso 4: Normalización...\")\n",
    "            normalization_options = {\n",
    "                'to_lowercase': True,\n",
    "                'remove_accents': False,  # Preservar acentos para mejor legibilidad\n",
    "                'expand_contractions': True,\n",
    "                'expand_abbreviations': True,\n",
    "                'normalize_numbers': 'keep',\n",
    "                'normalize_case_patterns': True,\n",
    "                'normalize_environmental_terms': True\n",
    "            }\n",
    "            normalization_result = self.normalizer.normalize_tokens(tokens, normalization_options)\n",
    "            normalized_tokens = normalization_result.get('normalized_tokens', tokens)\n",
    "            current_text = ' '.join(normalized_tokens)\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['normalization'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'normalización',\n",
    "                    'metrics': {'token_count_change': normalization_result.get('token_count_change', 0)}\n",
    "                })\n",
    "            \n",
    "            # Paso 5: Eliminación de ruido (configuración adaptativa)\n",
    "            print(\"Ejecutando Paso 5: Eliminación de ruido...\")\n",
    "            noise_options = {\n",
    "                'remove_stopwords': config['aggressive_cleaning'],\n",
    "                'aggressive_stopwords': False,\n",
    "                'remove_short_words': True,\n",
    "                'min_word_length': 2,\n",
    "                'remove_noise_words': True,\n",
    "                'remove_by_frequency': False,\n",
    "                'remove_non_alphabetic': False,\n",
    "                'remove_by_pos': False,\n",
    "                'remove_environmental_noise': False,\n",
    "                'use_advanced_removal': False\n",
    "            }\n",
    "            noise_removal_result = self.noise_remover.comprehensive_noise_removal(current_text, noise_options)\n",
    "            current_text = noise_removal_result['cleaned_text']\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['noise_removal'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'eliminación_ruido',\n",
    "                    'metrics': {'reduction_percentage': noise_removal_result['reduction_percentage']}\n",
    "                })\n",
    "            \n",
    "            # Paso 6: Lematización (mejorada)\n",
    "            print(\"Ejecutando Paso 6: Lematización...\")\n",
    "            lemmatization_options = {\n",
    "                'preserve_environmental_terms': True,\n",
    "                'preserve_verbs': True,\n",
    "                'min_word_length': 2,\n",
    "                'remove_duplicates': False,\n",
    "                'maintain_grammar': True\n",
    "            }\n",
    "            lemmatization_result = self.lemmatizer.comprehensive_processing_improved(\n",
    "                current_text, \n",
    "                method='spacy_improved',\n",
    "                options=lemmatization_options\n",
    "            )\n",
    "            current_text = lemmatization_result['final_processed_text']\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['lemmatization'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'lematización',\n",
    "                    'metrics': {'changes_made': lemmatization_result.get('changes_made', 0)}\n",
    "                })\n",
    "            \n",
    "            # Paso 7: Procesamiento con BERT (mejorado)\n",
    "            print(\"Ejecutando Paso 7: Procesamiento con BERT...\")\n",
    "            bert_options = {\n",
    "                'environmental_focus': True,\n",
    "                'generate_variations': False,\n",
    "                'optimize_length': True,\n",
    "                'preserve_meaning': True,\n",
    "                'min_similarity_threshold': config['min_similarity']\n",
    "            }\n",
    "            bert_result = self.bert_processor.comprehensive_text_improvement_enhanced(\n",
    "                current_text,\n",
    "                target_type=content_type,\n",
    "                options=bert_options\n",
    "            )\n",
    "            final_text = bert_result['final_improved_text']\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['bert_processing'] = final_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'bert_processing',\n",
    "                    'metrics': {\n",
    "                        'semantic_preservation': bert_result['improvement_summary']['semantic_preservation'],\n",
    "                        'coherence_score': bert_result['improvement_summary']['coherence_score'],\n",
    "                        'grammar_score': bert_result['improvement_summary']['grammar_score']\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            # Evaluación de calidad mejorada\n",
    "            print(\"Ejecutando evaluación de calidad...\")\n",
    "            evaluation = self._evaluate_quality_enhanced(text, final_text, bert_result['improvement_summary'])\n",
    "            \n",
    "            # Generar recomendaciones\n",
    "            recommendations = self._generate_recommendations_enhanced(evaluation, processing_steps)\n",
    "            \n",
    "            return {\n",
    "                'original_text': text,\n",
    "                'final_text': final_text,\n",
    "                'content_type': content_type,\n",
    "                'intermediate_results': intermediate_results if track_steps else {},\n",
    "                'processing_steps': processing_steps if track_steps else [],\n",
    "                'evaluation': evaluation,\n",
    "                'recommendations': recommendations,\n",
    "                'bert_details': bert_result,\n",
    "                'processing_config': config\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error durante el procesamiento: {str(e)}\")\n",
    "            return self._create_fallback_result(text, content_type, str(e))\n",
    "    \n",
    "    def _evaluate_quality_enhanced(self, original_text: str, final_text: str, improvement_summary: Dict) -> Dict:\n",
    "        \"\"\"Evaluación de calidad mejorada\"\"\"\n",
    "        try:\n",
    "            original_words = len(original_text.split()) if original_text else 0\n",
    "            final_words = len(final_text.split()) if final_text else 0\n",
    "            \n",
    "            # Usar métricas del improvement_summary si están disponibles\n",
    "            semantic_similarity = improvement_summary.get('semantic_preservation', 0.7)\n",
    "            coherence_score = improvement_summary.get('coherence_score', 0.7)\n",
    "            grammar_score = improvement_summary.get('grammar_score', 0.7)\n",
    "            \n",
    "            # Score ambiental\n",
    "            env_score = self.bert_processor._calculate_environmental_score(final_text)\n",
    "            \n",
    "            # Evaluación de legibilidad mejorada\n",
    "            readability_score = self._calculate_readability(final_text)\n",
    "            \n",
    "            # Score general ponderado\n",
    "            overall_quality = (\n",
    "                semantic_similarity * 3 +\n",
    "                coherence_score * 2.5 +\n",
    "                grammar_score * 2 +\n",
    "                (env_score / 10) * 1.5 +\n",
    "                (readability_score / 10) * 1\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'semantic_similarity': semantic_similarity * 10,\n",
    "                'coherence': coherence_score * 10,\n",
    "                'grammar': grammar_score * 10,\n",
    "                'environmental_relevance': env_score,\n",
    "                'readability': readability_score,\n",
    "                'length_optimization': min(10, max(1, 10 - abs(final_words - 15) / 5)) if final_words > 0 else 5.0,\n",
    "                'overall_quality': min(10, overall_quality)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error en evaluación: {e}\")\n",
    "            return {\n",
    "                'semantic_similarity': 7.0,\n",
    "                'coherence': 7.0,\n",
    "                'grammar': 7.0,\n",
    "                'environmental_relevance': 5.0,\n",
    "                'readability': 7.0,\n",
    "                'length_optimization': 7.0,\n",
    "                'overall_quality': 6.8\n",
    "            }\n",
    "    \n",
    "    def _calculate_readability(self, text: str) -> float:\n",
    "        \"\"\"Calcula un score de legibilidad básico\"\"\"\n",
    "        if not text:\n",
    "            return 5.0\n",
    "        \n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        # Métricas básicas\n",
    "        avg_words_per_sentence = len(words) / len(sentences) if sentences else len(words)\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "        \n",
    "        # Score basado en complejidad\n",
    "        readability = 10\n",
    "        \n",
    "        # Penalizar oraciones muy largas\n",
    "        if avg_words_per_sentence > 20:\n",
    "            readability -= 2\n",
    "        elif avg_words_per_sentence > 15:\n",
    "            readability -= 1\n",
    "        \n",
    "        # Penalizar palabras muy largas\n",
    "        if avg_word_length > 8:\n",
    "            readability -= 2\n",
    "        elif avg_word_length > 6:\n",
    "            readability -= 1\n",
    "        \n",
    "        # Bonificar longitud apropiada\n",
    "        if 5 <= len(words) <= 20:\n",
    "            readability += 1\n",
    "        \n",
    "        return max(1.0, min(10.0, readability))\n",
    "    \n",
    "    def _generate_recommendations_enhanced(self, evaluation: Dict, processing_steps: List) -> List[str]:\n",
    "        \"\"\"Genera recomendaciones mejoradas\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        try:\n",
    "            if evaluation.get('semantic_similarity', 0) < 7:\n",
    "                recommendations.append(\"Ajustar parámetros de lematización para preservar mejor el significado original\")\n",
    "            \n",
    "            if evaluation.get('coherence', 0) < 7:\n",
    "                recommendations.append(\"Mejorar la coherencia textual manteniendo la estructura gramatical\")\n",
    "            \n",
    "            if evaluation.get('grammar', 0) < 7:\n",
    "                recommendations.append(\"Revisar la gramática del texto procesado\")\n",
    "            \n",
    "            if evaluation.get('environmental_relevance', 0) < 5:\n",
    "                recommendations.append(\"Incorporar más términos ambientales relevantes\")\n",
    "            \n",
    "            if evaluation.get('readability', 0) < 6:\n",
    "                recommendations.append(\"Simplificar el vocabulario para mejorar la legibilidad\")\n",
    "            \n",
    "            if evaluation.get('length_optimization', 0) < 7:\n",
    "                recommendations.append(\"Ajustar la longitud según el tipo de contenido\")\n",
    "            \n",
    "            if not recommendations:\n",
    "                recommendations.append(\"El texto ha sido procesado exitosamente con alta calidad\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            recommendations.append(f\"Error generando recomendaciones: {str(e)}\")\n",
    "        \n",
    "        return recommendations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
