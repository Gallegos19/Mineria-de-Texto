{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 0: Configuracion Inicial y Dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\adrib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\adrib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\adrib\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ===== CONFIGURACIÓN INICIAL =====\n",
    "import re\n",
    "import unicodedata\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import warnings\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Configurar advertencias\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar dispositivo para PyTorch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ===== INSTALACIÓN DE DEPENDENCIAS =====\n",
    "# Instalar spaCy y modelo de español\n",
    "try:\n",
    "    import spacy\n",
    "    nlp = spacy.load(\"es_core_news_md\")\n",
    "except:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"es_core_news_md\"])\n",
    "    nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Instalar NLTK y descargar recursos\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "    from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "    from nltk.corpus import stopwords\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Instalar transformers para BERT\n",
    "try:\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"transformers\"])\n",
    "    from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Texto de ejemplo para todo el procesamiento\n",
    "TEXTO_EJEMPLO = \"\"\"\n",
    "El cambio climático representa uno de los mayores desafíos ambientales del siglo XXI.\n",
    "La deforestación, la contaminación del aire y el uso excesivo de combustibles fósiles\n",
    "están afectando gravemente nuestro planeta. Es fundamental implementar energías renovables\n",
    "y promover prácticas sostenibles para proteger el medio ambiente para las futuras generaciones.\n",
    "Las ONGs trabajan para proteger la biodiversidad, pero necesitamos más acciones concretas.\n",
    "Visita https://ejemplo.com para más información o contacta a info@medioambiente.org.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 1: Ingesta de Texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MÓDULO 1: INGESTA DEL TEXTO =====\n",
    "class TextIngestion:\n",
    "    \"\"\"\n",
    "    Módulo para la ingesta de texto desde diferentes fuentes.\n",
    "    Permite entrada manual, desde archivos o URLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.supported_formats = ['txt', 'csv', 'json']\n",
    "    \n",
    "    def ingest_manual_text(self, text):\n",
    "        \"\"\"\n",
    "        Ingesta de texto manual directo\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            raise ValueError(\"El texto debe ser una cadena de caracteres\")\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'source': 'manual',\n",
    "            'length': len(text),\n",
    "            'word_count': len(text.split())\n",
    "        }\n",
    "    \n",
    "    def ingest_from_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Ingesta de texto desde archivo\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            \n",
    "            return {\n",
    "                'original_text': text,\n",
    "                'source': f'file: {file_path}',\n",
    "                'length': len(text),\n",
    "                'word_count': len(text.split())\n",
    "            }\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error al leer archivo: {str(e)}\")\n",
    "    \n",
    "    def validate_text(self, text_data):\n",
    "        \"\"\"\n",
    "        Validación básica del texto ingresado\n",
    "        \"\"\"\n",
    "        text = text_data['original_text']\n",
    "        \n",
    "        validations = {\n",
    "            'is_empty': len(text.strip()) == 0,\n",
    "            'min_length': len(text) >= 10,\n",
    "            'has_letters': bool(re.search(r'[a-zA-ZáéíóúÁÉÍÓÚñÑ]', text)),\n",
    "            'encoding_issues': '�' in text\n",
    "        }\n",
    "        \n",
    "        return validations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 2: Limpieza Basica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MÓDULO 2: LIMPIEZA BÁSICA =====\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        self.url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        self.email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "        self.phone_pattern = re.compile(r'(\\+?[0-9]{1,3}[-.\\s]?)?(\\(?[0-9]{3}\\)?[-.\\s]?[0-9]{3}[-.\\s]?[0-9]{4})')\n",
    "        self.html_pattern = re.compile(r'<[^>]+>')\n",
    "        self.special_chars_pattern = re.compile(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)áéíóúÁÉÍÓÚñÑüÜ]')\n",
    "        self.multiple_spaces_pattern = re.compile(r'\\s+')\n",
    "        self.multiple_punctuation_pattern = re.compile(r'([.!?]){2,}')\n",
    "    \n",
    "    def remove_urls(self, text: str) -> str:\n",
    "        return self.url_pattern.sub('', text)\n",
    "    \n",
    "    def remove_emails(self, text: str) -> str:\n",
    "        return self.email_pattern.sub('', text)\n",
    "    \n",
    "    def remove_phone_numbers(self, text: str) -> str:\n",
    "        return self.phone_pattern.sub('', text)\n",
    "    \n",
    "    def remove_html_tags(self, text: str) -> str:\n",
    "        return self.html_pattern.sub('', text)\n",
    "    \n",
    "    def remove_special_characters(self, text: str, keep_punctuation: bool = True) -> str:\n",
    "        if keep_punctuation:\n",
    "            cleaned = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)áéíóúÁÉÍÓÚñÑüÜ]', ' ', text)\n",
    "        else:\n",
    "            cleaned = re.sub(r'[^\\w\\s áéíóúÁÉÍÓÚñÑüÜ]', ' ', text)\n",
    "        return cleaned\n",
    "    \n",
    "    def normalize_whitespace(self, text: str) -> str:\n",
    "        text = self.multiple_spaces_pattern.sub(' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def normalize_punctuation(self, text: str) -> str:\n",
    "        return self.multiple_punctuation_pattern.sub(r'\\1', text)\n",
    "    \n",
    "    def remove_extra_newlines(self, text: str) -> str:\n",
    "        text = re.sub(r'\\n+', '\\n', text)\n",
    "        text = re.sub(r'\\n', ' ', text)\n",
    "        return text\n",
    "    \n",
    "    def clean_encoding_issues(self, text: str) -> str:\n",
    "        encoding_fixes = {\n",
    "            'Ã¡': 'á', 'Ã©': 'é', 'Ã­': 'í', 'Ã³': 'ó', 'Ãº': 'ú',\n",
    "            'Ã±': 'ñ', 'Ã¼': 'ü', 'Â': '', 'â€™': \"'\", 'â€œ': '\"',\n",
    "            'â€': '\"', 'â€¦': '...', 'â€\"': '-', '�': ''\n",
    "        }\n",
    "        \n",
    "        for wrong, correct in encoding_fixes.items():\n",
    "            text = text.replace(wrong, correct)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def basic_clean(self, text: str, options: Dict = None) -> Dict:\n",
    "        if options is None:\n",
    "            options = {\n",
    "                'remove_urls': True,\n",
    "                'remove_emails': True,\n",
    "                'remove_phones': True,\n",
    "                'remove_html': True,\n",
    "                'remove_special_chars': True,\n",
    "                'keep_punctuation': True,\n",
    "                'normalize_whitespace': True,\n",
    "                'normalize_punctuation': True,\n",
    "                'remove_newlines': True,\n",
    "                'fix_encoding': True\n",
    "            }\n",
    "        \n",
    "        original_text = text\n",
    "        original_length = len(text)\n",
    "        \n",
    "        if options.get('fix_encoding', True):\n",
    "            text = self.clean_encoding_issues(text)\n",
    "        \n",
    "        if options.get('remove_html', True):\n",
    "            text = self.remove_html_tags(text)\n",
    "        \n",
    "        if options.get('remove_urls', True):\n",
    "            text = self.remove_urls(text)\n",
    "        \n",
    "        if options.get('remove_emails', True):\n",
    "            text = self.remove_emails(text)\n",
    "        \n",
    "        if options.get('remove_phones', True):\n",
    "            text = self.remove_phone_numbers(text)\n",
    "        \n",
    "        if options.get('remove_newlines', True):\n",
    "            text = self.remove_extra_newlines(text)\n",
    "        \n",
    "        if options.get('remove_special_chars', True):\n",
    "            text = self.remove_special_characters(text, options.get('keep_punctuation', True))\n",
    "        \n",
    "        if options.get('normalize_punctuation', True):\n",
    "            text = self.normalize_punctuation(text)\n",
    "        \n",
    "        if options.get('normalize_whitespace', True):\n",
    "            text = self.normalize_whitespace(text)\n",
    "        \n",
    "        cleaned_length = len(text)\n",
    "        reduction_percentage = ((original_length - cleaned_length) / original_length) * 100 if original_length > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            'original_text': original_text,\n",
    "            'cleaned_text': text,\n",
    "            'original_length': original_length,\n",
    "            'cleaned_length': cleaned_length,\n",
    "            'reduction_percentage': round(reduction_percentage, 2),\n",
    "            'cleaning_options': options\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 3: Tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MÓDULO 3: TOKENIZACIÓN =====\n",
    "class TextTokenizer:\n",
    "    def __init__(self):\n",
    "        self.nlp = nlp\n",
    "        self.sentence_endings = re.compile(r'[.!?]+')\n",
    "        self.word_pattern = re.compile(r'\\b\\w+\\b')\n",
    "        self.punctuation_pattern = re.compile(r'[^\\w\\s]')\n",
    "    \n",
    "    def tokenize_sentences(self, text: str, method: str = 'nltk') -> List[str]:\n",
    "        if method == 'nltk':\n",
    "            sentences = sent_tokenize(text, language='spanish')\n",
    "        elif method == 'spacy':\n",
    "            doc = self.nlp(text)\n",
    "            sentences = [sent.text.strip() for sent in doc.sents]\n",
    "        elif method == 'regex':\n",
    "            sentences = self.sentence_endings.split(text)\n",
    "            sentences = [s.strip() for s in sentences if s.strip()]\n",
    "        else:\n",
    "            raise ValueError(\"Método debe ser 'nltk', 'spacy' o 'regex'\")\n",
    "        \n",
    "        return [s for s in sentences if len(s.strip()) > 0]\n",
    "    \n",
    "    def tokenize_words(self, text: str, method: str = 'nltk') -> List[str]:\n",
    "        if method == 'nltk':\n",
    "            words = word_tokenize(text, language='spanish')\n",
    "        elif method == 'spacy':\n",
    "            doc = self.nlp(text)\n",
    "            words = [token.text for token in doc if not token.is_space]\n",
    "        elif method == 'regex':\n",
    "            words = self.word_pattern.findall(text)\n",
    "        else:\n",
    "            raise ValueError(\"Método debe ser 'nltk', 'spacy' o 'regex'\")\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    def tokenize_paragraphs(self, text: str) -> List[str]:\n",
    "        paragraphs = re.split(r'\\n\\s*\\n', text)\n",
    "        return [p.strip() for p in paragraphs if p.strip()]\n",
    "    \n",
    "    def advanced_tokenization(self, text: str) -> Dict:\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        tokens_info = []\n",
    "        for token in doc:\n",
    "            if not token.is_space:\n",
    "                token_info = {\n",
    "                    'text': token.text,\n",
    "                    'lemma': token.lemma_,\n",
    "                    'pos': token.pos_,\n",
    "                    'tag': token.tag_,\n",
    "                    'is_alpha': token.is_alpha,\n",
    "                    'is_stop': token.is_stop,\n",
    "                    'is_punct': token.is_punct,\n",
    "                    'is_digit': token.is_digit,\n",
    "                    'shape': token.shape_,\n",
    "                    'is_title': token.is_title,\n",
    "                    'is_lower': token.is_lower,\n",
    "                    'is_upper': token.is_upper\n",
    "                }\n",
    "                tokens_info.append(token_info)\n",
    "        \n",
    "        return {\n",
    "            'tokens': tokens_info,\n",
    "            'total_tokens': len(tokens_info),\n",
    "            'sentences': [sent.text for sent in doc.sents],\n",
    "            'entities': [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        }\n",
    "    \n",
    "    def tokenize_by_pos(self, text: str) -> Dict[str, List[str]]:\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        pos_groups = {\n",
    "            'sustantivos': [],\n",
    "            'verbos': [],\n",
    "            'adjetivos': [],\n",
    "            'adverbios': [],\n",
    "            'preposiciones': [],\n",
    "            'conjunciones': [],\n",
    "            'determinantes': [],\n",
    "            'pronombres': [],\n",
    "            'otros': []\n",
    "        }\n",
    "        \n",
    "        pos_mapping = {\n",
    "            'NOUN': 'sustantivos',\n",
    "            'VERB': 'verbos',\n",
    "            'ADJ': 'adjetivos',\n",
    "            'ADV': 'adverbios',\n",
    "            'ADP': 'preposiciones',\n",
    "            'CONJ': 'conjunciones',\n",
    "            'CCONJ': 'conjunciones',\n",
    "            'DET': 'determinantes',\n",
    "            'PRON': 'pronombres'\n",
    "        }\n",
    "        \n",
    "        for token in doc:\n",
    "            if token.is_alpha and not token.is_stop:\n",
    "                category = pos_mapping.get(token.pos_, 'otros')\n",
    "                pos_groups[category].append(token.lemma_.lower())\n",
    "        \n",
    "        for category in pos_groups:\n",
    "            pos_groups[category] = list(dict.fromkeys(pos_groups[category]))\n",
    "        \n",
    "        return pos_groups\n",
    "    \n",
    "    def extract_environmental_terms(self, text: str) -> Dict[str, List[str]]:\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        environmental_categories = {\n",
    "            'problemas_ambientales': [],\n",
    "            'soluciones': [],\n",
    "            'recursos_naturales': [],\n",
    "            'energia': [],\n",
    "            'contaminacion': [],\n",
    "            'conservacion': []\n",
    "        }\n",
    "        \n",
    "        environmental_terms = {\n",
    "            'problemas_ambientales': [\n",
    "                'cambio climático', 'calentamiento global', 'deforestación', \n",
    "                'contaminación', 'extinción', 'desertificación', 'erosión'\n",
    "            ],\n",
    "            'soluciones': [\n",
    "                'reciclaje', 'energía renovable', 'sostenible', 'sustentable',\n",
    "                'conservación', 'reforestación', 'eficiencia energética'\n",
    "            ],\n",
    "            'recursos_naturales': [\n",
    "                'agua', 'bosque', 'océano', 'biodiversidad', 'ecosistema',\n",
    "                'fauna', 'flora', 'selva', 'río', 'lago'\n",
    "            ],\n",
    "            'energia': [\n",
    "                'solar', 'eólica', 'hidroeléctrica', 'geotérmica', \n",
    "                'biomasa', 'combustible fósil', 'petróleo', 'carbón'\n",
    "            ],\n",
    "            'contaminacion': [\n",
    "                'emisiones', 'gases', 'residuos', 'basura', 'tóxicos',\n",
    "                'plástico', 'químicos', 'desechos'\n",
    "            ],\n",
    "            'conservacion': [\n",
    "                'protección', 'preservación', 'área protegida', 'parque nacional',\n",
    "                'reserva', 'santuario', 'hábitat'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for category, terms in environmental_terms.items():\n",
    "            for term in terms:\n",
    "                if term in text_lower:\n",
    "                    environmental_categories[category].append(term)\n",
    "        \n",
    "        return environmental_categories\n",
    "    \n",
    "    def tokenization_statistics(self, text: str) -> Dict:\n",
    "        sentences = self.tokenize_sentences(text, 'spacy')\n",
    "        words = self.tokenize_words(text, 'spacy')\n",
    "        paragraphs = self.tokenize_paragraphs(text)\n",
    "        advanced_tokens = self.advanced_tokenization(text)\n",
    "        pos_groups = self.tokenize_by_pos(text)\n",
    "        env_terms = self.extract_environmental_terms(text)\n",
    "        \n",
    "        word_lengths = [len(word) for word in words if word.isalpha()]\n",
    "        sentence_lengths = [len(sent.split()) for sent in sentences]\n",
    "        \n",
    "        stats = {\n",
    "            'total_characters': len(text),\n",
    "            'total_words': len(words),\n",
    "            'total_sentences': len(sentences),\n",
    "            'total_paragraphs': len(paragraphs),\n",
    "            'unique_words': len(set(word.lower() for word in words if word.isalpha())),\n",
    "            'avg_word_length': np.mean(word_lengths) if word_lengths else 0,\n",
    "            'avg_sentence_length': np.mean(sentence_lengths) if sentence_lengths else 0,\n",
    "            'pos_distribution': {k: len(v) for k, v in pos_groups.items()},\n",
    "            'environmental_terms_count': sum(len(terms) for terms in env_terms.values()),\n",
    "            'entities_found': len(advanced_tokens['entities']),\n",
    "            'lexical_diversity': len(set(word.lower() for word in words if word.isalpha())) / len(words) if words else 0\n",
    "        }\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 4: Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== MÓDULO 4: NORMALIZACIÓN =====\n",
    "class TextNormalizer:\n",
    "    def __init__(self):\n",
    "        self.contractions = {\n",
    "            'del': 'de el', 'al': 'a el', 'pa': 'para', 'pal': 'para el',\n",
    "            'pá': 'para', 'q': 'que', 'xq': 'porque', 'porq': 'porque',\n",
    "            'x': 'por', 'tb': 'también', 'tmb': 'también', 'tbn': 'también',\n",
    "            'pq': 'porque', 'xk': 'porque', 'k': 'que', 'dnd': 'donde',\n",
    "            'dónde': 'donde', 'cuándo': 'cuando', 'cómo': 'como',\n",
    "            'qué': 'que', 'cuál': 'cual', 'cuáles': 'cuales',\n",
    "            'quién': 'quien', 'quiénes': 'quienes'\n",
    "        }\n",
    "        \n",
    "        self.abbreviations = {\n",
    "            'dr.': 'doctor', 'dra.': 'doctora', 'sr.': 'señor',\n",
    "            'sra.': 'señora', 'srta.': 'señorita', 'prof.': 'profesor',\n",
    "            'profa.': 'profesora', 'ing.': 'ingeniero', 'lic.': 'licenciado',\n",
    "            'etc.': 'etcétera', 'vs.': 'versus', 'ej.': 'ejemplo',\n",
    "            'p.ej.': 'por ejemplo', 'i.e.': 'es decir', 'e.g.': 'por ejemplo',\n",
    "            'aprox.': 'aproximadamente', 'máx.': 'máximo', 'mín.': 'mínimo',\n",
    "            'kg.': 'kilogramos', 'km.': 'kilómetros', 'm.': 'metros',\n",
    "            'cm.': 'centímetros', 'mm.': 'milímetros', 'co2': 'dióxido de carbono',\n",
    "            'ong': 'organización no gubernamental', 'onu': 'organización de las naciones unidas'\n",
    "        }\n",
    "        \n",
    "        self.number_words = {\n",
    "            'cero': '0', 'uno': '1', 'dos': '2', 'tres': '3', 'cuatro': '4',\n",
    "            'cinco': '5', 'seis': '6', 'siete': '7', 'ocho': '8', 'nueve': '9',\n",
    "            'diez': '10', 'once': '11', 'doce': '12', 'trece': '13', 'catorce': '14',\n",
    "            'quince': '15', 'dieciséis': '16', 'diecisiete': '17', 'dieciocho': '18',\n",
    "            'diecinueve': '19', 'veinte': '20', 'treinta': '30', 'cuarenta': '40',\n",
    "            'cincuenta': '50', 'sesenta': '60', 'setenta': '70', 'ochenta': '80',\n",
    "            'noventa': '90', 'cien': '100', 'mil': '1000', 'millón': '1000000'\n",
    "        }\n",
    "        \n",
    "        self.env_normalizations = {\n",
    "            'co2': 'dióxido de carbono', 'co₂': 'dióxido de carbono',\n",
    "            'ch4': 'metano', 'ch₄': 'metano', 'n2o': 'óxido nitroso',\n",
    "            'n₂o': 'óxido nitroso', 'ghg': 'gases de efecto invernadero',\n",
    "            'gei': 'gases de efecto invernadero', 'renewable energy': 'energía renovable',\n",
    "            'green house': 'efecto invernadero', 'global warming': 'calentamiento global',\n",
    "            'climate change': 'cambio climático', 'sustainable development': 'desarrollo sostenible',\n",
    "            'carbon footprint': 'huella de carbono', 'biodiversity': 'biodiversidad',\n",
    "            'ecosystem': 'ecosistema', 'deforestation': 'deforestación',\n",
    "            'reforestation': 'reforestación'\n",
    "        }\n",
    "    \n",
    "    def to_lowercase(self, tokens: List[str]) -> List[str]:\n",
    "        return [token.lower() for token in tokens]\n",
    "    \n",
    "    def remove_accents(self, tokens: List[str]) -> List[str]:\n",
    "        normalized_tokens = []\n",
    "        for token in tokens:\n",
    "            text_nfd = unicodedata.normalize('NFD', token)\n",
    "            text_without_accents = ''.join(\n",
    "                char for char in text_nfd \n",
    "                if unicodedata.category(char) != 'Mn'\n",
    "            )\n",
    "            normalized_tokens.append(text_without_accents)\n",
    "        return normalized_tokens\n",
    "    \n",
    "    def expand_contractions(self, tokens: List[str]) -> List[str]:\n",
    "        expanded_tokens = []\n",
    "        for token in tokens:\n",
    "            token_lower = token.lower()\n",
    "            if token_lower in self.contractions:\n",
    "                expansion = self.contractions[token_lower]\n",
    "                if ' ' in expansion:\n",
    "                    expanded_tokens.extend(expansion.split())\n",
    "                else:\n",
    "                    expanded_tokens.append(expansion)\n",
    "            else:\n",
    "                expanded_tokens.append(token)\n",
    "        return expanded_tokens\n",
    "    \n",
    "    def expand_abbreviations(self, tokens: List[str]) -> List[str]:\n",
    "        expanded_tokens = []\n",
    "        for token in tokens:\n",
    "            token_lower = token.lower()\n",
    "            if token_lower in self.abbreviations:\n",
    "                expansion = self.abbreviations[token_lower]\n",
    "                if ' ' in expansion:\n",
    "                    expanded_tokens.extend(expansion.split())\n",
    "                else:\n",
    "                    expanded_tokens.append(expansion)\n",
    "            else:\n",
    "                expanded_tokens.append(token)\n",
    "        return expanded_tokens\n",
    "    \n",
    "    def normalize_numbers(self, tokens: List[str], strategy: str = 'keep') -> List[str]:\n",
    "        if strategy == 'remove':\n",
    "            return [token for token in tokens if not token.isdigit()]\n",
    "        elif strategy == 'words_to_digits':\n",
    "            normalized_tokens = []\n",
    "            for token in tokens:\n",
    "                token_lower = token.lower()\n",
    "                if token_lower in self.number_words:\n",
    "                    normalized_tokens.append(self.number_words[token_lower])\n",
    "                else:\n",
    "                    normalized_tokens.append(token)\n",
    "            return normalized_tokens\n",
    "        elif strategy == 'digits_to_words':\n",
    "            digit_to_word = {v: k for k, v in self.number_words.items() if int(v) <= 20}\n",
    "            normalized_tokens = []\n",
    "            for token in tokens:\n",
    "                if token.isdigit() and token in digit_to_word:\n",
    "                    normalized_tokens.append(digit_to_word[token])\n",
    "                else:\n",
    "                    normalized_tokens.append(token)\n",
    "            return normalized_tokens\n",
    "        else:\n",
    "            return tokens\n",
    "    \n",
    "    def normalize_case_patterns(self, tokens: List[str]) -> List[str]:\n",
    "        normalized_tokens = []\n",
    "        for token in tokens:\n",
    "            if token.isupper() and len(token) > 1:\n",
    "                normalized_tokens.append(token.lower())\n",
    "            elif token.islower():\n",
    "                normalized_tokens.append(token)\n",
    "            else:\n",
    "                normalized_tokens.append(token)\n",
    "        return normalized_tokens\n",
    "    \n",
    "    def normalize_environmental_terms(self, tokens: List[str]) -> List[str]:\n",
    "        text = ' '.join(tokens).lower()\n",
    "        \n",
    "        for term, normalized in self.env_normalizations.items():\n",
    "            text = re.sub(r'\\b' + re.escape(term) + r'\\b', normalized, text)\n",
    "        \n",
    "        return text.split()\n",
    "    \n",
    "    def normalize_tokens(self, tokens: List[str], options: Dict = None) -> Dict:\n",
    "        if options is None:\n",
    "            options = {\n",
    "                'to_lowercase': True,\n",
    "                'remove_accents': True,\n",
    "                'expand_contractions': True,\n",
    "                'expand_abbreviations': True,\n",
    "                'normalize_numbers': 'keep',\n",
    "                'normalize_case_patterns': True,\n",
    "                'normalize_environmental_terms': True\n",
    "            }\n",
    "        \n",
    "        original_tokens = tokens.copy()\n",
    "        steps_applied = []\n",
    "        \n",
    "        if options.get('normalize_case_patterns', True):\n",
    "            tokens = self.normalize_case_patterns(tokens)\n",
    "            steps_applied.append('case_patterns')\n",
    "        \n",
    "        if options.get('to_lowercase', True):\n",
    "            tokens = self.to_lowercase(tokens)\n",
    "            steps_applied.append('lowercase')\n",
    "        \n",
    "        if options.get('expand_contractions', True):\n",
    "            tokens = self.expand_contractions(tokens)\n",
    "            steps_applied.append('contractions')\n",
    "        \n",
    "        if options.get('expand_abbreviations', True):\n",
    "            tokens = self.expand_abbreviations(tokens)\n",
    "            steps_applied.append('abbreviations')\n",
    "        \n",
    "        if options.get('normalize_environmental_terms', True):\n",
    "            tokens = self.normalize_environmental_terms(tokens)\n",
    "            steps_applied.append('environmental_terms')\n",
    "        \n",
    "        if options.get('normalize_numbers', 'keep') != 'keep':\n",
    "            tokens = self.normalize_numbers(tokens, options['normalize_numbers'])\n",
    "            steps_applied.append('numbers')\n",
    "        \n",
    "        if options.get('remove_accents', True):\n",
    "            tokens = self.remove_accents(tokens)\n",
    "            steps_applied.append('accents')\n",
    "        \n",
    "        return {\n",
    "            'original_tokens': original_tokens,\n",
    "            'normalized_tokens': tokens,\n",
    "            'original_count': len(original_tokens),\n",
    "            'normalized_count': len(tokens),\n",
    "            'token_count_change': len(tokens) - len(original_tokens),\n",
    "            'steps_applied': steps_applied,\n",
    "            'normalization_options': options\n",
    "        }\n",
    "    \n",
    "    def normalize_tokenized_text(self, tokenization_result: Dict, options: Dict = None) -> Dict:\n",
    "        if 'tokens' in tokenization_result:\n",
    "            tokens = [token['text'] for token in tokenization_result['tokens']]\n",
    "        elif isinstance(tokenization_result, list):\n",
    "            tokens = tokenization_result\n",
    "        else:\n",
    "            tokens = tokenization_result.get('words', [])\n",
    "            if not tokens:\n",
    "                raise ValueError(\"No se pudieron extraer tokens del resultado de tokenización\")\n",
    "        \n",
    "        normalization_result = self.normalize_tokens(tokens, options)\n",
    "        \n",
    "        if 'tokens' in tokenization_result and isinstance(tokenization_result['tokens'], list):\n",
    "            normalized_tokens_info = []\n",
    "            \n",
    "            for i, token_info in enumerate(tokenization_result['tokens']):\n",
    "                if i < len(normalization_result['normalized_tokens']):\n",
    "                    normalized_token = normalization_result['normalized_tokens'][i]\n",
    "                    normalized_tokens_info.append({\n",
    "                        **token_info,\n",
    "                        'original_text': token_info['text'],\n",
    "                        'text': normalized_token,\n",
    "                        'normalized': True\n",
    "                    })\n",
    "            \n",
    "            normalization_result['normalized_tokens_info'] = normalized_tokens_info\n",
    "        \n",
    "        return normalization_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 5: Eliminacion del Ruido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MÓDULO 5: ELIMINACIÓN DE RUIDO (CORREGIDO) ===\n",
      "\n",
      "--- PRUEBA 1 ---\n",
      "Texto original: 'Las plantas ayudan a limpiar el aire que respiramos todos los días'\n",
      "Texto limpio: 'plantas ayudan limpiar aire respiramos todos días'\n",
      "Reducción: 41.7%\n",
      "Pasos: noise_words, short_words, stopwords\n",
      "✓ Procesamiento exitoso\n",
      "\n",
      "--- PRUEBA 2 ---\n",
      "Texto original: 'El cambio climático es un problema muy serio que afecta a todo el mundo'\n",
      "Texto limpio: 'cambio climático problema serio afecta todo mundo'\n",
      "Reducción: 50.0%\n",
      "Pasos: noise_words, short_words, stopwords\n",
      "✓ Procesamiento exitoso\n",
      "\n",
      "--- PRUEBA 3 ---\n",
      "Texto original: 'Bueno, pues, el tema del cambio climático es, eh, muy importante y fundamental para el futuro'\n",
      "Texto limpio: 'Bueno, pues, tema cambio climático es, eh, importante fundamental para futuro'\n",
      "Reducción: 31.2%\n",
      "Pasos: noise_words, short_words, stopwords\n",
      "✓ Procesamiento exitoso\n",
      "\n",
      "--- PRUEBA 4 ---\n",
      "Texto original: 'La energía solar es una alternativa limpia y renovable'\n",
      "Texto limpio: 'La energía solar es una alternativa limpia renovable'\n",
      "Reducción: 11.1%\n",
      "Pasos: noise_words, short_words\n",
      "✓ Procesamiento exitoso\n",
      "\n",
      "✨ MÓDULO DE ELIMINACIÓN DE RUIDO CORREGIDO ✨\n"
     ]
    }
   ],
   "source": [
    "# ===== MÓDULO 5: ELIMINACIÓN DE RUIDO (CORREGIDO) =====\n",
    "\n",
    "class NoiseRemover:\n",
    "    \"\"\"\n",
    "    Módulo para la eliminación de ruido del texto.\n",
    "    Elimina stopwords, palabras muy frecuentes/raras, contenido irrelevante\n",
    "    y ruido específico del dominio de manera inteligente y menos agresiva.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Stopwords básicas en español (más conservadoras)\n",
    "        try:\n",
    "            self.spanish_stopwords = set(stopwords.words('spanish'))\n",
    "        except:\n",
    "            # Fallback si NLTK no está disponible\n",
    "            self.spanish_stopwords = {\n",
    "                'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se',\n",
    "                'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con', 'para',\n",
    "                'al', 'del', 'los', 'las', 'una', 'como', 'pero', 'sus', 'me',\n",
    "                'hasta', 'hay', 'donde', 'han', 'quien', 'están', 'estado',\n",
    "                'desde', 'todo', 'nos', 'durante', 'todos', 'uno', 'les', 'ni',\n",
    "                'contra', 'otros', 'ese', 'eso', 'ante', 'ellos', 'e', 'esto',\n",
    "                'mí', 'antes', 'algunos', 'qué', 'unos', 'yo', 'otro', 'otras',\n",
    "                'otra', 'él', 'tanto', 'esa', 'estos', 'mucho', 'quienes', 'nada',\n",
    "                'muchos', 'cual', 'poco', 'ella', 'estar', 'estas', 'algunas',\n",
    "                'algo', 'nosotros', 'mi', 'mis', 'tú', 'te', 'ti', 'tu', 'tus',\n",
    "                'ellas', 'nosotras', 'vosotros', 'vosotras', 'os', 'mío', 'mía',\n",
    "                'míos', 'mías', 'tuyo', 'tuya', 'tuyos', 'tuyas', 'suyo', 'suya',\n",
    "                'suyos', 'suyas', 'nuestro', 'nuestra', 'nuestros', 'nuestras',\n",
    "                'vuestro', 'vuestra', 'vuestros', 'vuestras', 'esos', 'esas'\n",
    "            }\n",
    "        \n",
    "        # Stopwords adicionales personalizadas (reducidas y más específicas)\n",
    "        self.custom_stopwords = {\n",
    "            # Conectores básicos que realmente no aportan significado\n",
    "            'pues', 'bueno', 'entonces', 'así', 'ahora', 'luego', 'después',\n",
    "            'antes', 'mientras', 'durante', 'mediante', 'según', 'incluso',\n",
    "            'sino', 'aunque', 'sin embargo', 'no obstante', 'por tanto', \n",
    "            'por consiguiente', 'en consecuencia',\n",
    "            \n",
    "            # Palabras de relleno muy generales\n",
    "            'cosa', 'cosas', 'algo', 'nada', 'todo', 'todos', 'todas',\n",
    "            'bastante', 'demasiado', 'suficiente',\n",
    "            \n",
    "            # Palabras muy generales que no aportan contexto específico\n",
    "            'forma', 'manera', 'modo', 'tipo', 'tipos', 'clase', 'clases',\n",
    "            'parte', 'partes', 'lado', 'lados', 'vez', 'veces', 'momento',\n",
    "            'momentos', 'tiempo', 'tiempos', 'lugar', 'lugares', 'caso', 'casos'\n",
    "        }\n",
    "        \n",
    "        # Stopwords específicas para contexto ambiental (muy reducidas)\n",
    "        self.environmental_stopwords = {\n",
    "            'tema', 'temas', 'situación', 'situaciones',\n",
    "            'aspecto', 'aspectos', 'punto', 'puntos', \n",
    "            'cuestión', 'cuestiones', 'asunto', 'asuntos'\n",
    "        }\n",
    "        \n",
    "        # Combinar stopwords de manera inteligente\n",
    "        self.basic_stopwords = self.spanish_stopwords\n",
    "        self.extended_stopwords = (self.spanish_stopwords | \n",
    "                                 self.custom_stopwords | \n",
    "                                 self.environmental_stopwords)\n",
    "        \n",
    "        # Patrones de ruido común (más específicos)\n",
    "        self.noise_patterns = [\n",
    "            r'\\b\\w{1,2}\\b',  # Palabras muy cortas (1-2 caracteres)\n",
    "            r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)áéíóúÁÉÍÓÚñÑüÜ]',  # Caracteres especiales\n",
    "        ]\n",
    "        \n",
    "        # Palabras de ruido específicas (interjecciones y muletillas)\n",
    "        self.noise_words = {\n",
    "            'mm', 'hmm', 'eh', 'ah', 'oh', 'uh', 'um', 'er',\n",
    "            'ok', 'okay', 'je', 'ja', 'jaja', 'jeje', 'jajaja',\n",
    "            'etc', 'etcetera', 'bla', 'blah', 'ajá', 'aja'\n",
    "        }\n",
    "        \n",
    "        # Palabras ambientales importantes que NUNCA deben eliminarse\n",
    "        self.environmental_keywords = {\n",
    "            'ambiente', 'ambiental', 'ambientales', 'ecológico', 'ecológica',\n",
    "            'sostenible', 'sustentable', 'verde', 'limpio', 'limpia',\n",
    "            'renovable', 'renovables', 'conservación', 'biodiversidad',\n",
    "            'clima', 'climático', 'climática', 'carbono', 'emisiones',\n",
    "            'contaminación', 'contaminante', 'reciclaje', 'reciclar',\n",
    "            'energía', 'energías', 'natural', 'naturales', 'planeta',\n",
    "            'tierra', 'agua', 'aire', 'bosque', 'bosques', 'océano',\n",
    "            'océanos', 'plantas', 'animales', 'especies', 'ecosistema',\n",
    "            'ecosistemas', 'deforestación', 'reforestación', 'solar',\n",
    "            'eólica', 'hidroeléctrica', 'geotérmica', 'biomasa',\n",
    "            'combustible', 'combustibles', 'fósil', 'fósiles', 'petróleo',\n",
    "            'carbón', 'gas', 'gases', 'residuos', 'basura', 'tóxicos',\n",
    "            'químicos', 'desechos', 'protección', 'preservación',\n",
    "            'calentamiento', 'global', 'efecto', 'invernadero'\n",
    "        }\n",
    "    \n",
    "    def _is_important_word(self, word: str) -> bool:\n",
    "        \"\"\"Determina si una palabra es importante y no debe eliminarse\"\"\"\n",
    "        word_lower = word.lower()\n",
    "        \n",
    "        # Palabras ambientales importantes\n",
    "        if word_lower in self.environmental_keywords:\n",
    "            return True\n",
    "        \n",
    "        # Palabras con significado específico (sustantivos, verbos, adjetivos importantes)\n",
    "        if len(word) >= 4 and word.isalpha():\n",
    "            return True\n",
    "        \n",
    "        # Números pueden ser importantes\n",
    "        if word.isdigit():\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def remove_stopwords(self, text: str, custom_stopwords: set = None, aggressive: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Elimina stopwords del texto de manera inteligente\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a procesar\n",
    "            custom_stopwords: Stopwords adicionales personalizadas\n",
    "            aggressive: Si True, usa eliminación más agresiva\n",
    "        \n",
    "        Returns:\n",
    "            Texto sin stopwords\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return text\n",
    "        \n",
    "        words = text.split()\n",
    "        original_word_count = len(words)\n",
    "        \n",
    "        # Seleccionar conjunto de stopwords según el contexto\n",
    "        if aggressive and original_word_count > 15:\n",
    "            stopwords_to_use = self.extended_stopwords.copy()\n",
    "        else:\n",
    "            stopwords_to_use = self.basic_stopwords.copy()\n",
    "        \n",
    "        if custom_stopwords:\n",
    "            stopwords_to_use.update(custom_stopwords)\n",
    "        \n",
    "        # Filtrar palabras manteniendo las importantes\n",
    "        filtered_words = []\n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            \n",
    "            # Mantener palabras importantes sin importar si son stopwords\n",
    "            if self._is_important_word(word):\n",
    "                filtered_words.append(word)\n",
    "            # Eliminar solo si es stopword y no es importante\n",
    "            elif word_lower not in stopwords_to_use:\n",
    "                filtered_words.append(word)\n",
    "        \n",
    "        # Verificar que no hayamos eliminado demasiadas palabras\n",
    "        if len(filtered_words) < original_word_count * 0.4:  # Si quedan menos del 40%\n",
    "            print(f\"⚠️ Eliminación de stopwords demasiado agresiva ({len(filtered_words)}/{original_word_count}), usando solo stopwords básicas\")\n",
    "            # Intentar con stopwords más básicas\n",
    "            filtered_words = []\n",
    "            for word in words:\n",
    "                if word.lower() not in self.basic_stopwords or self._is_important_word(word):\n",
    "                    filtered_words.append(word)\n",
    "        \n",
    "        # Si aún es muy agresivo, devolver texto original\n",
    "        if len(filtered_words) < original_word_count * 0.3:\n",
    "            print(f\"⚠️ Eliminación aún muy agresiva, devolviendo texto original\")\n",
    "            return text\n",
    "        \n",
    "        result = ' '.join(filtered_words) if filtered_words else text\n",
    "        return result if result.strip() else text\n",
    "    \n",
    "    def remove_by_frequency(self, text: str, min_freq: int = 1, max_freq_ratio: float = 0.7) -> Dict:\n",
    "        \"\"\"\n",
    "        Elimina palabras muy raras o muy frecuentes (versión menos agresiva)\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a procesar\n",
    "            min_freq: Frecuencia mínima para mantener una palabra (reducido a 1)\n",
    "            max_freq_ratio: Ratio máximo de frecuencia (aumentado a 0.7 = 70% del total)\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con texto filtrado y estadísticas\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return {\n",
    "                'filtered_text': text,\n",
    "                'original_word_count': 0,\n",
    "                'filtered_word_count': 0,\n",
    "                'rare_words_removed': 0,\n",
    "                'frequent_words_removed': 0,\n",
    "                'rare_words': [],\n",
    "                'frequent_words': []\n",
    "            }\n",
    "        \n",
    "        words = text.split()\n",
    "        word_freq = {}\n",
    "        \n",
    "        # Contar frecuencias\n",
    "        for word in words:\n",
    "            word_lower = word.lower()\n",
    "            word_freq[word_lower] = word_freq.get(word_lower, 0) + 1\n",
    "        \n",
    "        total_words = len(words)\n",
    "        max_freq = int(total_words * max_freq_ratio)\n",
    "        \n",
    "        # Identificar palabras a eliminar (más conservador)\n",
    "        rare_words = {word for word, freq in word_freq.items() \n",
    "                     if freq < min_freq and not self._is_important_word(word)}\n",
    "        frequent_words = {word for word, freq in word_freq.items() \n",
    "                         if freq > max_freq and not self._is_important_word(word)}\n",
    "        words_to_remove = rare_words | frequent_words\n",
    "        \n",
    "        # Filtrar palabras\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word.lower() not in words_to_remove\n",
    "        ]\n",
    "        \n",
    "        # Verificar que no hayamos eliminado demasiado\n",
    "        if len(filtered_words) < len(words) * 0.5:\n",
    "            print(\"⚠️ Filtrado por frecuencia demasiado agresivo, devolviendo texto original\")\n",
    "            filtered_words = words\n",
    "            rare_words = set()\n",
    "            frequent_words = set()\n",
    "        \n",
    "        return {\n",
    "            'filtered_text': ' '.join(filtered_words),\n",
    "            'original_word_count': len(words),\n",
    "            'filtered_word_count': len(filtered_words),\n",
    "            'rare_words_removed': len(rare_words),\n",
    "            'frequent_words_removed': len(frequent_words),\n",
    "            'rare_words': list(rare_words)[:10],\n",
    "            'frequent_words': list(frequent_words)\n",
    "        }\n",
    "    \n",
    "    def remove_noise_patterns(self, text: str) -> str:\n",
    "        \"\"\"Elimina patrones de ruido usando regex (menos agresivo)\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        original_text = text\n",
    "        \n",
    "        # Solo aplicar patrones muy específicos\n",
    "        for pattern in self.noise_patterns[:1]:  # Solo el primer patrón (palabras muy cortas)\n",
    "            text = re.sub(pattern, ' ', text)\n",
    "        \n",
    "        # Normalizar espacios\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        # Si el resultado es muy diferente, devolver original\n",
    "        if len(text) < len(original_text) * 0.7:\n",
    "            return original_text\n",
    "        \n",
    "        return text if text else original_text\n",
    "    \n",
    "    def remove_noise_words(self, text: str) -> str:\n",
    "        \"\"\"Elimina palabras de ruido específicas\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        words = text.split()\n",
    "        filtered_words = [\n",
    "            word for word in words \n",
    "            if word.lower() not in self.noise_words\n",
    "        ]\n",
    "        return ' '.join(filtered_words) if filtered_words else text\n",
    "    \n",
    "    def remove_short_words(self, text: str, min_length: int = 2) -> str:\n",
    "        \"\"\"Elimina palabras muy cortas (menos agresivo)\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        words = text.split()\n",
    "        filtered_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Mantener palabras importantes aunque sean cortas\n",
    "            if len(word) >= min_length or self._is_important_word(word):\n",
    "                filtered_words.append(word)\n",
    "        \n",
    "        return ' '.join(filtered_words) if filtered_words else text\n",
    "    \n",
    "    def remove_non_alphabetic(self, text: str, keep_numbers: bool = True) -> str:\n",
    "        \"\"\"Elimina tokens que no son alfabéticos (más permisivo)\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        words = text.split()\n",
    "        filtered_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word.isalpha():\n",
    "                filtered_words.append(word)\n",
    "            elif keep_numbers and (word.isdigit() or self._contains_numbers(word)):\n",
    "                filtered_words.append(word)\n",
    "            elif self._is_important_word(word):\n",
    "                filtered_words.append(word)\n",
    "        \n",
    "        return ' '.join(filtered_words) if filtered_words else text\n",
    "    \n",
    "    def _contains_numbers(self, word: str) -> bool:\n",
    "        \"\"\"Verifica si una palabra contiene números importantes\"\"\"\n",
    "        return any(char.isdigit() for char in word) and len(word) <= 10\n",
    "    \n",
    "    def remove_by_pos(self, text: str, pos_to_remove: List[str] = None) -> str:\n",
    "        \"\"\"\n",
    "        Elimina palabras según su categoría gramatical (menos agresivo)\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a procesar\n",
    "            pos_to_remove: Lista de POS tags a eliminar\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        if pos_to_remove is None:\n",
    "            # Solo eliminar categorías muy específicas y poco importantes\n",
    "            pos_to_remove = ['DET', 'ADP']  # Solo determinantes y preposiciones\n",
    "        \n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            filtered_words = []\n",
    "            \n",
    "            for token in doc:\n",
    "                if not token.is_space:\n",
    "                    # Mantener palabras importantes sin importar su POS\n",
    "                    if (self._is_important_word(token.text) or \n",
    "                        token.pos_ not in pos_to_remove):\n",
    "                        filtered_words.append(token.text)\n",
    "            \n",
    "            result = ' '.join(filtered_words)\n",
    "            return result if result.strip() else text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error en filtrado por POS: {e}\")\n",
    "            return text\n",
    "    \n",
    "    def remove_environmental_noise(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Elimina ruido específico del dominio ambiental (muy conservador)\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        \n",
    "        # Solo palabras muy generales que realmente no aportan en contexto ambiental\n",
    "        env_noise = {\n",
    "            'general', 'específico', 'particular', 'especial', 'normal',\n",
    "            'actual', 'presente', 'nuevo', 'viejo', 'grande', 'pequeño',\n",
    "            'mejor', 'peor', 'principal', 'secundario'\n",
    "        }\n",
    "        \n",
    "        words = text.split()\n",
    "        filtered_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            # Solo eliminar si es ruido ambiental Y no es una palabra importante\n",
    "            if (word.lower() not in env_noise or \n",
    "                self._is_important_word(word)):\n",
    "                filtered_words.append(word)\n",
    "        \n",
    "        return ' '.join(filtered_words) if filtered_words else text\n",
    "    \n",
    "    def advanced_noise_removal(self, text: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Eliminación avanzada de ruido usando spaCy (menos agresiva)\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return {\n",
    "                'cleaned_text': text,\n",
    "                'original_tokens': 0,\n",
    "                'kept_tokens': 0,\n",
    "                'removed_tokens': 0,\n",
    "                'removal_details': []\n",
    "            }\n",
    "        \n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            \n",
    "            kept_tokens = []\n",
    "            removed_tokens = []\n",
    "            \n",
    "            for token in doc:\n",
    "                # Criterios más conservadores para eliminar\n",
    "                should_remove = (\n",
    "                    token.is_space or          # Es espacio\n",
    "                    (token.is_punct and len(token.text) == 1) or  # Puntuación simple\n",
    "                    (len(token.text) < 2 and not token.is_alpha) or  # Muy corto y no alfabético\n",
    "                    (token.text.lower() in self.noise_words)  # Palabras de ruido específicas\n",
    "                )\n",
    "                \n",
    "                # NUNCA eliminar palabras importantes\n",
    "                if self._is_important_word(token.text):\n",
    "                    should_remove = False\n",
    "                \n",
    "                if should_remove:\n",
    "                    removed_tokens.append({\n",
    "                        'text': token.text,\n",
    "                        'reason': self._get_removal_reason(token)\n",
    "                    })\n",
    "                else:\n",
    "                    # Usar el texto original en lugar del lema para preservar mejor el significado\n",
    "                    kept_tokens.append(token.text)\n",
    "            \n",
    "            result_text = ' '.join(kept_tokens)\n",
    "            \n",
    "            # Verificar que el resultado tenga sentido\n",
    "            if len(kept_tokens) < len([t for t in doc if not t.is_space]) * 0.5:\n",
    "                print(\"⚠️ Eliminación avanzada demasiado agresiva, usando texto original\")\n",
    "                return {\n",
    "                    'cleaned_text': text,\n",
    "                    'original_tokens': len(doc),\n",
    "                    'kept_tokens': len([t for t in doc if not t.is_space]),\n",
    "                    'removed_tokens': 0,\n",
    "                    'removal_details': []\n",
    "                }\n",
    "            \n",
    "            return {\n",
    "                'cleaned_text': result_text,\n",
    "                'original_tokens': len(doc),\n",
    "                'kept_tokens': len(kept_tokens),\n",
    "                'removed_tokens': len(removed_tokens),\n",
    "                'removal_details': removed_tokens[:20]\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error en eliminación avanzada: {e}\")\n",
    "            return {\n",
    "                'cleaned_text': text,\n",
    "                'original_tokens': 0,\n",
    "                'kept_tokens': 0,\n",
    "                'removed_tokens': 0,\n",
    "                'removal_details': []\n",
    "            }\n",
    "    \n",
    "    def _get_removal_reason(self, token) -> str:\n",
    "        \"\"\"Determina la razón por la cual se elimina un token\"\"\"\n",
    "        if token.is_space:\n",
    "            return 'whitespace'\n",
    "        elif token.is_punct and len(token.text) == 1:\n",
    "            return 'simple_punctuation'\n",
    "        elif len(token.text) < 2 and not token.is_alpha:\n",
    "            return 'too_short_non_alpha'\n",
    "        elif token.text.lower() in self.noise_words:\n",
    "            return 'noise_word'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    def comprehensive_noise_removal(self, text: str, options: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Eliminación completa de ruido con múltiples estrategias (CORREGIDA)\n",
    "        \n",
    "        Args:\n",
    "            text: Texto a limpiar\n",
    "            options: Opciones de limpieza\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con texto limpio y estadísticas\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            # Configuración más conservadora por defecto\n",
    "            options = {\n",
    "                'remove_stopwords': True,\n",
    "                'aggressive_stopwords': False,  # Nuevo parámetro\n",
    "                'remove_short_words': True,\n",
    "                'min_word_length': 2,  # Reducido de 3 a 2\n",
    "                'remove_noise_words': True,\n",
    "                'remove_by_frequency': False,  # Desactivado por defecto para textos cortos\n",
    "                'min_frequency': 1,\n",
    "                'max_frequency_ratio': 0.7,  # Más permisivo\n",
    "                'remove_non_alphabetic': False,  # Desactivado por defecto\n",
    "                'keep_numbers': True,\n",
    "                'remove_by_pos': False,  # Desactivado por defecto\n",
    "                'remove_environmental_noise': False,  # Desactivado por defecto\n",
    "                'use_advanced_removal': False  # Desactivado por defecto\n",
    "            }\n",
    "        \n",
    "        # Validar entrada\n",
    "        if not text or not isinstance(text, str):\n",
    "            return {\n",
    "                'original_text': text or '',\n",
    "                'cleaned_text': text or '',\n",
    "                'original_word_count': 0,\n",
    "                'final_word_count': 0,\n",
    "                'words_removed': 0,\n",
    "                'reduction_percentage': 0,\n",
    "                'processing_steps': ['invalid_input'],\n",
    "                'options_used': options\n",
    "            }\n",
    "        \n",
    "        original_text = text.strip()\n",
    "        if len(original_text) == 0:\n",
    "            return {\n",
    "                'original_text': original_text,\n",
    "                'cleaned_text': original_text,\n",
    "                'original_word_count': 0,\n",
    "                'final_word_count': 0,\n",
    "                'words_removed': 0,\n",
    "                'reduction_percentage': 0,\n",
    "                'processing_steps': ['empty_text'],\n",
    "                'options_used': options\n",
    "            }\n",
    "        \n",
    "        current_text = original_text\n",
    "        processing_steps = []\n",
    "        \n",
    "        # Determinar agresividad basada en longitud del texto\n",
    "        word_count = len(current_text.split())\n",
    "        is_short_text = word_count <= 10\n",
    "        \n",
    "        try:\n",
    "            # Paso 1: Eliminar palabras de ruido específicas (siempre seguro)\n",
    "            if options.get('remove_noise_words', True):\n",
    "                current_text = self.remove_noise_words(current_text)\n",
    "                processing_steps.append('noise_words')\n",
    "            \n",
    "            # Paso 2: Eliminar palabras muy cortas (conservador)\n",
    "            if options.get('remove_short_words', True):\n",
    "                min_length = options.get('min_word_length', 2)\n",
    "                current_text = self.remove_short_words(current_text, min_length)\n",
    "                processing_steps.append('short_words')\n",
    "            \n",
    "            # Paso 3: Eliminar stopwords (adaptativo según longitud)\n",
    "            if options.get('remove_stopwords', True) and not is_short_text:\n",
    "                aggressive = options.get('aggressive_stopwords', False) and word_count > 20\n",
    "                current_text = self.remove_stopwords(current_text, aggressive=aggressive)\n",
    "                processing_steps.append('stopwords')\n",
    "            \n",
    "            # Paso 4: Filtrado por frecuencia (solo para textos largos)\n",
    "            if (options.get('remove_by_frequency', False) and \n",
    "                word_count > 20):\n",
    "                min_freq = options.get('min_frequency', 1)\n",
    "                max_ratio = options.get('max_frequency_ratio', 0.7)\n",
    "                freq_result = self.remove_by_frequency(current_text, min_freq, max_ratio)\n",
    "                current_text = freq_result['filtered_text']\n",
    "                processing_steps.append('frequency_filtering')\n",
    "            \n",
    "            # Paso 5: Eliminar no alfabéticos (opcional)\n",
    "            if options.get('remove_non_alphabetic', False):\n",
    "                keep_nums = options.get('keep_numbers', True)\n",
    "                current_text = self.remove_non_alphabetic(current_text, keep_nums)\n",
    "                processing_steps.append('non_alphabetic')\n",
    "            \n",
    "            # Paso 6: Filtrado por POS (opcional y conservador)\n",
    "            if options.get('remove_by_pos', False) and word_count > 15:\n",
    "                current_text = self.remove_by_pos(current_text)\n",
    "                processing_steps.append('pos_filtering')\n",
    "            \n",
    "            # Paso 7: Eliminar ruido ambiental (opcional)\n",
    "            if options.get('remove_environmental_noise', False):\n",
    "                current_text = self.remove_environmental_noise(current_text)\n",
    "                processing_steps.append('environmental_noise')\n",
    "            \n",
    "            # Paso 8: Eliminación avanzada (opcional)\n",
    "            if options.get('use_advanced_removal', False):\n",
    "                advanced_result = self.advanced_noise_removal(current_text)\n",
    "                current_text = advanced_result['cleaned_text']\n",
    "                processing_steps.append('advanced_removal')\n",
    "            \n",
    "            # Validación final: asegurar que el texto no esté vacío o sea incoherente\n",
    "            if not current_text or len(current_text.strip()) == 0:\n",
    "                print(\"⚠️ El procesamiento resultó en texto vacío, devolviendo texto original\")\n",
    "                current_text = original_text\n",
    "                processing_steps = ['fallback_to_original']\n",
    "            \n",
    "            # Verificar que el texto resultante tenga sentido mínimo\n",
    "            final_words = current_text.split()\n",
    "            original_words = original_text.split()\n",
    "            \n",
    "            if len(final_words) < len(original_words) * 0.2:  # Si quedan menos del 20%\n",
    "                print(f\"⚠️ Eliminación demasiado agresiva ({len(final_words)}/{len(original_words)}), devolviendo texto original\")\n",
    "                current_text = original_text\n",
    "                processing_steps = ['too_aggressive_fallback']\n",
    "            \n",
    "            # Calcular estadísticas finales\n",
    "            original_word_count = len(original_words)\n",
    "            final_word_count = len(final_words)\n",
    "            words_removed = original_word_count - final_word_count\n",
    "            reduction_percentage = (words_removed / original_word_count) * 100 if original_word_count > 0 else 0\n",
    "            \n",
    "            result = {\n",
    "                'original_text': original_text,\n",
    "                'cleaned_text': current_text,\n",
    "                'original_word_count': original_word_count,\n",
    "                'final_word_count': final_word_count,\n",
    "                'words_removed': words_removed,\n",
    "                'reduction_percentage': round(reduction_percentage, 2),\n",
    "                'processing_steps': processing_steps,\n",
    "                'options_used': options\n",
    "            }\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error durante eliminación de ruido: {e}\")\n",
    "            # Devolver texto original en caso de error\n",
    "            return {\n",
    "                'original_text': original_text,\n",
    "                'cleaned_text': original_text,\n",
    "                'original_word_count': len(original_text.split()),\n",
    "                'final_word_count': len(original_text.split()),\n",
    "                'words_removed': 0,\n",
    "                'reduction_percentage': 0,\n",
    "                'processing_steps': ['error_fallback'],\n",
    "                'options_used': options,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "# Ejemplo de uso del módulo corregido\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== MÓDULO 5: ELIMINACIÓN DE RUIDO (CORREGIDO) ===\")\n",
    "    \n",
    "    # Crear instancia del eliminador de ruido\n",
    "    noise_remover = NoiseRemover()\n",
    "    \n",
    "    # Textos de prueba\n",
    "    test_texts = [\n",
    "        \"Las plantas ayudan a limpiar el aire que respiramos todos los días\",\n",
    "        \"El cambio climático es un problema muy serio que afecta a todo el mundo\",\n",
    "        \"Bueno, pues, el tema del cambio climático es, eh, muy importante y fundamental para el futuro\",\n",
    "        \"La energía solar es una alternativa limpia y renovable\"\n",
    "    ]\n",
    "    \n",
    "    for i, text in enumerate(test_texts, 1):\n",
    "        print(f\"\\n--- PRUEBA {i} ---\")\n",
    "        print(f\"Texto original: '{text}'\")\n",
    "        \n",
    "        # Procesamiento conservador (por defecto)\n",
    "        result = noise_remover.comprehensive_noise_removal(text)\n",
    "        print(f\"Texto limpio: '{result['cleaned_text']}'\")\n",
    "        print(f\"Reducción: {result['reduction_percentage']:.1f}%\")\n",
    "        print(f\"Pasos: {', '.join(result['processing_steps'])}\")\n",
    "        \n",
    "        # Verificar que el resultado tenga sentido\n",
    "        if result['cleaned_text'] and len(result['cleaned_text'].strip()) > 0:\n",
    "            print(\"✓ Procesamiento exitoso\")\n",
    "        else:\n",
    "            print(\"❌ Procesamiento falló - texto vacío\")\n",
    "    \n",
    "    print(f\"\\n✨ MÓDULO DE ELIMINACIÓN DE RUIDO CORREGIDO ✨\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 6: Lematizacion y Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTProcessor:\n",
    "    def __init__(self, model_name: str = 'dccuchile/bert-base-spanish-wwm-uncased'):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.max_length = 512\n",
    "        \n",
    "        # Términos ambientales clave y sus contextos asociados\n",
    "        self.environmental_terms = {\n",
    "            'plantas': {\n",
    "                'context': 'a través del proceso de fotosíntesis',\n",
    "                'scientific': 'como biofiltros naturales',\n",
    "                'impact': 'contribuyendo al equilibrio ecológico'\n",
    "            },\n",
    "            'aire': {\n",
    "                'context': 'eliminando contaminantes y produciendo oxígeno',\n",
    "                'scientific': 'mejorando la calidad atmosférica',\n",
    "                'impact': 'reduciendo enfermedades respiratorias'\n",
    "            },\n",
    "            'respirar': {\n",
    "                'context': 'mejorando así nuestra salud respiratoria',\n",
    "                'scientific': 'optimizando el intercambio gaseoso',\n",
    "                'impact': 'aumentando el bienestar general'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Configuración de mejora por tipo de texto\n",
    "        self.improvement_templates = {\n",
    "            'descripcion': {\n",
    "                'intro': \"¿Sabías que {text}\",\n",
    "                'outro': \"Este proceso natural es esencial para nuestro ecosistema.\"\n",
    "            },\n",
    "            'contenido': {\n",
    "                'intro': \"Estudios demuestran que {text}\",\n",
    "                'outro': \"Estos beneficios ecológicos son fundamentales para la sostenibilidad.\"\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Carga el modelo BERT si no está cargado\"\"\"\n",
    "        if self.tokenizer is None or self.model is None:\n",
    "            try:\n",
    "                print(f\"⏳ Cargando modelo BERT: {self.model_name}\")\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "                self.model = BertModel.from_pretrained(self.model_name).to(self.device)\n",
    "                print(\"✓ Modelo cargado exitosamente\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error cargando modelo: {str(e)}\")\n",
    "                print(\"🔁 Intentando con modelo alternativo...\")\n",
    "                self.model_name = 'bert-base-multilingual-cased'\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "                self.model = BertModel.from_pretrained(self.model_name).to(self.device)\n",
    "\n",
    "    def improve_text_with_context(self, text: str, improvement_type: str = 'descripcion', style: str = 'balanced') -> Dict:\n",
    "        \"\"\"\n",
    "        Mejora el texto con contexto ambiental, adaptándose al tipo y estilo solicitado\n",
    "        \"\"\"\n",
    "            # Validar el parámetro style\n",
    "        valid_styles = ['balanced', 'technical', 'conservative']\n",
    "        if style not in valid_styles:\n",
    "            style = 'balanced'  # Valor por defecto si no es válido\n",
    "            print(f\"⚠️ Estilo '{style}' no válido. Usando 'balanced' por defecto\")\n",
    "            \n",
    "        self._load_model()\n",
    "        \n",
    "        # Paso 1: Análisis inicial del texto\n",
    "        original_embedding = self.generate_embeddings(text)\n",
    "        original_terms = self._detect_environmental_terms(text)\n",
    "        \n",
    "        # Paso 2: Generar versiones mejoradas\n",
    "        versions = []\n",
    "        \n",
    "        # Versión básica (conservadora)\n",
    "        basic_improved = self._basic_improvement(text, improvement_type)\n",
    "        versions.append({\n",
    "            'type': 'basic',\n",
    "            'text': basic_improved,\n",
    "            'style': 'conservative'\n",
    "        })\n",
    "        \n",
    "        # Versión con contexto ambiental\n",
    "        if original_terms:\n",
    "            env_improved = self._add_environmental_context(text, style)\n",
    "            versions.append({\n",
    "                'type': 'environmental',\n",
    "                'text': env_improved,\n",
    "                'style': 'environmental'\n",
    "            })\n",
    "        \n",
    "        # Versión técnica (si hay términos detectados)\n",
    "        if len(original_terms) >= 2:\n",
    "            tech_improved = self._technical_improvement(text)\n",
    "            versions.append({\n",
    "                'type': 'technical',\n",
    "                'text': tech_improved,\n",
    "                'style': 'technical'\n",
    "            })\n",
    "        \n",
    "        # Paso 3: Evaluar y seleccionar la mejor versión\n",
    "        evaluated_versions = []\n",
    "        for version in versions:\n",
    "            similarity = self.calculate_semantic_similarity(text, version['text'])\n",
    "            evaluated_versions.append({\n",
    "                **version,\n",
    "                'similarity': similarity['average_similarity'],\n",
    "                'env_score': self._calculate_environmental_score(version['text']),\n",
    "                'readability': self._calculate_readability(version['text'])\n",
    "            })\n",
    "        \n",
    "        # Seleccionar la mejor versión según el estilo solicitado\n",
    "        if style == 'balanced':\n",
    "            best_version = max(evaluated_versions, key=lambda x: x['similarity'] + x['env_score'])\n",
    "        elif style == 'technical':\n",
    "            best_version = max(evaluated_versions, key=lambda x: x['env_score'])\n",
    "        else:  # conservative\n",
    "            best_version = max(evaluated_versions, key=lambda x: x['similarity'])\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'improved_text': best_version['text'],\n",
    "            'improvement_score': best_version['similarity'],\n",
    "            'environmental_score': best_version['env_score'],\n",
    "            'readability_score': best_version['readability'],\n",
    "            'all_versions': evaluated_versions,\n",
    "            'selected_style': best_version['style']\n",
    "        }\n",
    "\n",
    "    def _basic_improvement(self, text: str, text_type: str) -> str:\n",
    "        \"\"\"Mejora básica de estructura y fluidez\"\"\"\n",
    "        template = self.improvement_templates.get(text_type, {})\n",
    "        \n",
    "        improved = text.capitalize()\n",
    "        if 'intro' in template:\n",
    "            improved = template['intro'].format(text=improved)\n",
    "        if 'outro' in template and len(text.split()) < 25:\n",
    "            improved += \" \" + template['outro']\n",
    "        \n",
    "        # Corrección gramatical básica\n",
    "        improved = improved.replace(\" todos los días\", \" diariamente\")\n",
    "        improved = improved.replace(\" ayudan a \", \" contribuyen a \")\n",
    "        \n",
    "        return improved\n",
    "\n",
    "    def _add_environmental_context(self, text: str, style: str = 'balanced') -> str:\n",
    "        \"\"\"Añade contexto ambiental relevante según el estilo\"\"\"\n",
    "        words = text.lower().split()\n",
    "        improved = text\n",
    "        \n",
    "        # Validar el parámetro style\n",
    "        valid_styles = ['technical', 'impact', 'balanced']\n",
    "        if style not in valid_styles:\n",
    "            style = 'balanced'  # Valor por defecto si no es válido\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            if word in self.environmental_terms:\n",
    "                context_info = self.environmental_terms[word]\n",
    "                \n",
    "                if style == 'technical':\n",
    "                    addition = context_info.get('scientific', context_info['context'])\n",
    "                elif style == 'impact':\n",
    "                    addition = context_info.get('impact', context_info['context'])\n",
    "                else:  # balanced\n",
    "                    addition = context_info['context']\n",
    "                \n",
    "                # Insertar en la posición correcta manteniendo capitalización\n",
    "                original_word = text.split()[i]\n",
    "                improved = improved.replace(original_word, f\"{original_word} {addition}\", 1)\n",
    "        \n",
    "        return improved\n",
    "\n",
    "\n",
    "    def _technical_improvement(self, text: str) -> str:\n",
    "        \"\"\"Crea una versión técnica del texto\"\"\"\n",
    "        technical_terms = {\n",
    "            'ayudan': 'contribuyen significativamente a',\n",
    "            'limpian': 'depuran y purifican',\n",
    "            'aire': 'la composición atmosférica',\n",
    "            'días': 'el ciclo diario'\n",
    "        }\n",
    "        \n",
    "        # Reemplazar términos básicos\n",
    "        technical_text = text\n",
    "        for term, replacement in technical_terms.items():\n",
    "            technical_text = technical_text.replace(term, replacement)\n",
    "        \n",
    "        # Añadir datos científicos si es posible\n",
    "        if 'plantas' in technical_text.lower():\n",
    "            technical_text += \" mediante procesos bioquímicos esenciales\"\n",
    "        \n",
    "        return technical_text\n",
    "\n",
    "    def _detect_environmental_terms(self, text: str) -> List[str]:\n",
    "        \"\"\"Detecta términos ambientales en el texto\"\"\"\n",
    "        return [term for term in self.environmental_terms if term in text.lower()]\n",
    "\n",
    "    def _calculate_readability(self, text: str) -> float:\n",
    "        \"\"\"Calcula un score de legibilidad (simplificado)\"\"\"\n",
    "        words = text.split()\n",
    "        avg_word_len = sum(len(word) for word in words) / len(words) if words else 0\n",
    "        sentence_count = text.count('.') + text.count('!') + text.count('?')\n",
    "        \n",
    "        # Fórmula simplificada (mayor score = más legible)\n",
    "        score = 10 - (avg_word_len / 10) - (len(words) / (sentence_count * 25) if sentence_count > 0 else 0)\n",
    "        return max(1, min(10, score))\n",
    "\n",
    "    def comprehensive_text_improvement(self, text: str, target_type: str = 'descripcion', options: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Interfaz principal para la mejora de texto con todas las capacidades\n",
    "        \"\"\"\n",
    "        if options is None:\n",
    "            options = {\n",
    "                'style': 'balanced',  # balanced, technical, conservative\n",
    "                'environmental_focus': True,\n",
    "                'length_optimization': True\n",
    "            }\n",
    "        \n",
    "        # Paso 1: Mejora básica\n",
    "        result = self.improve_text_with_context(\n",
    "            text,\n",
    "            improvement_type=target_type,\n",
    "            style=options['style']\n",
    "        )\n",
    "        \n",
    "        # Paso 2: Optimización de longitud si es necesario\n",
    "        if options['length_optimization']:\n",
    "            optimized = self._optimize_length(result['improved_text'], target_type)\n",
    "            if optimized != result['improved_text']:\n",
    "                similarity = self.calculate_semantic_similarity(result['improved_text'], optimized)\n",
    "                result['improved_text'] = optimized\n",
    "                result['improvement_score'] = (result['improvement_score'] + similarity['average_similarity']) / 2\n",
    "        \n",
    "        # Paso 3: Generar reporte completo\n",
    "        original_length = len(text.split())\n",
    "        improved_length = len(result['improved_text'].split())\n",
    "        \n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'improved_text': result['improved_text'],\n",
    "            'improvement_details': {\n",
    "                'quality_metrics': {\n",
    "                    'semantic_similarity': round(result['improvement_score'] * 10, 1),\n",
    "                    'environmental_relevance': round(result['environmental_score'], 1),\n",
    "                    'readability': round(result['readability_score'], 1)\n",
    "                },\n",
    "                'length_analysis': {\n",
    "                    'original': original_length,\n",
    "                    'improved': improved_length,\n",
    "                    'change': improved_length - original_length\n",
    "                },\n",
    "                'style_used': result['selected_style'],\n",
    "                'environmental_terms_added': result['environmental_score'] - self._calculate_environmental_score(text)\n",
    "            },\n",
    "            'all_versions': result.get('all_versions', [])\n",
    "        }\n",
    "\n",
    "    def _optimize_length(self, text: str, text_type: str) -> str:\n",
    "        \"\"\"Optimiza la longitud del texto según su tipo\"\"\"\n",
    "        words = text.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        # Objetivos de longitud por tipo de texto\n",
    "        targets = {\n",
    "            'descripcion': (15, 30),\n",
    "            'contenido': (30, 50),\n",
    "            'titulo': (5, 10)\n",
    "        }.get(text_type, (20, 40))\n",
    "        \n",
    "        min_target, max_target = targets\n",
    "        \n",
    "        if word_count < min_target:\n",
    "            # Texto demasiado corto - expandir\n",
    "            expansions = {\n",
    "                'descripcion': \" Este proceso es fundamental para el equilibrio ecológico.\",\n",
    "                'contenido': \" Investigaciones científicas respaldan estos beneficios ambientales.\",\n",
    "                'default': \" Un aspecto clave para la sostenibilidad ambiental.\"\n",
    "            }\n",
    "            expansion = expansions.get(text_type, expansions['default'])\n",
    "            return text + expansion\n",
    "        \n",
    "        elif word_count > max_target:\n",
    "            # Texto demasiado largo - resumir\n",
    "            if text_type == 'descripcion':\n",
    "                return ' '.join(words[:max_target]) + '...'\n",
    "            else:\n",
    "                sentences = text.split('.')\n",
    "                if len(sentences) > 1:\n",
    "                    return sentences[0] + '.'\n",
    "                return ' '.join(words[:max_target])\n",
    "        \n",
    "        return text\n",
    "\n",
    "    def generate_embeddings(self, text: str) -> Dict:\n",
    "        \"\"\"Genera embeddings de texto usando BERT\"\"\"\n",
    "        self._load_model()\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt',\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Usar el embedding promedio de todos los tokens\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "        \n",
    "        return {\n",
    "            'embeddings': embeddings,\n",
    "            'text': text,\n",
    "            'token_count': inputs['input_ids'].shape[1]\n",
    "        }\n",
    "\n",
    "    def calculate_semantic_similarity(self, text1: str, text2: str) -> Dict:\n",
    "        \"\"\"Calcula la similitud semántica entre dos textos\"\"\"\n",
    "        emb1 = self.generate_embeddings(text1)['embeddings']\n",
    "        emb2 = self.generate_embeddings(text2)['embeddings']\n",
    "        \n",
    "        similarity = np.dot(emb1, emb2.T) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "        \n",
    "        return {\n",
    "            'average_similarity': float(similarity[0][0]),\n",
    "            'text1': text1,\n",
    "            'text2': text2\n",
    "        }\n",
    "\n",
    "    def _calculate_environmental_score(self, text: str) -> float:\n",
    "        \"\"\"Calcula un score de relevancia ambiental (0-10)\"\"\"\n",
    "        terms_found = self._detect_environmental_terms(text)\n",
    "        words = text.split()\n",
    "        \n",
    "        if not words:\n",
    "            return 0.0\n",
    "            \n",
    "        # Score base por términos encontrados\n",
    "        base_score = len(terms_found) * 2.0\n",
    "        \n",
    "        # Bonus por densidad de términos ambientales\n",
    "        density = len(terms_found) / len(words)\n",
    "        density_bonus = min(3.0, density * 10)\n",
    "        \n",
    "        return min(10.0, base_score + density_bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 7: Procesamiento con BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_text_improvement(self, text: str, target_type: str = 'contenido', options: Dict = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Mejora completa del texto usando todas las capacidades de BERT\n",
    "    \"\"\"\n",
    "    if options is None:\n",
    "        options = {\n",
    "            'environmental_focus': True,\n",
    "            'generate_variations': True,\n",
    "            'optimize_length': True,\n",
    "            'include_embeddings': False,\n",
    "            'similarity_threshold': 0.7\n",
    "        }\n",
    "    \n",
    "    # Análisis inicial\n",
    "    original_embeddings = self.generate_embeddings(text) if options.get('include_embeddings') else None\n",
    "    \n",
    "    # Mejora principal\n",
    "    main_improvement = self.improve_text_with_context_enhanced(\n",
    "        text, \n",
    "        improvement_type=target_type, \n",
    "        environmental_focus=options.get('environmental_focus', True)\n",
    "    )\n",
    "    \n",
    "    # Resultado final\n",
    "    final_text = main_improvement['best_improvement']['text']\n",
    "    \n",
    "    return {\n",
    "        'original_text': text,\n",
    "        'target_type': target_type,\n",
    "        'options': options,\n",
    "        'main_improvement': main_improvement,\n",
    "        'variations': None,\n",
    "        'final_improved_text': final_text,\n",
    "        'improvement_summary': {\n",
    "            'original_length': len(text.split()),\n",
    "            'final_length': len(final_text.split()),\n",
    "            'environmental_terms_added': self._calculate_environmental_score(final_text) - self._calculate_environmental_score(text),\n",
    "            'semantic_preservation': main_improvement['improvement_score']\n",
    "        },\n",
    "        'original_embeddings': original_embeddings\n",
    "    }\n",
    "\n",
    "def _calculate_environmental_score(self, text: str) -> int:\n",
    "    \"\"\"\n",
    "    Calcula la puntuación ambiental basada en la presencia de términos ambientales\n",
    "    \"\"\"\n",
    "    text_lower = text.lower()\n",
    "    return sum(1 for term in self.environmental_keywords if term.lower() in text_lower)\n",
    "\n",
    "def generate_embeddings(self, text: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Genera embeddings para el texto usando BERT\n",
    "    \"\"\"\n",
    "    # Inicializar modelo si no está cargado\n",
    "    if self.tokenizer is None or self.model is None:\n",
    "        try:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = BertModel.from_pretrained(self.model_name).to(self.device)\n",
    "        except Exception as e:\n",
    "            # Fallback a un modelo más pequeño si hay problemas\n",
    "            print(f\"⚠️ Error cargando modelo {self.model_name}: {str(e)}\")\n",
    "            self.model_name = 'dccuchile/bert-base-spanish-wwm-uncased'\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = BertModel.from_pretrained(self.model_name).to(self.device)\n",
    "    \n",
    "    # Tokenizar y obtener embeddings\n",
    "    tokens = self.tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                           max_length=self.max_length, padding=True)\n",
    "    \n",
    "    # Mover tokens a GPU si está disponible\n",
    "    input_ids = tokens['input_ids'].to(self.device)\n",
    "    attention_mask = tokens['attention_mask'].to(self.device)\n",
    "    \n",
    "    # Obtener embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "    # Usar el embedding del token [CLS] como representación del texto\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    return {\n",
    "        'embeddings': embeddings,\n",
    "        'token_count': len(tokens['input_ids'][0]),\n",
    "        'text': text\n",
    "    }\n",
    "\n",
    "def calculate_semantic_similarity(self, text1: str, text2: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Calcula la similitud semántica entre dos textos usando embeddings BERT\n",
    "    \"\"\"\n",
    "    # Obtener embeddings\n",
    "    embeddings1 = self.generate_embeddings(text1)['embeddings']\n",
    "    embeddings2 = self.generate_embeddings(text2)['embeddings']\n",
    "    \n",
    "    # Calcular similitud coseno\n",
    "    similarity = np.dot(embeddings1, embeddings2.T) / (\n",
    "        np.linalg.norm(embeddings1) * np.linalg.norm(embeddings2)\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'average_similarity': float(similarity[0][0]),\n",
    "        'text1': text1,\n",
    "        'text2': text2\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modulo 8: Sistema Completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SISTEMA COMPLETO MEJORADO ===\n",
      "Procesando: 'Las plantas ayudan a limpiar el aire que respiramos todos los días'\n",
      "============================================================\n",
      "Ejecutando Paso 1: Ingesta...\n",
      "Ejecutando Paso 2: Limpieza...\n",
      "Ejecutando Paso 3: Tokenización...\n",
      "Ejecutando Paso 4: Normalización...\n",
      "Ejecutando Paso 5: Eliminación de ruido...\n",
      "Ejecutando Paso 6: Lematización...\n",
      "Ejecutando Paso 7: Procesamiento con BERT...\n",
      "❌ Error durante el procesamiento: 'style'\n",
      "RESULTADO FINAL:\n",
      "Original: 'Las plantas ayudan a limpiar el aire que respiramos todos los días'\n",
      "Mejorado: 'Las plantas ayudan a limpiar el aire que respiramos todos los días'\n",
      "\n",
      "Pasos intermedios:\n",
      "\n",
      "EVALUACIÓN:\n",
      "- Semantic Similarity: 10.00/10\n",
      "- Coherence: 7.00/10\n",
      "- Grammar: 7.00/10\n",
      "- Environmental Relevance: 5.00/10\n",
      "- Readability: 7.00/10\n",
      "- Length Optimization: 7.00/10\n",
      "- Overall Quality: 7.00/10\n",
      "\n",
      "RECOMENDACIONES:\n",
      "- Error durante el procesamiento: 'style'\n",
      "\n",
      "✨ SISTEMA MEJORADO COMPLETADO ✨\n"
     ]
    }
   ],
   "source": [
    "# ===== SISTEMA COMPLETO MEJORADO (COMPLETO) =====\n",
    "\n",
    "class TextMiningSystem:\n",
    "    \"\"\"\n",
    "    Sistema completo de minería de texto que integra todos los módulos\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.ingestion = TextIngestion()\n",
    "        self.cleaner = TextCleaner()\n",
    "        self.tokenizer = TextTokenizer()\n",
    "        self.normalizer = TextNormalizer()\n",
    "        self.noise_remover = NoiseRemover()\n",
    "        self.lemmatizer = TextLemmatizer()\n",
    "        self.bert_processor = BERTProcessor()\n",
    "    \n",
    "    def _validate_text(self, text, step_name=\"\"):\n",
    "        \"\"\"Valida que el texto no sea None o vacío\"\"\"\n",
    "        if text is None:\n",
    "            print(f\"⚠️ Texto es None en el paso: {step_name}\")\n",
    "            return None\n",
    "        if not isinstance(text, str):\n",
    "            print(f\"⚠️ Texto debe ser string en el paso: {step_name}, recibido: {type(text)}\")\n",
    "            return None\n",
    "        if len(text.strip()) == 0:\n",
    "            print(f\"⚠️ Advertencia: Texto vacío en el paso: {step_name}\")\n",
    "            return None\n",
    "        return text.strip()\n",
    "    \n",
    "    def _create_fallback_result(self, text: str, content_type: str, error_msg: str) -> Dict:\n",
    "        \"\"\"Crea un resultado de fallback en caso de error\"\"\"\n",
    "        return {\n",
    "            'original_text': text,\n",
    "            'final_text': text,\n",
    "            'content_type': content_type,\n",
    "            'intermediate_results': {},\n",
    "            'processing_steps': [],\n",
    "            'evaluation': {\n",
    "                'semantic_similarity': 10.0,\n",
    "                'coherence': 7.0,\n",
    "                'grammar': 7.0,\n",
    "                'environmental_relevance': 5.0,\n",
    "                'readability': 7.0,\n",
    "                'length_optimization': 7.0,\n",
    "                'overall_quality': 7.0\n",
    "            },\n",
    "            'recommendations': [f\"Error durante el procesamiento: {error_msg}\"],\n",
    "            'bert_details': None,\n",
    "            'error': error_msg\n",
    "        }\n",
    "    \n",
    "    def process_text_complete_enhanced(self, text: str, content_type: str = 'contenido', track_steps: bool = False) -> Dict:\n",
    "        \"\"\"Procesamiento mejorado con mejor control de calidad\"\"\"\n",
    "        \n",
    "        # Configuraciones específicas por tipo de contenido\n",
    "        processing_configs = {\n",
    "            'titulo': {\n",
    "                'aggressive_cleaning': False,\n",
    "                'preserve_length': True,\n",
    "                'min_similarity': 0.7\n",
    "            },\n",
    "            'descripcion': {\n",
    "                'aggressive_cleaning': False,\n",
    "                'preserve_length': False,\n",
    "                'min_similarity': 0.6\n",
    "            },\n",
    "            'contenido': {\n",
    "                'aggressive_cleaning': True,\n",
    "                'preserve_length': False,\n",
    "                'min_similarity': 0.5\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        config = processing_configs.get(content_type, processing_configs['contenido'])\n",
    "        \n",
    "        # Validación inicial\n",
    "        original_input = text\n",
    "        validated_text = self._validate_text(text, \"entrada inicial\")\n",
    "        if validated_text is None:\n",
    "            text = original_input\n",
    "        else:\n",
    "            text = validated_text\n",
    "        \n",
    "        intermediate_results = {}\n",
    "        processing_steps = []\n",
    "        \n",
    "        try:\n",
    "            # Paso 1: Ingesta\n",
    "            print(\"Ejecutando Paso 1: Ingesta...\")\n",
    "            ingestion_result = self.ingestion.ingest_manual_text(text)\n",
    "            current_text = ingestion_result['original_text']\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['ingestion'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'ingesta',\n",
    "                    'metrics': {'length': ingestion_result['length']}\n",
    "                })\n",
    "            \n",
    "            # Paso 2: Limpieza (menos agresiva)\n",
    "            print(\"Ejecutando Paso 2: Limpieza...\")\n",
    "            cleaning_options = {\n",
    "                'remove_urls': True,\n",
    "                'remove_emails': True,\n",
    "                'remove_phones': True,\n",
    "                'remove_html': True,\n",
    "                'remove_special_chars': not config['preserve_length'],\n",
    "                'keep_punctuation': True,\n",
    "                'normalize_whitespace': True,\n",
    "                'normalize_punctuation': True,\n",
    "                'remove_newlines': True,\n",
    "                'fix_encoding': True\n",
    "            }\n",
    "            cleaning_result = self.cleaner.basic_clean(current_text, cleaning_options)\n",
    "            current_text = cleaning_result['cleaned_text']\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['cleaning'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'limpieza',\n",
    "                    'metrics': {'reduction_percentage': cleaning_result['reduction_percentage']}\n",
    "                })\n",
    "            \n",
    "            # Paso 3: Tokenización\n",
    "            print(\"Ejecutando Paso 3: Tokenización...\")\n",
    "            tokens = self.tokenizer.tokenize_words(current_text, 'spacy')\n",
    "            if not tokens:\n",
    "                tokens = current_text.split()\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['tokenization'] = ' '.join(tokens[:20]) + ('...' if len(tokens) > 20 else '')\n",
    "                processing_steps.append({\n",
    "                    'step': 'tokenización',\n",
    "                    'metrics': {'token_count': len(tokens)}\n",
    "                })\n",
    "            \n",
    "            # Paso 4: Normalización (mejorada)\n",
    "            print(\"Ejecutando Paso 4: Normalización...\")\n",
    "            normalization_options = {\n",
    "                'to_lowercase': True,\n",
    "                'remove_accents': False,  # Preservar acentos para mejor legibilidad\n",
    "                'expand_contractions': True,\n",
    "                'expand_abbreviations': True,\n",
    "                'normalize_numbers': 'keep',\n",
    "                'normalize_case_patterns': True,\n",
    "                'normalize_environmental_terms': True\n",
    "            }\n",
    "            normalization_result = self.normalizer.normalize_tokens(tokens, normalization_options)\n",
    "            normalized_tokens = normalization_result.get('normalized_tokens', tokens)\n",
    "            current_text = ' '.join(normalized_tokens)\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['normalization'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'normalización',\n",
    "                    'metrics': {'token_count_change': normalization_result.get('token_count_change', 0)}\n",
    "                })\n",
    "            \n",
    "            # Paso 5: Eliminación de ruido (configuración adaptativa)\n",
    "            print(\"Ejecutando Paso 5: Eliminación de ruido...\")\n",
    "            noise_options = {\n",
    "                'remove_stopwords': config['aggressive_cleaning'],\n",
    "                'aggressive_stopwords': False,\n",
    "                'remove_short_words': True,\n",
    "                'min_word_length': 2,\n",
    "                'remove_noise_words': True,\n",
    "                'remove_by_frequency': False,\n",
    "                'remove_non_alphabetic': False,\n",
    "                'remove_by_pos': False,\n",
    "                'remove_environmental_noise': False,\n",
    "                'use_advanced_removal': False\n",
    "            }\n",
    "            noise_removal_result = self.noise_remover.comprehensive_noise_removal(current_text, noise_options)\n",
    "            current_text = noise_removal_result['cleaned_text']\n",
    "            \n",
    "            # Validar que el texto no esté vacío después de la eliminación de ruido\n",
    "            if not current_text or len(current_text.strip()) == 0:\n",
    "                print(\"⚠️ Texto vacío después de eliminación de ruido, usando texto anterior\")\n",
    "                current_text = ' '.join(normalized_tokens)\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['noise_removal'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'eliminación_ruido',\n",
    "                    'metrics': {'reduction_percentage': noise_removal_result['reduction_percentage']}\n",
    "                })\n",
    "            \n",
    "            # Paso 6: Lematización (usando método existente)\n",
    "            print(\"Ejecutando Paso 6: Lematización...\")\n",
    "            lemmatization_options = {\n",
    "                'preserve_environmental_terms': True,\n",
    "                'min_word_length': 2,\n",
    "                'remove_duplicates': False\n",
    "            }\n",
    "            lemmatization_result = self.lemmatizer.comprehensive_processing(\n",
    "                current_text, \n",
    "                method='spacy',\n",
    "                options=lemmatization_options\n",
    "            )\n",
    "            current_text = lemmatization_result['final_processed_text']\n",
    "            \n",
    "            # Validar resultado de lematización\n",
    "            if not current_text or len(current_text.strip()) == 0:\n",
    "                print(\"⚠️ Lematización resultó en texto vacío, usando texto anterior\")\n",
    "                current_text = noise_removal_result['cleaned_text']\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['lemmatization'] = current_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'lematización',\n",
    "                    'metrics': {'changes_made': lemmatization_result.get('changes_made', 0)}\n",
    "                })\n",
    "            \n",
    "            # Paso 7: Procesamiento con BERT (mejorado)\n",
    "            print(\"Ejecutando Paso 7: Procesamiento con BERT...\")\n",
    "            \n",
    "            # Validar texto antes de BERT\n",
    "            if not current_text or len(current_text.strip()) == 0:\n",
    "                print(\"⚠️ Texto vacío antes de BERT, usando texto original\")\n",
    "                current_text = text\n",
    "            \n",
    "            bert_options = {\n",
    "                'environmental_focus': True,\n",
    "                'generate_variations': False,\n",
    "                'optimize_length': True,\n",
    "                'preserve_meaning': True,\n",
    "                'min_similarity_threshold': config['min_similarity']\n",
    "            }\n",
    "            \n",
    "            # Usar método existente de BERT\n",
    "            bert_result = self.bert_processor.comprehensive_text_improvement(\n",
    "                current_text,\n",
    "                target_type=content_type,\n",
    "                options=bert_options\n",
    "            )\n",
    "            final_text = bert_result['final_improved_text']\n",
    "            \n",
    "            if track_steps:\n",
    "                intermediate_results['bert_processing'] = final_text\n",
    "                processing_steps.append({\n",
    "                    'step': 'bert_processing',\n",
    "                    'metrics': {\n",
    "                        'semantic_preservation': bert_result['improvement_summary'].get('semantic_preservation', 0.7)\n",
    "                    }\n",
    "                })\n",
    "            \n",
    "            # Evaluación de calidad mejorada\n",
    "            print(\"Ejecutando evaluación de calidad...\")\n",
    "            evaluation = self._evaluate_quality_enhanced(text, final_text, bert_result['improvement_summary'])\n",
    "            \n",
    "            # Generar recomendaciones\n",
    "            recommendations = self._generate_recommendations_enhanced(evaluation, processing_steps)\n",
    "            \n",
    "            return {\n",
    "                'original_text': text,\n",
    "                'final_text': final_text,\n",
    "                'content_type': content_type,\n",
    "                'intermediate_results': intermediate_results if track_steps else {},\n",
    "                'processing_steps': processing_steps if track_steps else [],\n",
    "                'evaluation': evaluation,\n",
    "                'recommendations': recommendations,\n",
    "                'bert_details': bert_result,\n",
    "                'processing_config': config\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error durante el procesamiento: {str(e)}\")\n",
    "            return self._create_fallback_result(text, content_type, str(e))\n",
    "    \n",
    "    def _evaluate_quality_enhanced(self, original_text: str, final_text: str, improvement_summary: Dict) -> Dict:\n",
    "        \"\"\"Evaluación de calidad mejorada\"\"\"\n",
    "        try:\n",
    "            original_words = len(original_text.split()) if original_text else 0\n",
    "            final_words = len(final_text.split()) if final_text else 0\n",
    "            \n",
    "            # Calcular similitud semántica usando BERT\n",
    "            try:\n",
    "                similarity = self.bert_processor.calculate_semantic_similarity(original_text, final_text)\n",
    "                semantic_similarity = similarity['average_similarity']\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error calculando similitud: {e}\")\n",
    "                semantic_similarity = 0.7\n",
    "            \n",
    "            # Evaluar coherencia básica\n",
    "            coherence_score = self._calculate_coherence(final_text)\n",
    "            \n",
    "            # Evaluar gramática básica\n",
    "            grammar_score = self._calculate_grammar(final_text)\n",
    "            \n",
    "            # Score ambiental\n",
    "            try:\n",
    "                env_score = self.bert_processor._calculate_environmental_score(final_text)\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error calculando score ambiental: {e}\")\n",
    "                env_score = 5.0\n",
    "            \n",
    "            # Evaluación de legibilidad\n",
    "            readability_score = self._calculate_readability(final_text)\n",
    "            \n",
    "            # Score general ponderado\n",
    "            overall_quality = (\n",
    "                semantic_similarity * 3 +\n",
    "                coherence_score * 2.5 +\n",
    "                grammar_score * 2 +\n",
    "                (env_score / 10) * 1.5 +\n",
    "                (readability_score / 10) * 1\n",
    "            )\n",
    "            \n",
    "            return {\n",
    "                'semantic_similarity': semantic_similarity * 10,\n",
    "                'coherence': coherence_score * 10,\n",
    "                'grammar': grammar_score * 10,\n",
    "                'environmental_relevance': env_score,\n",
    "                'readability': readability_score,\n",
    "                'length_optimization': min(10, max(1, 10 - abs(final_words - 15) / 5)) if final_words > 0 else 5.0,\n",
    "                'overall_quality': min(10, overall_quality)\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error en evaluación: {e}\")\n",
    "            return {\n",
    "                'semantic_similarity': 7.0,\n",
    "                'coherence': 7.0,\n",
    "                'grammar': 7.0,\n",
    "                'environmental_relevance': 5.0,\n",
    "                'readability': 7.0,\n",
    "                'length_optimization': 7.0,\n",
    "                'overall_quality': 6.8\n",
    "            }\n",
    "    \n",
    "    def _calculate_coherence(self, text: str) -> float:\n",
    "        \"\"\"Calcula un score de coherencia básico\"\"\"\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            return 0.3\n",
    "        \n",
    "        words = text.split()\n",
    "        \n",
    "        # Verificaciones básicas de coherencia\n",
    "        coherence_score = 0.5\n",
    "        \n",
    "        # Verificar longitud apropiada\n",
    "        if 3 <= len(words) <= 25:\n",
    "            coherence_score += 0.2\n",
    "        \n",
    "        # Verificar que tenga estructura básica\n",
    "        try:\n",
    "            doc = nlp(text)\n",
    "            has_noun = any(token.pos_ == 'NOUN' for token in doc)\n",
    "            has_verb = any(token.pos_ == 'VERB' for token in doc)\n",
    "            \n",
    "            if has_noun:\n",
    "                coherence_score += 0.15\n",
    "            if has_verb:\n",
    "                coherence_score += 0.15\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return min(1.0, coherence_score)\n",
    "    \n",
    "    def _calculate_grammar(self, text: str) -> float:\n",
    "        \"\"\"Calcula un score de gramática básico\"\"\"\n",
    "        if not text:\n",
    "            return 0.3\n",
    "        \n",
    "        grammar_score = 0.6  # Base score\n",
    "        \n",
    "        # Verificar que no termine abruptamente\n",
    "        if text.strip().endswith(('.', '!', '?')):\n",
    "            grammar_score += 0.1\n",
    "        \n",
    "        # Verificar longitud apropiada\n",
    "        words = text.split()\n",
    "        if 3 <= len(words) <= 20:\n",
    "            grammar_score += 0.1\n",
    "        \n",
    "        # Verificar que no tenga palabras sueltas sin contexto\n",
    "        if len(words) >= 3:\n",
    "            grammar_score += 0.1\n",
    "        \n",
    "        # Verificar que no tenga repeticiones excesivas\n",
    "        unique_words = len(set(words))\n",
    "        if unique_words / len(words) > 0.7:  # Al menos 70% de palabras únicas\n",
    "            grammar_score += 0.1\n",
    "        \n",
    "        return min(1.0, grammar_score)\n",
    "    \n",
    "    def _calculate_readability(self, text: str) -> float:\n",
    "        \"\"\"Calcula un score de legibilidad básico\"\"\"\n",
    "        if not text:\n",
    "            return 5.0\n",
    "        \n",
    "        words = text.split()\n",
    "        sentences = text.split('.')\n",
    "        \n",
    "        # Métricas básicas\n",
    "        avg_words_per_sentence = len(words) / len(sentences) if sentences else len(words)\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words) if words else 0\n",
    "        \n",
    "        # Score basado en complejidad\n",
    "        readability = 8.0  # Base score\n",
    "        \n",
    "        # Penalizar oraciones muy largas\n",
    "        if avg_words_per_sentence > 20:\n",
    "            readability -= 2\n",
    "        elif avg_words_per_sentence > 15:\n",
    "            readability -= 1\n",
    "        \n",
    "        # Penalizar palabras muy largas\n",
    "        if avg_word_length > 8:\n",
    "            readability -= 1.5\n",
    "        elif avg_word_length > 6:\n",
    "            readability -= 0.5\n",
    "        \n",
    "        # Bonificar longitud apropiada\n",
    "        if 5 <= len(words) <= 20:\n",
    "            readability += 1\n",
    "        \n",
    "        return max(1.0, min(10.0, readability))\n",
    "    \n",
    "    def _generate_recommendations_enhanced(self, evaluation: Dict, processing_steps: List) -> List[str]:\n",
    "        \"\"\"Genera recomendaciones mejoradas\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        try:\n",
    "            if evaluation.get('semantic_similarity', 0) < 7:\n",
    "                recommendations.append(\"Ajustar parámetros de lematización para preservar mejor el significado original\")\n",
    "            \n",
    "            if evaluation.get('coherence', 0) < 7:\n",
    "                recommendations.append(\"Mejorar la coherencia textual manteniendo la estructura gramatical\")\n",
    "            \n",
    "            if evaluation.get('grammar', 0) < 7:\n",
    "                recommendations.append(\"Revisar la gramática del texto procesado\")\n",
    "            \n",
    "            if evaluation.get('environmental_relevance', 0) < 5:\n",
    "                recommendations.append(\"Incorporar más términos ambientales relevantes\")\n",
    "            \n",
    "            if evaluation.get('readability', 0) < 6:\n",
    "                recommendations.append(\"Simplificar el vocabulario para mejorar la legibilidad\")\n",
    "            \n",
    "            if evaluation.get('length_optimization', 0) < 7:\n",
    "                recommendations.append(\"Ajustar la longitud según el tipo de contenido\")\n",
    "            \n",
    "            if not recommendations:\n",
    "                recommendations.append(\"El texto ha sido procesado exitosamente con alta calidad\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            recommendations.append(f\"Error generando recomendaciones: {str(e)}\")\n",
    "        \n",
    "        return recommendations\n",
    "    \n",
    "    # Mantener métodos originales para compatibilidad\n",
    "    def process_text_complete(self, text: str, content_type: str = 'contenido', track_steps: bool = False) -> Dict:\n",
    "        \"\"\"Método original para compatibilidad\"\"\"\n",
    "        return self.process_text_complete_enhanced(text, content_type, track_steps)\n",
    "    \n",
    "    def batch_process(self, texts: List[Dict], progress_callback=None) -> List[Dict]:\n",
    "        \"\"\"Procesa múltiples textos en lote\"\"\"\n",
    "        results = []\n",
    "        total = len(texts)\n",
    "        \n",
    "        for i, text_info in enumerate(texts):\n",
    "            try:\n",
    "                if progress_callback:\n",
    "                    progress_callback(i + 1, total)\n",
    "                \n",
    "                if not isinstance(text_info, dict) or 'text' not in text_info:\n",
    "                    continue\n",
    "                \n",
    "                text = text_info['text']\n",
    "                if not text or not isinstance(text, str):\n",
    "                    continue\n",
    "                \n",
    "                result = self.process_text_complete_enhanced(\n",
    "                    text, \n",
    "                    text_info.get('content_type', 'contenido')\n",
    "                )\n",
    "                \n",
    "                results.append({\n",
    "                    'id': text_info.get('id', f'text_{i}'),\n",
    "                    'content_type': text_info.get('content_type', 'contenido'),\n",
    "                    'original': text,\n",
    "                    'improved': result['final_text'],\n",
    "                    'evaluation': result['evaluation'],\n",
    "                    'success': True\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error procesando texto {i}: {e}\")\n",
    "                results.append({\n",
    "                    'id': text_info.get('id', f'text_{i}'),\n",
    "                    'content_type': text_info.get('content_type', 'contenido'),\n",
    "                    'original': text_info.get('text', 'Error'),\n",
    "                    'improved': text_info.get('text', 'Error'),\n",
    "                    'evaluation': {'overall_quality': 0.0},\n",
    "                    'success': False,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def generate_report(self, result: Dict) -> str:\n",
    "        \"\"\"Genera un reporte detallado del procesamiento\"\"\"\n",
    "        try:\n",
    "            report = f\"# Reporte de Procesamiento de Texto\\n\\n\"\n",
    "            report += f\"## Texto Original\\n{result.get('original_text', 'No disponible')}\\n\\n\"\n",
    "            report += f\"## Texto Mejorado\\n{result.get('final_text', 'No disponible')}\\n\\n\"\n",
    "            report += f\"## Tipo de Contenido\\n{result.get('content_type', 'No especificado')}\\n\\n\"\n",
    "            \n",
    "            report += f\"## Evaluación de Calidad\\n\"\n",
    "            evaluation = result.get('evaluation', {})\n",
    "            for metric, score in evaluation.items():\n",
    "                metric_name = metric.replace('_', ' ').title()\n",
    "                report += f\"- **{metric_name}**: {score:.2f}/10\\n\"\n",
    "            \n",
    "            report += f\"\\n## Recomendaciones\\n\"\n",
    "            recommendations = result.get('recommendations', [])\n",
    "            for rec in recommendations:\n",
    "                report += f\"- {rec}\\n\"\n",
    "            \n",
    "            return report\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error generando reporte: {str(e)}\"\n",
    "\n",
    "# Ejemplo de uso del sistema mejorado\n",
    "print(\"\\n=== SISTEMA COMPLETO MEJORADO ===\")\n",
    "\n",
    "# Crear instancia del sistema mejorado\n",
    "text_mining_system = TextMiningSystem()\n",
    "\n",
    "# Probar con el texto problemático\n",
    "test_text = \"Las plantas ayudan a limpiar el aire que respiramos todos los días\"\n",
    "\n",
    "print(f\"Procesando: '{test_text}'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    result = text_mining_system.process_text_complete_enhanced(\n",
    "        test_text,\n",
    "        'descripcion',\n",
    "        track_steps=True\n",
    "    )\n",
    "\n",
    "    print(\"RESULTADO FINAL:\")\n",
    "    print(f\"Original: '{result['original_text']}'\")\n",
    "    print(f\"Mejorado: '{result['final_text']}'\")\n",
    "\n",
    "    print(f\"\\nPasos intermedios:\")\n",
    "    for step_name, text in result['intermediate_results'].items():\n",
    "        print(f\"- {step_name}: '{text[:100]}{'...' if len(text) > 100 else ''}'\")\n",
    "\n",
    "    print(f\"\\nEVALUACIÓN:\")\n",
    "    for metric, score in result['evaluation'].items():\n",
    "        print(f\"- {metric.replace('_', ' ').title()}: {score:.2f}/10\")\n",
    "\n",
    "    print(f\"\\nRECOMENDACIONES:\")\n",
    "    for rec in result['recommendations']:\n",
    "        print(f\"- {rec}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(f\"\\n✨ SISTEMA MEJORADO COMPLETADO ✨\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
